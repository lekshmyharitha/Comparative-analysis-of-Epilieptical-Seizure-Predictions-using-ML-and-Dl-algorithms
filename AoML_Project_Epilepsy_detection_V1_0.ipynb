{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "AoML_Project_Epilepsy_detection_V1.0.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "pj0E8Z6kjUBM",
        "3PaktGuIlZlG",
        "nmLDRcvf4ZRB",
        "fgKKzi3E4ZRC",
        "Y5vZf7prjGPJ",
        "s04dIHPKiRnr",
        "nx4jgPG9iRns",
        "LoaSJIcDQtZ-",
        "dgADEfSTLVwe",
        "m15sZ6T-Pdfx",
        "tqTrb0vV7WLg",
        "2b5lVMbl7pOL",
        "1jRj6_6TiqkM"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm_O24Yg4ZQ5"
      },
      "source": [
        "#import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math \n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import Series, DataFrame\n",
        "import warnings\n",
        "warnings.filterwarnings( \"ignore\" )\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import KFold\n",
        "import keras\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout, MaxPooling1D,GlobalAveragePooling1D\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.losses import binary_crossentropy"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "cdGZ4jsy4cvy",
        "outputId": "6165a911-cb14-45c2-c0e0-6cdfff56ba9b"
      },
      "source": [
        "from google.colab import files\n",
        "uploade = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f3ffee86-0c87-4d6a-b808-02547de235e0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f3ffee86-0c87-4d6a-b808-02547de235e0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving boston.csv to boston.csv\n",
            "Saving chb.csv to chb.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj0E8Z6kjUBM"
      },
      "source": [
        "#KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQD_EgDO4ZQ_"
      },
      "source": [
        "class ClassKNN(object):\n",
        "    def __init__(self, k):\n",
        "        self.k = k\n",
        "        \n",
        "    @staticmethod   \n",
        "    def _euclidean_distance(row1, row2):\n",
        "        \"\"\"\n",
        "        Calculate and return the euclidean distance between the two parameters passed.\n",
        "        \"\"\"\n",
        "        row1, row2 = np.array(row1), np.array(row2)\n",
        "        distance = 0.0\n",
        "        for i in range(len(row1) - 1):\n",
        "            distance +=(row1[i] - row2[i])**2\n",
        "        return np.sqrt(distance)\n",
        "    \n",
        "    def predict(self, train_set, test_instance):\n",
        "        \"\"\"\n",
        "        Calculate and Return the class of the test instance provided.\n",
        "        \"\"\"\n",
        "        #### Creating a sorted list based on the distance\n",
        "        distances = []\n",
        "        for i in range(len(train_set)):\n",
        "            dist = self._euclidean_distance(train_set[i][:-1], test_instance) # Slicing last column as its the target column\n",
        "            distances.append((train_set[i], dist))\n",
        "        distances.sort(key=lambda x:x[1]) # sort distance list by distance\n",
        "        #####\n",
        "\n",
        "        ##### Creating a list of K neighbors\n",
        "        neighbors = [] # Store list of K neighbors\n",
        "        for i in range(self.k):\n",
        "            neighbors.append(distances[i][0])\n",
        "        ########\n",
        "        ######## Calculating the majority class vote based on the meighbors list\n",
        "        classes = {}\n",
        "        for i in range(len(neighbors)):\n",
        "            response = neighbors[i][-1] # Get the target value\n",
        "            if response in classes:\n",
        "                classes[response] +=1  # If class already exits increment by 1\n",
        "            else:\n",
        "                classes[response] =1\n",
        "        ########\n",
        "        sorted_classes = sorted(classes.items(), key=lambda x:x[1], reverse = True )\n",
        "        return sorted_classes[0][0] # return the class with majority vote \n",
        "    \n",
        "    @staticmethod\n",
        "    def evaluate(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculate and return the accuracy.\n",
        "        \"\"\"\n",
        "        n_correct = 0\n",
        "        for act, pred in zip(y_true, y_pred):\n",
        "            if act == pred:\n",
        "                n_correct += 1\n",
        "        return n_correct / len(y_true)   # Return the accuracy of the classification\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPIP067S4ZQ_"
      },
      "source": [
        "def train_test_split(dataset, test_size = 0.25):\n",
        "    \"\"\"\n",
        "    Split the data into test and train set.\n",
        "    \"\"\"\n",
        "    n_test = int(len(dataset) * test_size)\n",
        "    test_set = dataset.sample(n_test)\n",
        "    train_set = []\n",
        "  \n",
        "    for ind in dataset.index:\n",
        "        if ind in test_set.index:\n",
        "            continue\n",
        "        train_set.append(dataset.iloc[ind])\n",
        "        \n",
        "    train_set = pd.DataFrame(train_set).astype(float).values.tolist()\n",
        "    test_set = test_set.astype(float).values.tolist()\n",
        "    return train_set, test_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6hQV2UDmtPx"
      },
      "source": [
        "def cal_TPR_FPR(tp,tn,fp,fn):\n",
        "  \"\"\"\n",
        "  Calcuate and return the true positive rate and false positive rate.\n",
        "  \"\"\"\n",
        "  tpr = tp/ (tp + fn)\n",
        "  fpr = fp / (fp + fn)\n",
        "  return tpr, fpr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufgCxo5KlgM8"
      },
      "source": [
        "### Dictionary to hold the time of execution \n",
        "execution_time_dic = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PaktGuIlZlG"
      },
      "source": [
        "##### EEG Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "CDFkY5QY4ZQ9",
        "outputId": "85674772-9a35-4859-9a00-c3b937631f98"
      },
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "      <th>X4</th>\n",
              "      <th>X5</th>\n",
              "      <th>X6</th>\n",
              "      <th>X7</th>\n",
              "      <th>X8</th>\n",
              "      <th>X9</th>\n",
              "      <th>X10</th>\n",
              "      <th>X11</th>\n",
              "      <th>X12</th>\n",
              "      <th>X13</th>\n",
              "      <th>X14</th>\n",
              "      <th>X15</th>\n",
              "      <th>X16</th>\n",
              "      <th>X17</th>\n",
              "      <th>X18</th>\n",
              "      <th>X19</th>\n",
              "      <th>X20</th>\n",
              "      <th>X21</th>\n",
              "      <th>X22</th>\n",
              "      <th>X23</th>\n",
              "      <th>X24</th>\n",
              "      <th>X25</th>\n",
              "      <th>X26</th>\n",
              "      <th>X27</th>\n",
              "      <th>X28</th>\n",
              "      <th>X29</th>\n",
              "      <th>X30</th>\n",
              "      <th>X31</th>\n",
              "      <th>X32</th>\n",
              "      <th>X33</th>\n",
              "      <th>X34</th>\n",
              "      <th>X35</th>\n",
              "      <th>X36</th>\n",
              "      <th>X37</th>\n",
              "      <th>X38</th>\n",
              "      <th>X39</th>\n",
              "      <th>...</th>\n",
              "      <th>X140</th>\n",
              "      <th>X141</th>\n",
              "      <th>X142</th>\n",
              "      <th>X143</th>\n",
              "      <th>X144</th>\n",
              "      <th>X145</th>\n",
              "      <th>X146</th>\n",
              "      <th>X147</th>\n",
              "      <th>X148</th>\n",
              "      <th>X149</th>\n",
              "      <th>X150</th>\n",
              "      <th>X151</th>\n",
              "      <th>X152</th>\n",
              "      <th>X153</th>\n",
              "      <th>X154</th>\n",
              "      <th>X155</th>\n",
              "      <th>X156</th>\n",
              "      <th>X157</th>\n",
              "      <th>X158</th>\n",
              "      <th>X159</th>\n",
              "      <th>X160</th>\n",
              "      <th>X161</th>\n",
              "      <th>X162</th>\n",
              "      <th>X163</th>\n",
              "      <th>X164</th>\n",
              "      <th>X165</th>\n",
              "      <th>X166</th>\n",
              "      <th>X167</th>\n",
              "      <th>X168</th>\n",
              "      <th>X169</th>\n",
              "      <th>X170</th>\n",
              "      <th>X171</th>\n",
              "      <th>X172</th>\n",
              "      <th>X173</th>\n",
              "      <th>X174</th>\n",
              "      <th>X175</th>\n",
              "      <th>X176</th>\n",
              "      <th>X177</th>\n",
              "      <th>X178</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>X21.V1.791</td>\n",
              "      <td>135</td>\n",
              "      <td>190</td>\n",
              "      <td>229</td>\n",
              "      <td>223</td>\n",
              "      <td>192</td>\n",
              "      <td>125</td>\n",
              "      <td>55</td>\n",
              "      <td>-9</td>\n",
              "      <td>-33</td>\n",
              "      <td>-38</td>\n",
              "      <td>-10</td>\n",
              "      <td>35</td>\n",
              "      <td>64</td>\n",
              "      <td>113</td>\n",
              "      <td>152</td>\n",
              "      <td>164</td>\n",
              "      <td>127</td>\n",
              "      <td>50</td>\n",
              "      <td>-47</td>\n",
              "      <td>-121</td>\n",
              "      <td>-138</td>\n",
              "      <td>-125</td>\n",
              "      <td>-101</td>\n",
              "      <td>-50</td>\n",
              "      <td>11</td>\n",
              "      <td>39</td>\n",
              "      <td>24</td>\n",
              "      <td>48</td>\n",
              "      <td>64</td>\n",
              "      <td>46</td>\n",
              "      <td>13</td>\n",
              "      <td>-19</td>\n",
              "      <td>-61</td>\n",
              "      <td>-96</td>\n",
              "      <td>-130</td>\n",
              "      <td>-132</td>\n",
              "      <td>-116</td>\n",
              "      <td>-115</td>\n",
              "      <td>-71</td>\n",
              "      <td>...</td>\n",
              "      <td>54</td>\n",
              "      <td>90</td>\n",
              "      <td>111</td>\n",
              "      <td>107</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>18</td>\n",
              "      <td>-25</td>\n",
              "      <td>-69</td>\n",
              "      <td>-65</td>\n",
              "      <td>-44</td>\n",
              "      <td>-33</td>\n",
              "      <td>-57</td>\n",
              "      <td>-88</td>\n",
              "      <td>-114</td>\n",
              "      <td>-130</td>\n",
              "      <td>-114</td>\n",
              "      <td>-83</td>\n",
              "      <td>-53</td>\n",
              "      <td>-79</td>\n",
              "      <td>-72</td>\n",
              "      <td>-85</td>\n",
              "      <td>-109</td>\n",
              "      <td>-98</td>\n",
              "      <td>-72</td>\n",
              "      <td>-65</td>\n",
              "      <td>-63</td>\n",
              "      <td>-11</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>-17</td>\n",
              "      <td>-15</td>\n",
              "      <td>-31</td>\n",
              "      <td>-77</td>\n",
              "      <td>-103</td>\n",
              "      <td>-127</td>\n",
              "      <td>-116</td>\n",
              "      <td>-83</td>\n",
              "      <td>-51</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>X15.V1.924</td>\n",
              "      <td>386</td>\n",
              "      <td>382</td>\n",
              "      <td>356</td>\n",
              "      <td>331</td>\n",
              "      <td>320</td>\n",
              "      <td>315</td>\n",
              "      <td>307</td>\n",
              "      <td>272</td>\n",
              "      <td>244</td>\n",
              "      <td>232</td>\n",
              "      <td>237</td>\n",
              "      <td>258</td>\n",
              "      <td>212</td>\n",
              "      <td>2</td>\n",
              "      <td>-267</td>\n",
              "      <td>-605</td>\n",
              "      <td>-850</td>\n",
              "      <td>-1001</td>\n",
              "      <td>-1109</td>\n",
              "      <td>-1090</td>\n",
              "      <td>-967</td>\n",
              "      <td>-746</td>\n",
              "      <td>-464</td>\n",
              "      <td>-152</td>\n",
              "      <td>118</td>\n",
              "      <td>318</td>\n",
              "      <td>427</td>\n",
              "      <td>473</td>\n",
              "      <td>485</td>\n",
              "      <td>447</td>\n",
              "      <td>397</td>\n",
              "      <td>339</td>\n",
              "      <td>312</td>\n",
              "      <td>314</td>\n",
              "      <td>326</td>\n",
              "      <td>335</td>\n",
              "      <td>332</td>\n",
              "      <td>324</td>\n",
              "      <td>310</td>\n",
              "      <td>...</td>\n",
              "      <td>27</td>\n",
              "      <td>146</td>\n",
              "      <td>229</td>\n",
              "      <td>269</td>\n",
              "      <td>297</td>\n",
              "      <td>307</td>\n",
              "      <td>303</td>\n",
              "      <td>305</td>\n",
              "      <td>306</td>\n",
              "      <td>307</td>\n",
              "      <td>280</td>\n",
              "      <td>231</td>\n",
              "      <td>159</td>\n",
              "      <td>85</td>\n",
              "      <td>51</td>\n",
              "      <td>43</td>\n",
              "      <td>62</td>\n",
              "      <td>63</td>\n",
              "      <td>63</td>\n",
              "      <td>69</td>\n",
              "      <td>89</td>\n",
              "      <td>123</td>\n",
              "      <td>136</td>\n",
              "      <td>127</td>\n",
              "      <td>102</td>\n",
              "      <td>95</td>\n",
              "      <td>105</td>\n",
              "      <td>131</td>\n",
              "      <td>163</td>\n",
              "      <td>168</td>\n",
              "      <td>164</td>\n",
              "      <td>150</td>\n",
              "      <td>146</td>\n",
              "      <td>152</td>\n",
              "      <td>157</td>\n",
              "      <td>156</td>\n",
              "      <td>154</td>\n",
              "      <td>143</td>\n",
              "      <td>129</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>X8.V1.1</td>\n",
              "      <td>-32</td>\n",
              "      <td>-39</td>\n",
              "      <td>-47</td>\n",
              "      <td>-37</td>\n",
              "      <td>-32</td>\n",
              "      <td>-36</td>\n",
              "      <td>-57</td>\n",
              "      <td>-73</td>\n",
              "      <td>-85</td>\n",
              "      <td>-94</td>\n",
              "      <td>-99</td>\n",
              "      <td>-94</td>\n",
              "      <td>-96</td>\n",
              "      <td>-104</td>\n",
              "      <td>-103</td>\n",
              "      <td>-92</td>\n",
              "      <td>-75</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-53</td>\n",
              "      <td>-37</td>\n",
              "      <td>-14</td>\n",
              "      <td>-10</td>\n",
              "      <td>-39</td>\n",
              "      <td>-78</td>\n",
              "      <td>-102</td>\n",
              "      <td>-98</td>\n",
              "      <td>-80</td>\n",
              "      <td>-54</td>\n",
              "      <td>-40</td>\n",
              "      <td>-35</td>\n",
              "      <td>-39</td>\n",
              "      <td>-32</td>\n",
              "      <td>-13</td>\n",
              "      <td>7</td>\n",
              "      <td>34</td>\n",
              "      <td>41</td>\n",
              "      <td>33</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>-82</td>\n",
              "      <td>-107</td>\n",
              "      <td>-126</td>\n",
              "      <td>-124</td>\n",
              "      <td>-108</td>\n",
              "      <td>-84</td>\n",
              "      <td>-68</td>\n",
              "      <td>-61</td>\n",
              "      <td>-56</td>\n",
              "      <td>-63</td>\n",
              "      <td>-62</td>\n",
              "      <td>-33</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>45</td>\n",
              "      <td>37</td>\n",
              "      <td>48</td>\n",
              "      <td>62</td>\n",
              "      <td>80</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>-11</td>\n",
              "      <td>-39</td>\n",
              "      <td>-44</td>\n",
              "      <td>-42</td>\n",
              "      <td>-45</td>\n",
              "      <td>-48</td>\n",
              "      <td>-42</td>\n",
              "      <td>-6</td>\n",
              "      <td>29</td>\n",
              "      <td>57</td>\n",
              "      <td>64</td>\n",
              "      <td>48</td>\n",
              "      <td>19</td>\n",
              "      <td>-12</td>\n",
              "      <td>-30</td>\n",
              "      <td>-35</td>\n",
              "      <td>-35</td>\n",
              "      <td>-36</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>X16.V1.60</td>\n",
              "      <td>-105</td>\n",
              "      <td>-101</td>\n",
              "      <td>-96</td>\n",
              "      <td>-92</td>\n",
              "      <td>-89</td>\n",
              "      <td>-95</td>\n",
              "      <td>-102</td>\n",
              "      <td>-100</td>\n",
              "      <td>-87</td>\n",
              "      <td>-79</td>\n",
              "      <td>-72</td>\n",
              "      <td>-68</td>\n",
              "      <td>-74</td>\n",
              "      <td>-80</td>\n",
              "      <td>-83</td>\n",
              "      <td>-73</td>\n",
              "      <td>-68</td>\n",
              "      <td>-61</td>\n",
              "      <td>-58</td>\n",
              "      <td>-59</td>\n",
              "      <td>-64</td>\n",
              "      <td>-79</td>\n",
              "      <td>-84</td>\n",
              "      <td>-97</td>\n",
              "      <td>-94</td>\n",
              "      <td>-84</td>\n",
              "      <td>-77</td>\n",
              "      <td>-75</td>\n",
              "      <td>-72</td>\n",
              "      <td>-68</td>\n",
              "      <td>-76</td>\n",
              "      <td>-76</td>\n",
              "      <td>-72</td>\n",
              "      <td>-67</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-67</td>\n",
              "      <td>-68</td>\n",
              "      <td>...</td>\n",
              "      <td>-69</td>\n",
              "      <td>-66</td>\n",
              "      <td>-74</td>\n",
              "      <td>-69</td>\n",
              "      <td>-61</td>\n",
              "      <td>-51</td>\n",
              "      <td>-45</td>\n",
              "      <td>-45</td>\n",
              "      <td>-49</td>\n",
              "      <td>-58</td>\n",
              "      <td>-64</td>\n",
              "      <td>-78</td>\n",
              "      <td>-80</td>\n",
              "      <td>-90</td>\n",
              "      <td>-87</td>\n",
              "      <td>-83</td>\n",
              "      <td>-78</td>\n",
              "      <td>-64</td>\n",
              "      <td>-38</td>\n",
              "      <td>-22</td>\n",
              "      <td>-29</td>\n",
              "      <td>-42</td>\n",
              "      <td>-51</td>\n",
              "      <td>-68</td>\n",
              "      <td>-71</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-74</td>\n",
              "      <td>-74</td>\n",
              "      <td>-80</td>\n",
              "      <td>-82</td>\n",
              "      <td>-81</td>\n",
              "      <td>-80</td>\n",
              "      <td>-77</td>\n",
              "      <td>-85</td>\n",
              "      <td>-77</td>\n",
              "      <td>-72</td>\n",
              "      <td>-69</td>\n",
              "      <td>-65</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>X20.V1.54</td>\n",
              "      <td>-9</td>\n",
              "      <td>-65</td>\n",
              "      <td>-98</td>\n",
              "      <td>-102</td>\n",
              "      <td>-78</td>\n",
              "      <td>-48</td>\n",
              "      <td>-16</td>\n",
              "      <td>0</td>\n",
              "      <td>-21</td>\n",
              "      <td>-59</td>\n",
              "      <td>-90</td>\n",
              "      <td>-103</td>\n",
              "      <td>-84</td>\n",
              "      <td>-43</td>\n",
              "      <td>-9</td>\n",
              "      <td>3</td>\n",
              "      <td>-21</td>\n",
              "      <td>-60</td>\n",
              "      <td>-96</td>\n",
              "      <td>-103</td>\n",
              "      <td>-75</td>\n",
              "      <td>-29</td>\n",
              "      <td>14</td>\n",
              "      <td>55</td>\n",
              "      <td>78</td>\n",
              "      <td>73</td>\n",
              "      <td>28</td>\n",
              "      <td>-13</td>\n",
              "      <td>-43</td>\n",
              "      <td>-68</td>\n",
              "      <td>-78</td>\n",
              "      <td>-75</td>\n",
              "      <td>-55</td>\n",
              "      <td>-41</td>\n",
              "      <td>-19</td>\n",
              "      <td>-20</td>\n",
              "      <td>-29</td>\n",
              "      <td>-36</td>\n",
              "      <td>-20</td>\n",
              "      <td>...</td>\n",
              "      <td>38</td>\n",
              "      <td>26</td>\n",
              "      <td>10</td>\n",
              "      <td>-4</td>\n",
              "      <td>-13</td>\n",
              "      <td>-8</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>19</td>\n",
              "      <td>29</td>\n",
              "      <td>57</td>\n",
              "      <td>63</td>\n",
              "      <td>45</td>\n",
              "      <td>7</td>\n",
              "      <td>-13</td>\n",
              "      <td>-23</td>\n",
              "      <td>-9</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>-2</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>27</td>\n",
              "      <td>27</td>\n",
              "      <td>14</td>\n",
              "      <td>15</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>-12</td>\n",
              "      <td>-32</td>\n",
              "      <td>-41</td>\n",
              "      <td>-65</td>\n",
              "      <td>-83</td>\n",
              "      <td>-89</td>\n",
              "      <td>-73</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 180 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0   X1   X2   X3   X4   X5  ...  X174  X175  X176  X177  X178  y\n",
              "0  X21.V1.791  135  190  229  223  192  ...  -103  -127  -116   -83   -51  4\n",
              "1  X15.V1.924  386  382  356  331  320  ...   157   156   154   143   129  1\n",
              "2     X8.V1.1  -32  -39  -47  -37  -32  ...   -12   -30   -35   -35   -36  5\n",
              "3   X16.V1.60 -105 -101  -96  -92  -89  ...   -85   -77   -72   -69   -65  5\n",
              "4   X20.V1.54   -9  -65  -98 -102  -78  ...   -41   -65   -83   -89   -73  5\n",
              "\n",
              "[5 rows x 180 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAlFjR7i4ZQ-"
      },
      "source": [
        "# Drop the first column of the dataframe\n",
        "df.drop(df.columns[0], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "6xJW-yPg4ZQ-",
        "outputId": "ee393f7e-1772-4d11-942d-f726eb036653"
      },
      "source": [
        "# Get the feature values\n",
        "feature_data = df.iloc[:,0:-1]\n",
        "feature_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "      <th>X4</th>\n",
              "      <th>X5</th>\n",
              "      <th>X6</th>\n",
              "      <th>X7</th>\n",
              "      <th>X8</th>\n",
              "      <th>X9</th>\n",
              "      <th>X10</th>\n",
              "      <th>X11</th>\n",
              "      <th>X12</th>\n",
              "      <th>X13</th>\n",
              "      <th>X14</th>\n",
              "      <th>X15</th>\n",
              "      <th>X16</th>\n",
              "      <th>X17</th>\n",
              "      <th>X18</th>\n",
              "      <th>X19</th>\n",
              "      <th>X20</th>\n",
              "      <th>X21</th>\n",
              "      <th>X22</th>\n",
              "      <th>X23</th>\n",
              "      <th>X24</th>\n",
              "      <th>X25</th>\n",
              "      <th>X26</th>\n",
              "      <th>X27</th>\n",
              "      <th>X28</th>\n",
              "      <th>X29</th>\n",
              "      <th>X30</th>\n",
              "      <th>X31</th>\n",
              "      <th>X32</th>\n",
              "      <th>X33</th>\n",
              "      <th>X34</th>\n",
              "      <th>X35</th>\n",
              "      <th>X36</th>\n",
              "      <th>X37</th>\n",
              "      <th>X38</th>\n",
              "      <th>X39</th>\n",
              "      <th>X40</th>\n",
              "      <th>...</th>\n",
              "      <th>X139</th>\n",
              "      <th>X140</th>\n",
              "      <th>X141</th>\n",
              "      <th>X142</th>\n",
              "      <th>X143</th>\n",
              "      <th>X144</th>\n",
              "      <th>X145</th>\n",
              "      <th>X146</th>\n",
              "      <th>X147</th>\n",
              "      <th>X148</th>\n",
              "      <th>X149</th>\n",
              "      <th>X150</th>\n",
              "      <th>X151</th>\n",
              "      <th>X152</th>\n",
              "      <th>X153</th>\n",
              "      <th>X154</th>\n",
              "      <th>X155</th>\n",
              "      <th>X156</th>\n",
              "      <th>X157</th>\n",
              "      <th>X158</th>\n",
              "      <th>X159</th>\n",
              "      <th>X160</th>\n",
              "      <th>X161</th>\n",
              "      <th>X162</th>\n",
              "      <th>X163</th>\n",
              "      <th>X164</th>\n",
              "      <th>X165</th>\n",
              "      <th>X166</th>\n",
              "      <th>X167</th>\n",
              "      <th>X168</th>\n",
              "      <th>X169</th>\n",
              "      <th>X170</th>\n",
              "      <th>X171</th>\n",
              "      <th>X172</th>\n",
              "      <th>X173</th>\n",
              "      <th>X174</th>\n",
              "      <th>X175</th>\n",
              "      <th>X176</th>\n",
              "      <th>X177</th>\n",
              "      <th>X178</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>135</td>\n",
              "      <td>190</td>\n",
              "      <td>229</td>\n",
              "      <td>223</td>\n",
              "      <td>192</td>\n",
              "      <td>125</td>\n",
              "      <td>55</td>\n",
              "      <td>-9</td>\n",
              "      <td>-33</td>\n",
              "      <td>-38</td>\n",
              "      <td>-10</td>\n",
              "      <td>35</td>\n",
              "      <td>64</td>\n",
              "      <td>113</td>\n",
              "      <td>152</td>\n",
              "      <td>164</td>\n",
              "      <td>127</td>\n",
              "      <td>50</td>\n",
              "      <td>-47</td>\n",
              "      <td>-121</td>\n",
              "      <td>-138</td>\n",
              "      <td>-125</td>\n",
              "      <td>-101</td>\n",
              "      <td>-50</td>\n",
              "      <td>11</td>\n",
              "      <td>39</td>\n",
              "      <td>24</td>\n",
              "      <td>48</td>\n",
              "      <td>64</td>\n",
              "      <td>46</td>\n",
              "      <td>13</td>\n",
              "      <td>-19</td>\n",
              "      <td>-61</td>\n",
              "      <td>-96</td>\n",
              "      <td>-130</td>\n",
              "      <td>-132</td>\n",
              "      <td>-116</td>\n",
              "      <td>-115</td>\n",
              "      <td>-71</td>\n",
              "      <td>-14</td>\n",
              "      <td>...</td>\n",
              "      <td>43</td>\n",
              "      <td>54</td>\n",
              "      <td>90</td>\n",
              "      <td>111</td>\n",
              "      <td>107</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>18</td>\n",
              "      <td>-25</td>\n",
              "      <td>-69</td>\n",
              "      <td>-65</td>\n",
              "      <td>-44</td>\n",
              "      <td>-33</td>\n",
              "      <td>-57</td>\n",
              "      <td>-88</td>\n",
              "      <td>-114</td>\n",
              "      <td>-130</td>\n",
              "      <td>-114</td>\n",
              "      <td>-83</td>\n",
              "      <td>-53</td>\n",
              "      <td>-79</td>\n",
              "      <td>-72</td>\n",
              "      <td>-85</td>\n",
              "      <td>-109</td>\n",
              "      <td>-98</td>\n",
              "      <td>-72</td>\n",
              "      <td>-65</td>\n",
              "      <td>-63</td>\n",
              "      <td>-11</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>-17</td>\n",
              "      <td>-15</td>\n",
              "      <td>-31</td>\n",
              "      <td>-77</td>\n",
              "      <td>-103</td>\n",
              "      <td>-127</td>\n",
              "      <td>-116</td>\n",
              "      <td>-83</td>\n",
              "      <td>-51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>386</td>\n",
              "      <td>382</td>\n",
              "      <td>356</td>\n",
              "      <td>331</td>\n",
              "      <td>320</td>\n",
              "      <td>315</td>\n",
              "      <td>307</td>\n",
              "      <td>272</td>\n",
              "      <td>244</td>\n",
              "      <td>232</td>\n",
              "      <td>237</td>\n",
              "      <td>258</td>\n",
              "      <td>212</td>\n",
              "      <td>2</td>\n",
              "      <td>-267</td>\n",
              "      <td>-605</td>\n",
              "      <td>-850</td>\n",
              "      <td>-1001</td>\n",
              "      <td>-1109</td>\n",
              "      <td>-1090</td>\n",
              "      <td>-967</td>\n",
              "      <td>-746</td>\n",
              "      <td>-464</td>\n",
              "      <td>-152</td>\n",
              "      <td>118</td>\n",
              "      <td>318</td>\n",
              "      <td>427</td>\n",
              "      <td>473</td>\n",
              "      <td>485</td>\n",
              "      <td>447</td>\n",
              "      <td>397</td>\n",
              "      <td>339</td>\n",
              "      <td>312</td>\n",
              "      <td>314</td>\n",
              "      <td>326</td>\n",
              "      <td>335</td>\n",
              "      <td>332</td>\n",
              "      <td>324</td>\n",
              "      <td>310</td>\n",
              "      <td>312</td>\n",
              "      <td>...</td>\n",
              "      <td>-136</td>\n",
              "      <td>27</td>\n",
              "      <td>146</td>\n",
              "      <td>229</td>\n",
              "      <td>269</td>\n",
              "      <td>297</td>\n",
              "      <td>307</td>\n",
              "      <td>303</td>\n",
              "      <td>305</td>\n",
              "      <td>306</td>\n",
              "      <td>307</td>\n",
              "      <td>280</td>\n",
              "      <td>231</td>\n",
              "      <td>159</td>\n",
              "      <td>85</td>\n",
              "      <td>51</td>\n",
              "      <td>43</td>\n",
              "      <td>62</td>\n",
              "      <td>63</td>\n",
              "      <td>63</td>\n",
              "      <td>69</td>\n",
              "      <td>89</td>\n",
              "      <td>123</td>\n",
              "      <td>136</td>\n",
              "      <td>127</td>\n",
              "      <td>102</td>\n",
              "      <td>95</td>\n",
              "      <td>105</td>\n",
              "      <td>131</td>\n",
              "      <td>163</td>\n",
              "      <td>168</td>\n",
              "      <td>164</td>\n",
              "      <td>150</td>\n",
              "      <td>146</td>\n",
              "      <td>152</td>\n",
              "      <td>157</td>\n",
              "      <td>156</td>\n",
              "      <td>154</td>\n",
              "      <td>143</td>\n",
              "      <td>129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-32</td>\n",
              "      <td>-39</td>\n",
              "      <td>-47</td>\n",
              "      <td>-37</td>\n",
              "      <td>-32</td>\n",
              "      <td>-36</td>\n",
              "      <td>-57</td>\n",
              "      <td>-73</td>\n",
              "      <td>-85</td>\n",
              "      <td>-94</td>\n",
              "      <td>-99</td>\n",
              "      <td>-94</td>\n",
              "      <td>-96</td>\n",
              "      <td>-104</td>\n",
              "      <td>-103</td>\n",
              "      <td>-92</td>\n",
              "      <td>-75</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-53</td>\n",
              "      <td>-37</td>\n",
              "      <td>-14</td>\n",
              "      <td>-10</td>\n",
              "      <td>-39</td>\n",
              "      <td>-78</td>\n",
              "      <td>-102</td>\n",
              "      <td>-98</td>\n",
              "      <td>-80</td>\n",
              "      <td>-54</td>\n",
              "      <td>-40</td>\n",
              "      <td>-35</td>\n",
              "      <td>-39</td>\n",
              "      <td>-32</td>\n",
              "      <td>-13</td>\n",
              "      <td>7</td>\n",
              "      <td>34</td>\n",
              "      <td>41</td>\n",
              "      <td>33</td>\n",
              "      <td>6</td>\n",
              "      <td>-15</td>\n",
              "      <td>...</td>\n",
              "      <td>-54</td>\n",
              "      <td>-82</td>\n",
              "      <td>-107</td>\n",
              "      <td>-126</td>\n",
              "      <td>-124</td>\n",
              "      <td>-108</td>\n",
              "      <td>-84</td>\n",
              "      <td>-68</td>\n",
              "      <td>-61</td>\n",
              "      <td>-56</td>\n",
              "      <td>-63</td>\n",
              "      <td>-62</td>\n",
              "      <td>-33</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>45</td>\n",
              "      <td>37</td>\n",
              "      <td>48</td>\n",
              "      <td>62</td>\n",
              "      <td>80</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>-11</td>\n",
              "      <td>-39</td>\n",
              "      <td>-44</td>\n",
              "      <td>-42</td>\n",
              "      <td>-45</td>\n",
              "      <td>-48</td>\n",
              "      <td>-42</td>\n",
              "      <td>-6</td>\n",
              "      <td>29</td>\n",
              "      <td>57</td>\n",
              "      <td>64</td>\n",
              "      <td>48</td>\n",
              "      <td>19</td>\n",
              "      <td>-12</td>\n",
              "      <td>-30</td>\n",
              "      <td>-35</td>\n",
              "      <td>-35</td>\n",
              "      <td>-36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-105</td>\n",
              "      <td>-101</td>\n",
              "      <td>-96</td>\n",
              "      <td>-92</td>\n",
              "      <td>-89</td>\n",
              "      <td>-95</td>\n",
              "      <td>-102</td>\n",
              "      <td>-100</td>\n",
              "      <td>-87</td>\n",
              "      <td>-79</td>\n",
              "      <td>-72</td>\n",
              "      <td>-68</td>\n",
              "      <td>-74</td>\n",
              "      <td>-80</td>\n",
              "      <td>-83</td>\n",
              "      <td>-73</td>\n",
              "      <td>-68</td>\n",
              "      <td>-61</td>\n",
              "      <td>-58</td>\n",
              "      <td>-59</td>\n",
              "      <td>-64</td>\n",
              "      <td>-79</td>\n",
              "      <td>-84</td>\n",
              "      <td>-97</td>\n",
              "      <td>-94</td>\n",
              "      <td>-84</td>\n",
              "      <td>-77</td>\n",
              "      <td>-75</td>\n",
              "      <td>-72</td>\n",
              "      <td>-68</td>\n",
              "      <td>-76</td>\n",
              "      <td>-76</td>\n",
              "      <td>-72</td>\n",
              "      <td>-67</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-67</td>\n",
              "      <td>-68</td>\n",
              "      <td>-69</td>\n",
              "      <td>...</td>\n",
              "      <td>-61</td>\n",
              "      <td>-69</td>\n",
              "      <td>-66</td>\n",
              "      <td>-74</td>\n",
              "      <td>-69</td>\n",
              "      <td>-61</td>\n",
              "      <td>-51</td>\n",
              "      <td>-45</td>\n",
              "      <td>-45</td>\n",
              "      <td>-49</td>\n",
              "      <td>-58</td>\n",
              "      <td>-64</td>\n",
              "      <td>-78</td>\n",
              "      <td>-80</td>\n",
              "      <td>-90</td>\n",
              "      <td>-87</td>\n",
              "      <td>-83</td>\n",
              "      <td>-78</td>\n",
              "      <td>-64</td>\n",
              "      <td>-38</td>\n",
              "      <td>-22</td>\n",
              "      <td>-29</td>\n",
              "      <td>-42</td>\n",
              "      <td>-51</td>\n",
              "      <td>-68</td>\n",
              "      <td>-71</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-74</td>\n",
              "      <td>-74</td>\n",
              "      <td>-80</td>\n",
              "      <td>-82</td>\n",
              "      <td>-81</td>\n",
              "      <td>-80</td>\n",
              "      <td>-77</td>\n",
              "      <td>-85</td>\n",
              "      <td>-77</td>\n",
              "      <td>-72</td>\n",
              "      <td>-69</td>\n",
              "      <td>-65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-9</td>\n",
              "      <td>-65</td>\n",
              "      <td>-98</td>\n",
              "      <td>-102</td>\n",
              "      <td>-78</td>\n",
              "      <td>-48</td>\n",
              "      <td>-16</td>\n",
              "      <td>0</td>\n",
              "      <td>-21</td>\n",
              "      <td>-59</td>\n",
              "      <td>-90</td>\n",
              "      <td>-103</td>\n",
              "      <td>-84</td>\n",
              "      <td>-43</td>\n",
              "      <td>-9</td>\n",
              "      <td>3</td>\n",
              "      <td>-21</td>\n",
              "      <td>-60</td>\n",
              "      <td>-96</td>\n",
              "      <td>-103</td>\n",
              "      <td>-75</td>\n",
              "      <td>-29</td>\n",
              "      <td>14</td>\n",
              "      <td>55</td>\n",
              "      <td>78</td>\n",
              "      <td>73</td>\n",
              "      <td>28</td>\n",
              "      <td>-13</td>\n",
              "      <td>-43</td>\n",
              "      <td>-68</td>\n",
              "      <td>-78</td>\n",
              "      <td>-75</td>\n",
              "      <td>-55</td>\n",
              "      <td>-41</td>\n",
              "      <td>-19</td>\n",
              "      <td>-20</td>\n",
              "      <td>-29</td>\n",
              "      <td>-36</td>\n",
              "      <td>-20</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>37</td>\n",
              "      <td>38</td>\n",
              "      <td>26</td>\n",
              "      <td>10</td>\n",
              "      <td>-4</td>\n",
              "      <td>-13</td>\n",
              "      <td>-8</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>19</td>\n",
              "      <td>29</td>\n",
              "      <td>57</td>\n",
              "      <td>63</td>\n",
              "      <td>45</td>\n",
              "      <td>7</td>\n",
              "      <td>-13</td>\n",
              "      <td>-23</td>\n",
              "      <td>-9</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>-2</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>27</td>\n",
              "      <td>27</td>\n",
              "      <td>14</td>\n",
              "      <td>15</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>-12</td>\n",
              "      <td>-32</td>\n",
              "      <td>-41</td>\n",
              "      <td>-65</td>\n",
              "      <td>-83</td>\n",
              "      <td>-89</td>\n",
              "      <td>-73</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 178 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    X1   X2   X3   X4   X5   X6   X7  ...  X172  X173  X174  X175  X176  X177  X178\n",
              "0  135  190  229  223  192  125   55  ...   -31   -77  -103  -127  -116   -83   -51\n",
              "1  386  382  356  331  320  315  307  ...   146   152   157   156   154   143   129\n",
              "2  -32  -39  -47  -37  -32  -36  -57  ...    48    19   -12   -30   -35   -35   -36\n",
              "3 -105 -101  -96  -92  -89  -95 -102  ...   -80   -77   -85   -77   -72   -69   -65\n",
              "4   -9  -65  -98 -102  -78  -48  -16  ...   -12   -32   -41   -65   -83   -89   -73\n",
              "\n",
              "[5 rows x 178 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "AsjiM5MX4ZQ_",
        "outputId": "65741407-9b88-4823-8de7-f8b0bd9028aa"
      },
      "source": [
        "#Mapping Target for one V/S all logistic classification\n",
        "df['y'] = df.y.map({1: 1, 2: 0, 3: 0, 4: 0, 5:0}) # Checking for presence of CLass 1\n",
        "df.head(5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "      <th>X4</th>\n",
              "      <th>X5</th>\n",
              "      <th>X6</th>\n",
              "      <th>X7</th>\n",
              "      <th>X8</th>\n",
              "      <th>X9</th>\n",
              "      <th>X10</th>\n",
              "      <th>X11</th>\n",
              "      <th>X12</th>\n",
              "      <th>X13</th>\n",
              "      <th>X14</th>\n",
              "      <th>X15</th>\n",
              "      <th>X16</th>\n",
              "      <th>X17</th>\n",
              "      <th>X18</th>\n",
              "      <th>X19</th>\n",
              "      <th>X20</th>\n",
              "      <th>X21</th>\n",
              "      <th>X22</th>\n",
              "      <th>X23</th>\n",
              "      <th>X24</th>\n",
              "      <th>X25</th>\n",
              "      <th>X26</th>\n",
              "      <th>X27</th>\n",
              "      <th>X28</th>\n",
              "      <th>X29</th>\n",
              "      <th>X30</th>\n",
              "      <th>X31</th>\n",
              "      <th>X32</th>\n",
              "      <th>X33</th>\n",
              "      <th>X34</th>\n",
              "      <th>X35</th>\n",
              "      <th>X36</th>\n",
              "      <th>X37</th>\n",
              "      <th>X38</th>\n",
              "      <th>X39</th>\n",
              "      <th>X40</th>\n",
              "      <th>...</th>\n",
              "      <th>X140</th>\n",
              "      <th>X141</th>\n",
              "      <th>X142</th>\n",
              "      <th>X143</th>\n",
              "      <th>X144</th>\n",
              "      <th>X145</th>\n",
              "      <th>X146</th>\n",
              "      <th>X147</th>\n",
              "      <th>X148</th>\n",
              "      <th>X149</th>\n",
              "      <th>X150</th>\n",
              "      <th>X151</th>\n",
              "      <th>X152</th>\n",
              "      <th>X153</th>\n",
              "      <th>X154</th>\n",
              "      <th>X155</th>\n",
              "      <th>X156</th>\n",
              "      <th>X157</th>\n",
              "      <th>X158</th>\n",
              "      <th>X159</th>\n",
              "      <th>X160</th>\n",
              "      <th>X161</th>\n",
              "      <th>X162</th>\n",
              "      <th>X163</th>\n",
              "      <th>X164</th>\n",
              "      <th>X165</th>\n",
              "      <th>X166</th>\n",
              "      <th>X167</th>\n",
              "      <th>X168</th>\n",
              "      <th>X169</th>\n",
              "      <th>X170</th>\n",
              "      <th>X171</th>\n",
              "      <th>X172</th>\n",
              "      <th>X173</th>\n",
              "      <th>X174</th>\n",
              "      <th>X175</th>\n",
              "      <th>X176</th>\n",
              "      <th>X177</th>\n",
              "      <th>X178</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>135</td>\n",
              "      <td>190</td>\n",
              "      <td>229</td>\n",
              "      <td>223</td>\n",
              "      <td>192</td>\n",
              "      <td>125</td>\n",
              "      <td>55</td>\n",
              "      <td>-9</td>\n",
              "      <td>-33</td>\n",
              "      <td>-38</td>\n",
              "      <td>-10</td>\n",
              "      <td>35</td>\n",
              "      <td>64</td>\n",
              "      <td>113</td>\n",
              "      <td>152</td>\n",
              "      <td>164</td>\n",
              "      <td>127</td>\n",
              "      <td>50</td>\n",
              "      <td>-47</td>\n",
              "      <td>-121</td>\n",
              "      <td>-138</td>\n",
              "      <td>-125</td>\n",
              "      <td>-101</td>\n",
              "      <td>-50</td>\n",
              "      <td>11</td>\n",
              "      <td>39</td>\n",
              "      <td>24</td>\n",
              "      <td>48</td>\n",
              "      <td>64</td>\n",
              "      <td>46</td>\n",
              "      <td>13</td>\n",
              "      <td>-19</td>\n",
              "      <td>-61</td>\n",
              "      <td>-96</td>\n",
              "      <td>-130</td>\n",
              "      <td>-132</td>\n",
              "      <td>-116</td>\n",
              "      <td>-115</td>\n",
              "      <td>-71</td>\n",
              "      <td>-14</td>\n",
              "      <td>...</td>\n",
              "      <td>54</td>\n",
              "      <td>90</td>\n",
              "      <td>111</td>\n",
              "      <td>107</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>18</td>\n",
              "      <td>-25</td>\n",
              "      <td>-69</td>\n",
              "      <td>-65</td>\n",
              "      <td>-44</td>\n",
              "      <td>-33</td>\n",
              "      <td>-57</td>\n",
              "      <td>-88</td>\n",
              "      <td>-114</td>\n",
              "      <td>-130</td>\n",
              "      <td>-114</td>\n",
              "      <td>-83</td>\n",
              "      <td>-53</td>\n",
              "      <td>-79</td>\n",
              "      <td>-72</td>\n",
              "      <td>-85</td>\n",
              "      <td>-109</td>\n",
              "      <td>-98</td>\n",
              "      <td>-72</td>\n",
              "      <td>-65</td>\n",
              "      <td>-63</td>\n",
              "      <td>-11</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>-17</td>\n",
              "      <td>-15</td>\n",
              "      <td>-31</td>\n",
              "      <td>-77</td>\n",
              "      <td>-103</td>\n",
              "      <td>-127</td>\n",
              "      <td>-116</td>\n",
              "      <td>-83</td>\n",
              "      <td>-51</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>386</td>\n",
              "      <td>382</td>\n",
              "      <td>356</td>\n",
              "      <td>331</td>\n",
              "      <td>320</td>\n",
              "      <td>315</td>\n",
              "      <td>307</td>\n",
              "      <td>272</td>\n",
              "      <td>244</td>\n",
              "      <td>232</td>\n",
              "      <td>237</td>\n",
              "      <td>258</td>\n",
              "      <td>212</td>\n",
              "      <td>2</td>\n",
              "      <td>-267</td>\n",
              "      <td>-605</td>\n",
              "      <td>-850</td>\n",
              "      <td>-1001</td>\n",
              "      <td>-1109</td>\n",
              "      <td>-1090</td>\n",
              "      <td>-967</td>\n",
              "      <td>-746</td>\n",
              "      <td>-464</td>\n",
              "      <td>-152</td>\n",
              "      <td>118</td>\n",
              "      <td>318</td>\n",
              "      <td>427</td>\n",
              "      <td>473</td>\n",
              "      <td>485</td>\n",
              "      <td>447</td>\n",
              "      <td>397</td>\n",
              "      <td>339</td>\n",
              "      <td>312</td>\n",
              "      <td>314</td>\n",
              "      <td>326</td>\n",
              "      <td>335</td>\n",
              "      <td>332</td>\n",
              "      <td>324</td>\n",
              "      <td>310</td>\n",
              "      <td>312</td>\n",
              "      <td>...</td>\n",
              "      <td>27</td>\n",
              "      <td>146</td>\n",
              "      <td>229</td>\n",
              "      <td>269</td>\n",
              "      <td>297</td>\n",
              "      <td>307</td>\n",
              "      <td>303</td>\n",
              "      <td>305</td>\n",
              "      <td>306</td>\n",
              "      <td>307</td>\n",
              "      <td>280</td>\n",
              "      <td>231</td>\n",
              "      <td>159</td>\n",
              "      <td>85</td>\n",
              "      <td>51</td>\n",
              "      <td>43</td>\n",
              "      <td>62</td>\n",
              "      <td>63</td>\n",
              "      <td>63</td>\n",
              "      <td>69</td>\n",
              "      <td>89</td>\n",
              "      <td>123</td>\n",
              "      <td>136</td>\n",
              "      <td>127</td>\n",
              "      <td>102</td>\n",
              "      <td>95</td>\n",
              "      <td>105</td>\n",
              "      <td>131</td>\n",
              "      <td>163</td>\n",
              "      <td>168</td>\n",
              "      <td>164</td>\n",
              "      <td>150</td>\n",
              "      <td>146</td>\n",
              "      <td>152</td>\n",
              "      <td>157</td>\n",
              "      <td>156</td>\n",
              "      <td>154</td>\n",
              "      <td>143</td>\n",
              "      <td>129</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-32</td>\n",
              "      <td>-39</td>\n",
              "      <td>-47</td>\n",
              "      <td>-37</td>\n",
              "      <td>-32</td>\n",
              "      <td>-36</td>\n",
              "      <td>-57</td>\n",
              "      <td>-73</td>\n",
              "      <td>-85</td>\n",
              "      <td>-94</td>\n",
              "      <td>-99</td>\n",
              "      <td>-94</td>\n",
              "      <td>-96</td>\n",
              "      <td>-104</td>\n",
              "      <td>-103</td>\n",
              "      <td>-92</td>\n",
              "      <td>-75</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-53</td>\n",
              "      <td>-37</td>\n",
              "      <td>-14</td>\n",
              "      <td>-10</td>\n",
              "      <td>-39</td>\n",
              "      <td>-78</td>\n",
              "      <td>-102</td>\n",
              "      <td>-98</td>\n",
              "      <td>-80</td>\n",
              "      <td>-54</td>\n",
              "      <td>-40</td>\n",
              "      <td>-35</td>\n",
              "      <td>-39</td>\n",
              "      <td>-32</td>\n",
              "      <td>-13</td>\n",
              "      <td>7</td>\n",
              "      <td>34</td>\n",
              "      <td>41</td>\n",
              "      <td>33</td>\n",
              "      <td>6</td>\n",
              "      <td>-15</td>\n",
              "      <td>...</td>\n",
              "      <td>-82</td>\n",
              "      <td>-107</td>\n",
              "      <td>-126</td>\n",
              "      <td>-124</td>\n",
              "      <td>-108</td>\n",
              "      <td>-84</td>\n",
              "      <td>-68</td>\n",
              "      <td>-61</td>\n",
              "      <td>-56</td>\n",
              "      <td>-63</td>\n",
              "      <td>-62</td>\n",
              "      <td>-33</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>45</td>\n",
              "      <td>37</td>\n",
              "      <td>48</td>\n",
              "      <td>62</td>\n",
              "      <td>80</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>-11</td>\n",
              "      <td>-39</td>\n",
              "      <td>-44</td>\n",
              "      <td>-42</td>\n",
              "      <td>-45</td>\n",
              "      <td>-48</td>\n",
              "      <td>-42</td>\n",
              "      <td>-6</td>\n",
              "      <td>29</td>\n",
              "      <td>57</td>\n",
              "      <td>64</td>\n",
              "      <td>48</td>\n",
              "      <td>19</td>\n",
              "      <td>-12</td>\n",
              "      <td>-30</td>\n",
              "      <td>-35</td>\n",
              "      <td>-35</td>\n",
              "      <td>-36</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-105</td>\n",
              "      <td>-101</td>\n",
              "      <td>-96</td>\n",
              "      <td>-92</td>\n",
              "      <td>-89</td>\n",
              "      <td>-95</td>\n",
              "      <td>-102</td>\n",
              "      <td>-100</td>\n",
              "      <td>-87</td>\n",
              "      <td>-79</td>\n",
              "      <td>-72</td>\n",
              "      <td>-68</td>\n",
              "      <td>-74</td>\n",
              "      <td>-80</td>\n",
              "      <td>-83</td>\n",
              "      <td>-73</td>\n",
              "      <td>-68</td>\n",
              "      <td>-61</td>\n",
              "      <td>-58</td>\n",
              "      <td>-59</td>\n",
              "      <td>-64</td>\n",
              "      <td>-79</td>\n",
              "      <td>-84</td>\n",
              "      <td>-97</td>\n",
              "      <td>-94</td>\n",
              "      <td>-84</td>\n",
              "      <td>-77</td>\n",
              "      <td>-75</td>\n",
              "      <td>-72</td>\n",
              "      <td>-68</td>\n",
              "      <td>-76</td>\n",
              "      <td>-76</td>\n",
              "      <td>-72</td>\n",
              "      <td>-67</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-67</td>\n",
              "      <td>-68</td>\n",
              "      <td>-69</td>\n",
              "      <td>...</td>\n",
              "      <td>-69</td>\n",
              "      <td>-66</td>\n",
              "      <td>-74</td>\n",
              "      <td>-69</td>\n",
              "      <td>-61</td>\n",
              "      <td>-51</td>\n",
              "      <td>-45</td>\n",
              "      <td>-45</td>\n",
              "      <td>-49</td>\n",
              "      <td>-58</td>\n",
              "      <td>-64</td>\n",
              "      <td>-78</td>\n",
              "      <td>-80</td>\n",
              "      <td>-90</td>\n",
              "      <td>-87</td>\n",
              "      <td>-83</td>\n",
              "      <td>-78</td>\n",
              "      <td>-64</td>\n",
              "      <td>-38</td>\n",
              "      <td>-22</td>\n",
              "      <td>-29</td>\n",
              "      <td>-42</td>\n",
              "      <td>-51</td>\n",
              "      <td>-68</td>\n",
              "      <td>-71</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-74</td>\n",
              "      <td>-74</td>\n",
              "      <td>-80</td>\n",
              "      <td>-82</td>\n",
              "      <td>-81</td>\n",
              "      <td>-80</td>\n",
              "      <td>-77</td>\n",
              "      <td>-85</td>\n",
              "      <td>-77</td>\n",
              "      <td>-72</td>\n",
              "      <td>-69</td>\n",
              "      <td>-65</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-9</td>\n",
              "      <td>-65</td>\n",
              "      <td>-98</td>\n",
              "      <td>-102</td>\n",
              "      <td>-78</td>\n",
              "      <td>-48</td>\n",
              "      <td>-16</td>\n",
              "      <td>0</td>\n",
              "      <td>-21</td>\n",
              "      <td>-59</td>\n",
              "      <td>-90</td>\n",
              "      <td>-103</td>\n",
              "      <td>-84</td>\n",
              "      <td>-43</td>\n",
              "      <td>-9</td>\n",
              "      <td>3</td>\n",
              "      <td>-21</td>\n",
              "      <td>-60</td>\n",
              "      <td>-96</td>\n",
              "      <td>-103</td>\n",
              "      <td>-75</td>\n",
              "      <td>-29</td>\n",
              "      <td>14</td>\n",
              "      <td>55</td>\n",
              "      <td>78</td>\n",
              "      <td>73</td>\n",
              "      <td>28</td>\n",
              "      <td>-13</td>\n",
              "      <td>-43</td>\n",
              "      <td>-68</td>\n",
              "      <td>-78</td>\n",
              "      <td>-75</td>\n",
              "      <td>-55</td>\n",
              "      <td>-41</td>\n",
              "      <td>-19</td>\n",
              "      <td>-20</td>\n",
              "      <td>-29</td>\n",
              "      <td>-36</td>\n",
              "      <td>-20</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>38</td>\n",
              "      <td>26</td>\n",
              "      <td>10</td>\n",
              "      <td>-4</td>\n",
              "      <td>-13</td>\n",
              "      <td>-8</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>19</td>\n",
              "      <td>29</td>\n",
              "      <td>57</td>\n",
              "      <td>63</td>\n",
              "      <td>45</td>\n",
              "      <td>7</td>\n",
              "      <td>-13</td>\n",
              "      <td>-23</td>\n",
              "      <td>-9</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>-2</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>27</td>\n",
              "      <td>27</td>\n",
              "      <td>14</td>\n",
              "      <td>15</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>-12</td>\n",
              "      <td>-32</td>\n",
              "      <td>-41</td>\n",
              "      <td>-65</td>\n",
              "      <td>-83</td>\n",
              "      <td>-89</td>\n",
              "      <td>-73</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 179 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    X1   X2   X3   X4   X5   X6   X7  ...  X173  X174  X175  X176  X177  X178  y\n",
              "0  135  190  229  223  192  125   55  ...   -77  -103  -127  -116   -83   -51  0\n",
              "1  386  382  356  331  320  315  307  ...   152   157   156   154   143   129  1\n",
              "2  -32  -39  -47  -37  -32  -36  -57  ...    19   -12   -30   -35   -35   -36  0\n",
              "3 -105 -101  -96  -92  -89  -95 -102  ...   -77   -85   -77   -72   -69   -65  0\n",
              "4   -9  -65  -98 -102  -78  -48  -16  ...   -32   -41   -65   -83   -89   -73  0\n",
              "\n",
              "[5 rows x 179 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWR-dAJ44ZRA"
      },
      "source": [
        "train_set, test_set = train_test_split(df )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1OhklnU4ZRA",
        "outputId": "09c2dfb6-2acc-46a7-9ad6-6b49c4aee8e6"
      },
      "source": [
        "len(train_set) , len(test_set)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8625, 2875)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ocoay4u4ZRA",
        "outputId": "4ab84153-ddd1-4e0b-b16d-40334caa2a2b"
      },
      "source": [
        "knn = ClassKNN(k=3)\n",
        "preds = []\n",
        "\n",
        "for row in test_set:\n",
        "    predictors_only = row[:-1]\n",
        "    prediction = knn.predict(train_set, predictors_only)\n",
        "    preds.append(prediction)\n",
        "    \n",
        "actual = np.array(test_set)[:, -1]\n",
        "knn.evaluate(actual, preds)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9293913043478261"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0GfqziD4ZRA",
        "outputId": "00e76021-6bee-4030-faba-7f8a068bf8e3"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "tp, fn, fp, tn = confusion_matrix(actual, preds,labels=[1,0]).reshape(-1)\n",
        "print('Outcome values : \\n', tp, fn, fp, tn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Outcome values : \n",
            " 382 202 1 2290\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UOm9Qzanr4O"
      },
      "source": [
        "tpr, fpr = cal_TPR_FPR(tp, fn, fp, tn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrtEpx4kC0_c"
      },
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "\n",
        "\n",
        "disp = plot_precision_recall_curve(classifier, X_test, y_test)\n",
        "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
        "                   'AP={0:0.2f}'.format(average_precision))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmLDRcvf4ZRB"
      },
      "source": [
        "##### With boston data set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwBtABS44ZRB",
        "outputId": "fdc1ff1f-c67e-40ed-c67f-e89adb44437c"
      },
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"boston.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.887621</td>\n",
              "      <td>0.185875</td>\n",
              "      <td>2.299474</td>\n",
              "      <td>2.299474</td>\n",
              "      <td>0.581396</td>\n",
              "      <td>healthy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.783744</td>\n",
              "      <td>0.163794</td>\n",
              "      <td>2.299480</td>\n",
              "      <td>2.299480</td>\n",
              "      <td>0.588340</td>\n",
              "      <td>healthy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.875873</td>\n",
              "      <td>0.154714</td>\n",
              "      <td>2.299477</td>\n",
              "      <td>2.299477</td>\n",
              "      <td>0.589802</td>\n",
              "      <td>healthy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.881293</td>\n",
              "      <td>0.138396</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>0.592911</td>\n",
              "      <td>healthy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.798116</td>\n",
              "      <td>0.125291</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>0.595338</td>\n",
              "      <td>healthy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0        f1        f2        f3        f4        f5    class\n",
              "0           0  0.887621  0.185875  2.299474  2.299474  0.581396  healthy\n",
              "1           1  0.783744  0.163794  2.299480  2.299480  0.588340  healthy\n",
              "2           2  0.875873  0.154714  2.299477  2.299477  0.589802  healthy\n",
              "3           3  0.881293  0.138396  2.299481  2.299481  0.592911  healthy\n",
              "4           4  0.798116  0.125291  2.299481  2.299481  0.595338  healthy"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO4Mv5Ax4ZRB"
      },
      "source": [
        "df=df.iloc[:,1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HPXadYT4ZRB"
      },
      "source": [
        "df[\"Target\"] = df[\"class\"]\n",
        "\n",
        "df['Target'] = df.Target.map({\"seizure\": 1, \"healthy\": 0, \"transation\": 0}) # Checking for presence of CLass 1\n",
        "df.head(5)\n",
        "df.drop(df.columns[5], axis=1, inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8MeKaJx4ZRB",
        "outputId": "5fcdf250-d1ed-488d-f2b0-55d70c6c5d6d"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.185875</td>\n",
              "      <td>2.299474</td>\n",
              "      <td>2.299474</td>\n",
              "      <td>0.581396</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.163794</td>\n",
              "      <td>2.299480</td>\n",
              "      <td>2.299480</td>\n",
              "      <td>0.588340</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.154714</td>\n",
              "      <td>2.299477</td>\n",
              "      <td>2.299477</td>\n",
              "      <td>0.589802</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.138396</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>0.592911</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.125291</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>0.595338</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         f2        f3        f4        f5  Target\n",
              "0  0.185875  2.299474  2.299474  0.581396       0\n",
              "1  0.163794  2.299480  2.299480  0.588340       0\n",
              "2  0.154714  2.299477  2.299477  0.589802       0\n",
              "3  0.138396  2.299481  2.299481  0.592911       0\n",
              "4  0.125291  2.299481  2.299481  0.595338       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MG6ywCj4ZRB"
      },
      "source": [
        "train_set, test_set = train_test_split(df )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yufsHN3O4ZRC",
        "outputId": "93f52649-a3e2-45ab-dbce-689e97febb30"
      },
      "source": [
        "len(train_set) , len(test_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(225, 75)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hJ2lVxf4ZRC",
        "outputId": "0f00e320-ad67-4ef4-caea-b78b8eafb9ba"
      },
      "source": [
        "knn = ClassKNN(k=3)\n",
        "preds = []\n",
        "\n",
        "for row in test_set:\n",
        "    predictors_only = row[:-1]\n",
        "    prediction = knn.predict(train_set, predictors_only)\n",
        "    preds.append(prediction)\n",
        "    \n",
        "actual = np.array(test_set)[:, -1]\n",
        "knn.evaluate(actual, preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9866666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_9z0DXz4ZRC",
        "outputId": "230f2c25-e6a2-44a3-9e88-ae6b7aa91480"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "tp, fn, fp, tn = confusion_matrix(actual, preds,labels=[1,0]).reshape(-1)\n",
        "print('Outcome values : \\n', tp, fn, fp, tn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Outcome values : \n",
            " 20 7 4 44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgS5V4CDnEuT",
        "outputId": "dd6a0a2b-ab92-4f58-85a9-87c1bdc96984"
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "precision = precision_score(actual, preds)\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(actual, preds)\n",
        "print('Recall: %f' % recall)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 0.966667\n",
            "Recall: 1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smpd5HwTmnew",
        "outputId": "4f459422-0dca-4a75-9599-2e1336f15ab2"
      },
      "source": [
        "print(classification_report(actual, preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.98      0.99        46\n",
            "         1.0       0.97      1.00      0.98        29\n",
            "\n",
            "    accuracy                           0.99        75\n",
            "   macro avg       0.98      0.99      0.99        75\n",
            "weighted avg       0.99      0.99      0.99        75\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgKKzi3E4ZRC"
      },
      "source": [
        "##### With CHB MIT Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU7rgTeL4ZRC",
        "outputId": "b81c8506-836f-47ff-a0ae-30fc4b031041"
      },
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"chb.csv\")\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>T8-P8-1_n_peaks</th>\n",
              "      <th>T8-P8-1_n_crossings</th>\n",
              "      <th>T8-P8-1_hfd</th>\n",
              "      <th>T8-P8-1_pfd</th>\n",
              "      <th>T8-P8-1_hurst_exp</th>\n",
              "      <th>T8-P8-1_spectral_entropy</th>\n",
              "      <th>T8-P8-1_total_power</th>\n",
              "      <th>T8-P8-1_median_freq</th>\n",
              "      <th>T8-P8-1_peak_freq</th>\n",
              "      <th>T8-P8-1_hjorth_mobility</th>\n",
              "      <th>T8-P8-1_hjorth_complexity</th>\n",
              "      <th>T8-P8-1_power_1hz</th>\n",
              "      <th>T8-P8-1_power_5hz</th>\n",
              "      <th>T8-P8-1_power_10hz</th>\n",
              "      <th>T8-P8-1_power_15hz</th>\n",
              "      <th>T8-P8-1_power_20hz</th>\n",
              "      <th>seizure</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>183</td>\n",
              "      <td>153</td>\n",
              "      <td>0.094016</td>\n",
              "      <td>0.611024</td>\n",
              "      <td>0.879401</td>\n",
              "      <td>0.747444</td>\n",
              "      <td>5.090000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.002766</td>\n",
              "      <td>171.222337</td>\n",
              "      <td>0.420823</td>\n",
              "      <td>0.226051</td>\n",
              "      <td>0.128492</td>\n",
              "      <td>0.106838</td>\n",
              "      <td>0.117797</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>183</td>\n",
              "      <td>165</td>\n",
              "      <td>0.092275</td>\n",
              "      <td>0.611024</td>\n",
              "      <td>0.746361</td>\n",
              "      <td>0.765824</td>\n",
              "      <td>5.560000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003211</td>\n",
              "      <td>148.493351</td>\n",
              "      <td>0.384135</td>\n",
              "      <td>0.234295</td>\n",
              "      <td>0.140616</td>\n",
              "      <td>0.115382</td>\n",
              "      <td>0.125572</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>186</td>\n",
              "      <td>170</td>\n",
              "      <td>0.098374</td>\n",
              "      <td>0.610385</td>\n",
              "      <td>0.687973</td>\n",
              "      <td>0.752050</td>\n",
              "      <td>5.450000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003564</td>\n",
              "      <td>142.209197</td>\n",
              "      <td>0.389655</td>\n",
              "      <td>0.238327</td>\n",
              "      <td>0.124545</td>\n",
              "      <td>0.109260</td>\n",
              "      <td>0.138213</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>189</td>\n",
              "      <td>166</td>\n",
              "      <td>0.099552</td>\n",
              "      <td>0.609756</td>\n",
              "      <td>0.726399</td>\n",
              "      <td>0.754101</td>\n",
              "      <td>5.920000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003514</td>\n",
              "      <td>140.840795</td>\n",
              "      <td>0.396989</td>\n",
              "      <td>0.232559</td>\n",
              "      <td>0.120853</td>\n",
              "      <td>0.118132</td>\n",
              "      <td>0.131467</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>188</td>\n",
              "      <td>167</td>\n",
              "      <td>0.101196</td>\n",
              "      <td>0.609756</td>\n",
              "      <td>0.755442</td>\n",
              "      <td>0.758187</td>\n",
              "      <td>5.830000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003494</td>\n",
              "      <td>142.434220</td>\n",
              "      <td>0.390794</td>\n",
              "      <td>0.230665</td>\n",
              "      <td>0.125717</td>\n",
              "      <td>0.119646</td>\n",
              "      <td>0.133177</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   T8-P8-1_n_peaks  T8-P8-1_n_crossings  ...  T8-P8-1_power_20hz  seizure\n",
              "0              183                  153  ...            0.117797        0\n",
              "1              183                  165  ...            0.125572        0\n",
              "2              186                  170  ...            0.138213        0\n",
              "3              189                  166  ...            0.131467        0\n",
              "4              188                  167  ...            0.133177        0\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGa0RZl14ZRC"
      },
      "source": [
        "#df=df.iloc[:,1:]\n",
        "#df=df.drop(['subject'],axis=1)\n",
        "#df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhsmX-ya4ZRD"
      },
      "source": [
        "train_set, test_set = train_test_split(df )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89jN-kAh4ZRD",
        "outputId": "274248ad-3bed-4630-f29c-8c1b86fc9661"
      },
      "source": [
        "len(train_set) , len(test_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(750, 250)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aNh4eSbhW3B"
      },
      "source": [
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq3qKjrC4ZRD"
      },
      "source": [
        "knn = ClassKNN(k=3)\n",
        "preds = []\n",
        "start_time = time.time()\n",
        "for row in test_set:\n",
        "    predictors_only = row[:-1]\n",
        "    prediction = knn.predict(train_set, predictors_only)\n",
        "    preds.append(prediction)\n",
        "end_time = time.time()    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BP6Xl4NwhrvD",
        "outputId": "f0775a46-7df0-4514-fc63-4f466d5cbaf1"
      },
      "source": [
        "actual = np.array(test_set)[:, -1]\n",
        "knn.evaluate(actual, preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.984"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUW-FacM4ZRD"
      },
      "source": [
        "tp, fn, fp, tn = confusion_matrix(actual, preds,labels=[1,0]).reshape(-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmhLzhSz4ZRD"
      },
      "source": [
        "fpr, tpr, thresholds = metrics.roc_curve(actual, preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkbEfHJ_iRng"
      },
      "source": [
        "# Logistic  Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSdsu_24iRno"
      },
      "source": [
        "def SplitData(xdata, ydata, test_s):\n",
        "    \"\"\"\n",
        "    This function splits the features data and target data into test and training data as per the test size provided.\n",
        "    \"\"\"\n",
        "    x_train, x_test, y_train, y_test = train_test_split(xdata,ydata,test_size = test_s ,stratify= ydata, random_state = 0)\n",
        "    return x_train, x_test, y_train, y_test\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWm2Gtn-iRno"
      },
      "source": [
        "def probability(theta, x):\n",
        "    \"\"\"\n",
        "    This function calculates the sigmoid function.\n",
        "    \"\"\"\n",
        "    pred = np.dot(x, theta)\n",
        "    return 1 / (1 + np.exp(-pred))\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeNZ_WDHiRno"
      },
      "source": [
        "def cost_function(x, y, theta):\n",
        "    \"\"\"\n",
        "    This function calculates the logistic regression cost function.(j(0) = (1/n)[y * log(h(0) + (1-y)log(1-h(0)))] )\n",
        "    \"\"\"\n",
        "    n = len(y)\n",
        "    J = -(1 / n) * np.sum(y * np.log(probability(theta, x)) + (1 - y) * np.log(1 - probability(theta, x)))\n",
        "    return J\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75f8RCddiRno"
      },
      "source": [
        "#gradient function\n",
        "def gradient_descent(x,y, theta,alpha, iterations):\n",
        "        n = len(y)\n",
        "        cost_history = [0] * iterations             # Create a array of the size equal to no.of iteration to save CostHistoty.\n",
        "        for iteration in range(iterations):\n",
        "                y_pred = probability(theta, x)      # Calculate h0(x)\n",
        "                gradient = x.T.dot(y_pred - y)/n\n",
        "                theta = theta - alpha*gradient      # update the parameter\n",
        "                cost = cost_function(x,y,theta)\n",
        "                cost_history[iteration] = cost\n",
        "                if(cost > cost_history[iteration-1]):  # cost increases from the previous step decrease the alpha value\n",
        "                     alpha = alpha*0.1\n",
        "                   \n",
        "        return theta, cost_history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb9KJTBfiRnp"
      },
      "source": [
        "def Accuracy(theta):\n",
        "    \"\"\"\n",
        "    This function calculates the accuracy of the classification\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    length = len(x_test)                               # Total number of samples\n",
        "    prediction = (probability(theta, x_test) > 0.5)    # Get the class of the data\n",
        "    _y = y_test.reshape(-1, 1)                         # Create a vector of the target data\n",
        "    correct = prediction == _y                         # Compare actual and predicted class \n",
        "    my_accuracy = (np.sum(correct) / length)*100       # Acurracy = num of correctly classified / total number of samples\n",
        "    print ('LR Accuracy %: ', my_accuracy)\n",
        "          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5vZf7prjGPJ"
      },
      "source": [
        "##### EEG Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwtULyDgiRnm"
      },
      "source": [
        "df = pd.read_csv(\"data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gLHMjp0iRnm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "78c9e2ed-2cd1-4bf2-f367-185dd8722b2e"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "      <th>X4</th>\n",
              "      <th>X5</th>\n",
              "      <th>X6</th>\n",
              "      <th>X7</th>\n",
              "      <th>X8</th>\n",
              "      <th>X9</th>\n",
              "      <th>X10</th>\n",
              "      <th>X11</th>\n",
              "      <th>X12</th>\n",
              "      <th>X13</th>\n",
              "      <th>X14</th>\n",
              "      <th>X15</th>\n",
              "      <th>X16</th>\n",
              "      <th>X17</th>\n",
              "      <th>X18</th>\n",
              "      <th>X19</th>\n",
              "      <th>X20</th>\n",
              "      <th>X21</th>\n",
              "      <th>X22</th>\n",
              "      <th>X23</th>\n",
              "      <th>X24</th>\n",
              "      <th>X25</th>\n",
              "      <th>X26</th>\n",
              "      <th>X27</th>\n",
              "      <th>X28</th>\n",
              "      <th>X29</th>\n",
              "      <th>X30</th>\n",
              "      <th>X31</th>\n",
              "      <th>X32</th>\n",
              "      <th>X33</th>\n",
              "      <th>X34</th>\n",
              "      <th>X35</th>\n",
              "      <th>X36</th>\n",
              "      <th>X37</th>\n",
              "      <th>X38</th>\n",
              "      <th>X39</th>\n",
              "      <th>...</th>\n",
              "      <th>X140</th>\n",
              "      <th>X141</th>\n",
              "      <th>X142</th>\n",
              "      <th>X143</th>\n",
              "      <th>X144</th>\n",
              "      <th>X145</th>\n",
              "      <th>X146</th>\n",
              "      <th>X147</th>\n",
              "      <th>X148</th>\n",
              "      <th>X149</th>\n",
              "      <th>X150</th>\n",
              "      <th>X151</th>\n",
              "      <th>X152</th>\n",
              "      <th>X153</th>\n",
              "      <th>X154</th>\n",
              "      <th>X155</th>\n",
              "      <th>X156</th>\n",
              "      <th>X157</th>\n",
              "      <th>X158</th>\n",
              "      <th>X159</th>\n",
              "      <th>X160</th>\n",
              "      <th>X161</th>\n",
              "      <th>X162</th>\n",
              "      <th>X163</th>\n",
              "      <th>X164</th>\n",
              "      <th>X165</th>\n",
              "      <th>X166</th>\n",
              "      <th>X167</th>\n",
              "      <th>X168</th>\n",
              "      <th>X169</th>\n",
              "      <th>X170</th>\n",
              "      <th>X171</th>\n",
              "      <th>X172</th>\n",
              "      <th>X173</th>\n",
              "      <th>X174</th>\n",
              "      <th>X175</th>\n",
              "      <th>X176</th>\n",
              "      <th>X177</th>\n",
              "      <th>X178</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>X21.V1.791</td>\n",
              "      <td>135</td>\n",
              "      <td>190</td>\n",
              "      <td>229</td>\n",
              "      <td>223</td>\n",
              "      <td>192</td>\n",
              "      <td>125</td>\n",
              "      <td>55</td>\n",
              "      <td>-9</td>\n",
              "      <td>-33</td>\n",
              "      <td>-38</td>\n",
              "      <td>-10</td>\n",
              "      <td>35</td>\n",
              "      <td>64</td>\n",
              "      <td>113</td>\n",
              "      <td>152</td>\n",
              "      <td>164</td>\n",
              "      <td>127</td>\n",
              "      <td>50</td>\n",
              "      <td>-47</td>\n",
              "      <td>-121</td>\n",
              "      <td>-138</td>\n",
              "      <td>-125</td>\n",
              "      <td>-101</td>\n",
              "      <td>-50</td>\n",
              "      <td>11</td>\n",
              "      <td>39</td>\n",
              "      <td>24</td>\n",
              "      <td>48</td>\n",
              "      <td>64</td>\n",
              "      <td>46</td>\n",
              "      <td>13</td>\n",
              "      <td>-19</td>\n",
              "      <td>-61</td>\n",
              "      <td>-96</td>\n",
              "      <td>-130</td>\n",
              "      <td>-132</td>\n",
              "      <td>-116</td>\n",
              "      <td>-115</td>\n",
              "      <td>-71</td>\n",
              "      <td>...</td>\n",
              "      <td>54</td>\n",
              "      <td>90</td>\n",
              "      <td>111</td>\n",
              "      <td>107</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>18</td>\n",
              "      <td>-25</td>\n",
              "      <td>-69</td>\n",
              "      <td>-65</td>\n",
              "      <td>-44</td>\n",
              "      <td>-33</td>\n",
              "      <td>-57</td>\n",
              "      <td>-88</td>\n",
              "      <td>-114</td>\n",
              "      <td>-130</td>\n",
              "      <td>-114</td>\n",
              "      <td>-83</td>\n",
              "      <td>-53</td>\n",
              "      <td>-79</td>\n",
              "      <td>-72</td>\n",
              "      <td>-85</td>\n",
              "      <td>-109</td>\n",
              "      <td>-98</td>\n",
              "      <td>-72</td>\n",
              "      <td>-65</td>\n",
              "      <td>-63</td>\n",
              "      <td>-11</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>-17</td>\n",
              "      <td>-15</td>\n",
              "      <td>-31</td>\n",
              "      <td>-77</td>\n",
              "      <td>-103</td>\n",
              "      <td>-127</td>\n",
              "      <td>-116</td>\n",
              "      <td>-83</td>\n",
              "      <td>-51</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>X15.V1.924</td>\n",
              "      <td>386</td>\n",
              "      <td>382</td>\n",
              "      <td>356</td>\n",
              "      <td>331</td>\n",
              "      <td>320</td>\n",
              "      <td>315</td>\n",
              "      <td>307</td>\n",
              "      <td>272</td>\n",
              "      <td>244</td>\n",
              "      <td>232</td>\n",
              "      <td>237</td>\n",
              "      <td>258</td>\n",
              "      <td>212</td>\n",
              "      <td>2</td>\n",
              "      <td>-267</td>\n",
              "      <td>-605</td>\n",
              "      <td>-850</td>\n",
              "      <td>-1001</td>\n",
              "      <td>-1109</td>\n",
              "      <td>-1090</td>\n",
              "      <td>-967</td>\n",
              "      <td>-746</td>\n",
              "      <td>-464</td>\n",
              "      <td>-152</td>\n",
              "      <td>118</td>\n",
              "      <td>318</td>\n",
              "      <td>427</td>\n",
              "      <td>473</td>\n",
              "      <td>485</td>\n",
              "      <td>447</td>\n",
              "      <td>397</td>\n",
              "      <td>339</td>\n",
              "      <td>312</td>\n",
              "      <td>314</td>\n",
              "      <td>326</td>\n",
              "      <td>335</td>\n",
              "      <td>332</td>\n",
              "      <td>324</td>\n",
              "      <td>310</td>\n",
              "      <td>...</td>\n",
              "      <td>27</td>\n",
              "      <td>146</td>\n",
              "      <td>229</td>\n",
              "      <td>269</td>\n",
              "      <td>297</td>\n",
              "      <td>307</td>\n",
              "      <td>303</td>\n",
              "      <td>305</td>\n",
              "      <td>306</td>\n",
              "      <td>307</td>\n",
              "      <td>280</td>\n",
              "      <td>231</td>\n",
              "      <td>159</td>\n",
              "      <td>85</td>\n",
              "      <td>51</td>\n",
              "      <td>43</td>\n",
              "      <td>62</td>\n",
              "      <td>63</td>\n",
              "      <td>63</td>\n",
              "      <td>69</td>\n",
              "      <td>89</td>\n",
              "      <td>123</td>\n",
              "      <td>136</td>\n",
              "      <td>127</td>\n",
              "      <td>102</td>\n",
              "      <td>95</td>\n",
              "      <td>105</td>\n",
              "      <td>131</td>\n",
              "      <td>163</td>\n",
              "      <td>168</td>\n",
              "      <td>164</td>\n",
              "      <td>150</td>\n",
              "      <td>146</td>\n",
              "      <td>152</td>\n",
              "      <td>157</td>\n",
              "      <td>156</td>\n",
              "      <td>154</td>\n",
              "      <td>143</td>\n",
              "      <td>129</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>X8.V1.1</td>\n",
              "      <td>-32</td>\n",
              "      <td>-39</td>\n",
              "      <td>-47</td>\n",
              "      <td>-37</td>\n",
              "      <td>-32</td>\n",
              "      <td>-36</td>\n",
              "      <td>-57</td>\n",
              "      <td>-73</td>\n",
              "      <td>-85</td>\n",
              "      <td>-94</td>\n",
              "      <td>-99</td>\n",
              "      <td>-94</td>\n",
              "      <td>-96</td>\n",
              "      <td>-104</td>\n",
              "      <td>-103</td>\n",
              "      <td>-92</td>\n",
              "      <td>-75</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-53</td>\n",
              "      <td>-37</td>\n",
              "      <td>-14</td>\n",
              "      <td>-10</td>\n",
              "      <td>-39</td>\n",
              "      <td>-78</td>\n",
              "      <td>-102</td>\n",
              "      <td>-98</td>\n",
              "      <td>-80</td>\n",
              "      <td>-54</td>\n",
              "      <td>-40</td>\n",
              "      <td>-35</td>\n",
              "      <td>-39</td>\n",
              "      <td>-32</td>\n",
              "      <td>-13</td>\n",
              "      <td>7</td>\n",
              "      <td>34</td>\n",
              "      <td>41</td>\n",
              "      <td>33</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>-82</td>\n",
              "      <td>-107</td>\n",
              "      <td>-126</td>\n",
              "      <td>-124</td>\n",
              "      <td>-108</td>\n",
              "      <td>-84</td>\n",
              "      <td>-68</td>\n",
              "      <td>-61</td>\n",
              "      <td>-56</td>\n",
              "      <td>-63</td>\n",
              "      <td>-62</td>\n",
              "      <td>-33</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>45</td>\n",
              "      <td>37</td>\n",
              "      <td>48</td>\n",
              "      <td>62</td>\n",
              "      <td>80</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>-11</td>\n",
              "      <td>-39</td>\n",
              "      <td>-44</td>\n",
              "      <td>-42</td>\n",
              "      <td>-45</td>\n",
              "      <td>-48</td>\n",
              "      <td>-42</td>\n",
              "      <td>-6</td>\n",
              "      <td>29</td>\n",
              "      <td>57</td>\n",
              "      <td>64</td>\n",
              "      <td>48</td>\n",
              "      <td>19</td>\n",
              "      <td>-12</td>\n",
              "      <td>-30</td>\n",
              "      <td>-35</td>\n",
              "      <td>-35</td>\n",
              "      <td>-36</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>X16.V1.60</td>\n",
              "      <td>-105</td>\n",
              "      <td>-101</td>\n",
              "      <td>-96</td>\n",
              "      <td>-92</td>\n",
              "      <td>-89</td>\n",
              "      <td>-95</td>\n",
              "      <td>-102</td>\n",
              "      <td>-100</td>\n",
              "      <td>-87</td>\n",
              "      <td>-79</td>\n",
              "      <td>-72</td>\n",
              "      <td>-68</td>\n",
              "      <td>-74</td>\n",
              "      <td>-80</td>\n",
              "      <td>-83</td>\n",
              "      <td>-73</td>\n",
              "      <td>-68</td>\n",
              "      <td>-61</td>\n",
              "      <td>-58</td>\n",
              "      <td>-59</td>\n",
              "      <td>-64</td>\n",
              "      <td>-79</td>\n",
              "      <td>-84</td>\n",
              "      <td>-97</td>\n",
              "      <td>-94</td>\n",
              "      <td>-84</td>\n",
              "      <td>-77</td>\n",
              "      <td>-75</td>\n",
              "      <td>-72</td>\n",
              "      <td>-68</td>\n",
              "      <td>-76</td>\n",
              "      <td>-76</td>\n",
              "      <td>-72</td>\n",
              "      <td>-67</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-67</td>\n",
              "      <td>-68</td>\n",
              "      <td>...</td>\n",
              "      <td>-69</td>\n",
              "      <td>-66</td>\n",
              "      <td>-74</td>\n",
              "      <td>-69</td>\n",
              "      <td>-61</td>\n",
              "      <td>-51</td>\n",
              "      <td>-45</td>\n",
              "      <td>-45</td>\n",
              "      <td>-49</td>\n",
              "      <td>-58</td>\n",
              "      <td>-64</td>\n",
              "      <td>-78</td>\n",
              "      <td>-80</td>\n",
              "      <td>-90</td>\n",
              "      <td>-87</td>\n",
              "      <td>-83</td>\n",
              "      <td>-78</td>\n",
              "      <td>-64</td>\n",
              "      <td>-38</td>\n",
              "      <td>-22</td>\n",
              "      <td>-29</td>\n",
              "      <td>-42</td>\n",
              "      <td>-51</td>\n",
              "      <td>-68</td>\n",
              "      <td>-71</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-74</td>\n",
              "      <td>-74</td>\n",
              "      <td>-80</td>\n",
              "      <td>-82</td>\n",
              "      <td>-81</td>\n",
              "      <td>-80</td>\n",
              "      <td>-77</td>\n",
              "      <td>-85</td>\n",
              "      <td>-77</td>\n",
              "      <td>-72</td>\n",
              "      <td>-69</td>\n",
              "      <td>-65</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>X20.V1.54</td>\n",
              "      <td>-9</td>\n",
              "      <td>-65</td>\n",
              "      <td>-98</td>\n",
              "      <td>-102</td>\n",
              "      <td>-78</td>\n",
              "      <td>-48</td>\n",
              "      <td>-16</td>\n",
              "      <td>0</td>\n",
              "      <td>-21</td>\n",
              "      <td>-59</td>\n",
              "      <td>-90</td>\n",
              "      <td>-103</td>\n",
              "      <td>-84</td>\n",
              "      <td>-43</td>\n",
              "      <td>-9</td>\n",
              "      <td>3</td>\n",
              "      <td>-21</td>\n",
              "      <td>-60</td>\n",
              "      <td>-96</td>\n",
              "      <td>-103</td>\n",
              "      <td>-75</td>\n",
              "      <td>-29</td>\n",
              "      <td>14</td>\n",
              "      <td>55</td>\n",
              "      <td>78</td>\n",
              "      <td>73</td>\n",
              "      <td>28</td>\n",
              "      <td>-13</td>\n",
              "      <td>-43</td>\n",
              "      <td>-68</td>\n",
              "      <td>-78</td>\n",
              "      <td>-75</td>\n",
              "      <td>-55</td>\n",
              "      <td>-41</td>\n",
              "      <td>-19</td>\n",
              "      <td>-20</td>\n",
              "      <td>-29</td>\n",
              "      <td>-36</td>\n",
              "      <td>-20</td>\n",
              "      <td>...</td>\n",
              "      <td>38</td>\n",
              "      <td>26</td>\n",
              "      <td>10</td>\n",
              "      <td>-4</td>\n",
              "      <td>-13</td>\n",
              "      <td>-8</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>19</td>\n",
              "      <td>29</td>\n",
              "      <td>57</td>\n",
              "      <td>63</td>\n",
              "      <td>45</td>\n",
              "      <td>7</td>\n",
              "      <td>-13</td>\n",
              "      <td>-23</td>\n",
              "      <td>-9</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>-2</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>27</td>\n",
              "      <td>27</td>\n",
              "      <td>14</td>\n",
              "      <td>15</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>-12</td>\n",
              "      <td>-32</td>\n",
              "      <td>-41</td>\n",
              "      <td>-65</td>\n",
              "      <td>-83</td>\n",
              "      <td>-89</td>\n",
              "      <td>-73</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 180 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0   X1   X2   X3   X4   X5  ...  X174  X175  X176  X177  X178  y\n",
              "0  X21.V1.791  135  190  229  223  192  ...  -103  -127  -116   -83   -51  4\n",
              "1  X15.V1.924  386  382  356  331  320  ...   157   156   154   143   129  1\n",
              "2     X8.V1.1  -32  -39  -47  -37  -32  ...   -12   -30   -35   -35   -36  5\n",
              "3   X16.V1.60 -105 -101  -96  -92  -89  ...   -85   -77   -72   -69   -65  5\n",
              "4   X20.V1.54   -9  -65  -98 -102  -78  ...   -41   -65   -83   -89   -73  5\n",
              "\n",
              "[5 rows x 180 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awbQ1UNkiRnn"
      },
      "source": [
        "#df['y'].value_counts() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls8zVnW5iRnn"
      },
      "source": [
        "df.drop(df.columns[0], axis=1, inplace=True) # Drop the 1st column from the table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxXvDds1iRnn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "cbd03f9f-0d1d-4bf9-82d0-19f933a93275"
      },
      "source": [
        "# Created a features dataframe from the original dataframe\n",
        "feature_data = df.iloc[:,0:-1]\n",
        "feature_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "      <th>X4</th>\n",
              "      <th>X5</th>\n",
              "      <th>X6</th>\n",
              "      <th>X7</th>\n",
              "      <th>X8</th>\n",
              "      <th>X9</th>\n",
              "      <th>X10</th>\n",
              "      <th>X11</th>\n",
              "      <th>X12</th>\n",
              "      <th>X13</th>\n",
              "      <th>X14</th>\n",
              "      <th>X15</th>\n",
              "      <th>X16</th>\n",
              "      <th>X17</th>\n",
              "      <th>X18</th>\n",
              "      <th>X19</th>\n",
              "      <th>X20</th>\n",
              "      <th>X21</th>\n",
              "      <th>X22</th>\n",
              "      <th>X23</th>\n",
              "      <th>X24</th>\n",
              "      <th>X25</th>\n",
              "      <th>X26</th>\n",
              "      <th>X27</th>\n",
              "      <th>X28</th>\n",
              "      <th>X29</th>\n",
              "      <th>X30</th>\n",
              "      <th>X31</th>\n",
              "      <th>X32</th>\n",
              "      <th>X33</th>\n",
              "      <th>X34</th>\n",
              "      <th>X35</th>\n",
              "      <th>X36</th>\n",
              "      <th>X37</th>\n",
              "      <th>X38</th>\n",
              "      <th>X39</th>\n",
              "      <th>X40</th>\n",
              "      <th>...</th>\n",
              "      <th>X139</th>\n",
              "      <th>X140</th>\n",
              "      <th>X141</th>\n",
              "      <th>X142</th>\n",
              "      <th>X143</th>\n",
              "      <th>X144</th>\n",
              "      <th>X145</th>\n",
              "      <th>X146</th>\n",
              "      <th>X147</th>\n",
              "      <th>X148</th>\n",
              "      <th>X149</th>\n",
              "      <th>X150</th>\n",
              "      <th>X151</th>\n",
              "      <th>X152</th>\n",
              "      <th>X153</th>\n",
              "      <th>X154</th>\n",
              "      <th>X155</th>\n",
              "      <th>X156</th>\n",
              "      <th>X157</th>\n",
              "      <th>X158</th>\n",
              "      <th>X159</th>\n",
              "      <th>X160</th>\n",
              "      <th>X161</th>\n",
              "      <th>X162</th>\n",
              "      <th>X163</th>\n",
              "      <th>X164</th>\n",
              "      <th>X165</th>\n",
              "      <th>X166</th>\n",
              "      <th>X167</th>\n",
              "      <th>X168</th>\n",
              "      <th>X169</th>\n",
              "      <th>X170</th>\n",
              "      <th>X171</th>\n",
              "      <th>X172</th>\n",
              "      <th>X173</th>\n",
              "      <th>X174</th>\n",
              "      <th>X175</th>\n",
              "      <th>X176</th>\n",
              "      <th>X177</th>\n",
              "      <th>X178</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>135</td>\n",
              "      <td>190</td>\n",
              "      <td>229</td>\n",
              "      <td>223</td>\n",
              "      <td>192</td>\n",
              "      <td>125</td>\n",
              "      <td>55</td>\n",
              "      <td>-9</td>\n",
              "      <td>-33</td>\n",
              "      <td>-38</td>\n",
              "      <td>-10</td>\n",
              "      <td>35</td>\n",
              "      <td>64</td>\n",
              "      <td>113</td>\n",
              "      <td>152</td>\n",
              "      <td>164</td>\n",
              "      <td>127</td>\n",
              "      <td>50</td>\n",
              "      <td>-47</td>\n",
              "      <td>-121</td>\n",
              "      <td>-138</td>\n",
              "      <td>-125</td>\n",
              "      <td>-101</td>\n",
              "      <td>-50</td>\n",
              "      <td>11</td>\n",
              "      <td>39</td>\n",
              "      <td>24</td>\n",
              "      <td>48</td>\n",
              "      <td>64</td>\n",
              "      <td>46</td>\n",
              "      <td>13</td>\n",
              "      <td>-19</td>\n",
              "      <td>-61</td>\n",
              "      <td>-96</td>\n",
              "      <td>-130</td>\n",
              "      <td>-132</td>\n",
              "      <td>-116</td>\n",
              "      <td>-115</td>\n",
              "      <td>-71</td>\n",
              "      <td>-14</td>\n",
              "      <td>...</td>\n",
              "      <td>43</td>\n",
              "      <td>54</td>\n",
              "      <td>90</td>\n",
              "      <td>111</td>\n",
              "      <td>107</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>18</td>\n",
              "      <td>-25</td>\n",
              "      <td>-69</td>\n",
              "      <td>-65</td>\n",
              "      <td>-44</td>\n",
              "      <td>-33</td>\n",
              "      <td>-57</td>\n",
              "      <td>-88</td>\n",
              "      <td>-114</td>\n",
              "      <td>-130</td>\n",
              "      <td>-114</td>\n",
              "      <td>-83</td>\n",
              "      <td>-53</td>\n",
              "      <td>-79</td>\n",
              "      <td>-72</td>\n",
              "      <td>-85</td>\n",
              "      <td>-109</td>\n",
              "      <td>-98</td>\n",
              "      <td>-72</td>\n",
              "      <td>-65</td>\n",
              "      <td>-63</td>\n",
              "      <td>-11</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>-17</td>\n",
              "      <td>-15</td>\n",
              "      <td>-31</td>\n",
              "      <td>-77</td>\n",
              "      <td>-103</td>\n",
              "      <td>-127</td>\n",
              "      <td>-116</td>\n",
              "      <td>-83</td>\n",
              "      <td>-51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>386</td>\n",
              "      <td>382</td>\n",
              "      <td>356</td>\n",
              "      <td>331</td>\n",
              "      <td>320</td>\n",
              "      <td>315</td>\n",
              "      <td>307</td>\n",
              "      <td>272</td>\n",
              "      <td>244</td>\n",
              "      <td>232</td>\n",
              "      <td>237</td>\n",
              "      <td>258</td>\n",
              "      <td>212</td>\n",
              "      <td>2</td>\n",
              "      <td>-267</td>\n",
              "      <td>-605</td>\n",
              "      <td>-850</td>\n",
              "      <td>-1001</td>\n",
              "      <td>-1109</td>\n",
              "      <td>-1090</td>\n",
              "      <td>-967</td>\n",
              "      <td>-746</td>\n",
              "      <td>-464</td>\n",
              "      <td>-152</td>\n",
              "      <td>118</td>\n",
              "      <td>318</td>\n",
              "      <td>427</td>\n",
              "      <td>473</td>\n",
              "      <td>485</td>\n",
              "      <td>447</td>\n",
              "      <td>397</td>\n",
              "      <td>339</td>\n",
              "      <td>312</td>\n",
              "      <td>314</td>\n",
              "      <td>326</td>\n",
              "      <td>335</td>\n",
              "      <td>332</td>\n",
              "      <td>324</td>\n",
              "      <td>310</td>\n",
              "      <td>312</td>\n",
              "      <td>...</td>\n",
              "      <td>-136</td>\n",
              "      <td>27</td>\n",
              "      <td>146</td>\n",
              "      <td>229</td>\n",
              "      <td>269</td>\n",
              "      <td>297</td>\n",
              "      <td>307</td>\n",
              "      <td>303</td>\n",
              "      <td>305</td>\n",
              "      <td>306</td>\n",
              "      <td>307</td>\n",
              "      <td>280</td>\n",
              "      <td>231</td>\n",
              "      <td>159</td>\n",
              "      <td>85</td>\n",
              "      <td>51</td>\n",
              "      <td>43</td>\n",
              "      <td>62</td>\n",
              "      <td>63</td>\n",
              "      <td>63</td>\n",
              "      <td>69</td>\n",
              "      <td>89</td>\n",
              "      <td>123</td>\n",
              "      <td>136</td>\n",
              "      <td>127</td>\n",
              "      <td>102</td>\n",
              "      <td>95</td>\n",
              "      <td>105</td>\n",
              "      <td>131</td>\n",
              "      <td>163</td>\n",
              "      <td>168</td>\n",
              "      <td>164</td>\n",
              "      <td>150</td>\n",
              "      <td>146</td>\n",
              "      <td>152</td>\n",
              "      <td>157</td>\n",
              "      <td>156</td>\n",
              "      <td>154</td>\n",
              "      <td>143</td>\n",
              "      <td>129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-32</td>\n",
              "      <td>-39</td>\n",
              "      <td>-47</td>\n",
              "      <td>-37</td>\n",
              "      <td>-32</td>\n",
              "      <td>-36</td>\n",
              "      <td>-57</td>\n",
              "      <td>-73</td>\n",
              "      <td>-85</td>\n",
              "      <td>-94</td>\n",
              "      <td>-99</td>\n",
              "      <td>-94</td>\n",
              "      <td>-96</td>\n",
              "      <td>-104</td>\n",
              "      <td>-103</td>\n",
              "      <td>-92</td>\n",
              "      <td>-75</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-53</td>\n",
              "      <td>-37</td>\n",
              "      <td>-14</td>\n",
              "      <td>-10</td>\n",
              "      <td>-39</td>\n",
              "      <td>-78</td>\n",
              "      <td>-102</td>\n",
              "      <td>-98</td>\n",
              "      <td>-80</td>\n",
              "      <td>-54</td>\n",
              "      <td>-40</td>\n",
              "      <td>-35</td>\n",
              "      <td>-39</td>\n",
              "      <td>-32</td>\n",
              "      <td>-13</td>\n",
              "      <td>7</td>\n",
              "      <td>34</td>\n",
              "      <td>41</td>\n",
              "      <td>33</td>\n",
              "      <td>6</td>\n",
              "      <td>-15</td>\n",
              "      <td>...</td>\n",
              "      <td>-54</td>\n",
              "      <td>-82</td>\n",
              "      <td>-107</td>\n",
              "      <td>-126</td>\n",
              "      <td>-124</td>\n",
              "      <td>-108</td>\n",
              "      <td>-84</td>\n",
              "      <td>-68</td>\n",
              "      <td>-61</td>\n",
              "      <td>-56</td>\n",
              "      <td>-63</td>\n",
              "      <td>-62</td>\n",
              "      <td>-33</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>45</td>\n",
              "      <td>37</td>\n",
              "      <td>48</td>\n",
              "      <td>62</td>\n",
              "      <td>80</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>-11</td>\n",
              "      <td>-39</td>\n",
              "      <td>-44</td>\n",
              "      <td>-42</td>\n",
              "      <td>-45</td>\n",
              "      <td>-48</td>\n",
              "      <td>-42</td>\n",
              "      <td>-6</td>\n",
              "      <td>29</td>\n",
              "      <td>57</td>\n",
              "      <td>64</td>\n",
              "      <td>48</td>\n",
              "      <td>19</td>\n",
              "      <td>-12</td>\n",
              "      <td>-30</td>\n",
              "      <td>-35</td>\n",
              "      <td>-35</td>\n",
              "      <td>-36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-105</td>\n",
              "      <td>-101</td>\n",
              "      <td>-96</td>\n",
              "      <td>-92</td>\n",
              "      <td>-89</td>\n",
              "      <td>-95</td>\n",
              "      <td>-102</td>\n",
              "      <td>-100</td>\n",
              "      <td>-87</td>\n",
              "      <td>-79</td>\n",
              "      <td>-72</td>\n",
              "      <td>-68</td>\n",
              "      <td>-74</td>\n",
              "      <td>-80</td>\n",
              "      <td>-83</td>\n",
              "      <td>-73</td>\n",
              "      <td>-68</td>\n",
              "      <td>-61</td>\n",
              "      <td>-58</td>\n",
              "      <td>-59</td>\n",
              "      <td>-64</td>\n",
              "      <td>-79</td>\n",
              "      <td>-84</td>\n",
              "      <td>-97</td>\n",
              "      <td>-94</td>\n",
              "      <td>-84</td>\n",
              "      <td>-77</td>\n",
              "      <td>-75</td>\n",
              "      <td>-72</td>\n",
              "      <td>-68</td>\n",
              "      <td>-76</td>\n",
              "      <td>-76</td>\n",
              "      <td>-72</td>\n",
              "      <td>-67</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-67</td>\n",
              "      <td>-68</td>\n",
              "      <td>-69</td>\n",
              "      <td>...</td>\n",
              "      <td>-61</td>\n",
              "      <td>-69</td>\n",
              "      <td>-66</td>\n",
              "      <td>-74</td>\n",
              "      <td>-69</td>\n",
              "      <td>-61</td>\n",
              "      <td>-51</td>\n",
              "      <td>-45</td>\n",
              "      <td>-45</td>\n",
              "      <td>-49</td>\n",
              "      <td>-58</td>\n",
              "      <td>-64</td>\n",
              "      <td>-78</td>\n",
              "      <td>-80</td>\n",
              "      <td>-90</td>\n",
              "      <td>-87</td>\n",
              "      <td>-83</td>\n",
              "      <td>-78</td>\n",
              "      <td>-64</td>\n",
              "      <td>-38</td>\n",
              "      <td>-22</td>\n",
              "      <td>-29</td>\n",
              "      <td>-42</td>\n",
              "      <td>-51</td>\n",
              "      <td>-68</td>\n",
              "      <td>-71</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-74</td>\n",
              "      <td>-74</td>\n",
              "      <td>-80</td>\n",
              "      <td>-82</td>\n",
              "      <td>-81</td>\n",
              "      <td>-80</td>\n",
              "      <td>-77</td>\n",
              "      <td>-85</td>\n",
              "      <td>-77</td>\n",
              "      <td>-72</td>\n",
              "      <td>-69</td>\n",
              "      <td>-65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-9</td>\n",
              "      <td>-65</td>\n",
              "      <td>-98</td>\n",
              "      <td>-102</td>\n",
              "      <td>-78</td>\n",
              "      <td>-48</td>\n",
              "      <td>-16</td>\n",
              "      <td>0</td>\n",
              "      <td>-21</td>\n",
              "      <td>-59</td>\n",
              "      <td>-90</td>\n",
              "      <td>-103</td>\n",
              "      <td>-84</td>\n",
              "      <td>-43</td>\n",
              "      <td>-9</td>\n",
              "      <td>3</td>\n",
              "      <td>-21</td>\n",
              "      <td>-60</td>\n",
              "      <td>-96</td>\n",
              "      <td>-103</td>\n",
              "      <td>-75</td>\n",
              "      <td>-29</td>\n",
              "      <td>14</td>\n",
              "      <td>55</td>\n",
              "      <td>78</td>\n",
              "      <td>73</td>\n",
              "      <td>28</td>\n",
              "      <td>-13</td>\n",
              "      <td>-43</td>\n",
              "      <td>-68</td>\n",
              "      <td>-78</td>\n",
              "      <td>-75</td>\n",
              "      <td>-55</td>\n",
              "      <td>-41</td>\n",
              "      <td>-19</td>\n",
              "      <td>-20</td>\n",
              "      <td>-29</td>\n",
              "      <td>-36</td>\n",
              "      <td>-20</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>37</td>\n",
              "      <td>38</td>\n",
              "      <td>26</td>\n",
              "      <td>10</td>\n",
              "      <td>-4</td>\n",
              "      <td>-13</td>\n",
              "      <td>-8</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>19</td>\n",
              "      <td>29</td>\n",
              "      <td>57</td>\n",
              "      <td>63</td>\n",
              "      <td>45</td>\n",
              "      <td>7</td>\n",
              "      <td>-13</td>\n",
              "      <td>-23</td>\n",
              "      <td>-9</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>-2</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>27</td>\n",
              "      <td>27</td>\n",
              "      <td>14</td>\n",
              "      <td>15</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>-12</td>\n",
              "      <td>-32</td>\n",
              "      <td>-41</td>\n",
              "      <td>-65</td>\n",
              "      <td>-83</td>\n",
              "      <td>-89</td>\n",
              "      <td>-73</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 178 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    X1   X2   X3   X4   X5   X6   X7  ...  X172  X173  X174  X175  X176  X177  X178\n",
              "0  135  190  229  223  192  125   55  ...   -31   -77  -103  -127  -116   -83   -51\n",
              "1  386  382  356  331  320  315  307  ...   146   152   157   156   154   143   129\n",
              "2  -32  -39  -47  -37  -32  -36  -57  ...    48    19   -12   -30   -35   -35   -36\n",
              "3 -105 -101  -96  -92  -89  -95 -102  ...   -80   -77   -85   -77   -72   -69   -65\n",
              "4   -9  -65  -98 -102  -78  -48  -16  ...   -12   -32   -41   -65   -83   -89   -73\n",
              "\n",
              "[5 rows x 178 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIIHz2rViRno",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "63b46dbb-73b4-472a-868e-86d310cc353e"
      },
      "source": [
        "#Mapping Target for one V/S all logistic classification\n",
        "df['y'] = df.y.map({1: 1, 2: 0, 3: 0, 4: 0, 5:0}) # CLass 1-  Recording of seizure activity, Class 0 - No seizure activity recorded\n",
        "df.head(5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "      <th>X4</th>\n",
              "      <th>X5</th>\n",
              "      <th>X6</th>\n",
              "      <th>X7</th>\n",
              "      <th>X8</th>\n",
              "      <th>X9</th>\n",
              "      <th>X10</th>\n",
              "      <th>X11</th>\n",
              "      <th>X12</th>\n",
              "      <th>X13</th>\n",
              "      <th>X14</th>\n",
              "      <th>X15</th>\n",
              "      <th>X16</th>\n",
              "      <th>X17</th>\n",
              "      <th>X18</th>\n",
              "      <th>X19</th>\n",
              "      <th>X20</th>\n",
              "      <th>X21</th>\n",
              "      <th>X22</th>\n",
              "      <th>X23</th>\n",
              "      <th>X24</th>\n",
              "      <th>X25</th>\n",
              "      <th>X26</th>\n",
              "      <th>X27</th>\n",
              "      <th>X28</th>\n",
              "      <th>X29</th>\n",
              "      <th>X30</th>\n",
              "      <th>X31</th>\n",
              "      <th>X32</th>\n",
              "      <th>X33</th>\n",
              "      <th>X34</th>\n",
              "      <th>X35</th>\n",
              "      <th>X36</th>\n",
              "      <th>X37</th>\n",
              "      <th>X38</th>\n",
              "      <th>X39</th>\n",
              "      <th>X40</th>\n",
              "      <th>...</th>\n",
              "      <th>X140</th>\n",
              "      <th>X141</th>\n",
              "      <th>X142</th>\n",
              "      <th>X143</th>\n",
              "      <th>X144</th>\n",
              "      <th>X145</th>\n",
              "      <th>X146</th>\n",
              "      <th>X147</th>\n",
              "      <th>X148</th>\n",
              "      <th>X149</th>\n",
              "      <th>X150</th>\n",
              "      <th>X151</th>\n",
              "      <th>X152</th>\n",
              "      <th>X153</th>\n",
              "      <th>X154</th>\n",
              "      <th>X155</th>\n",
              "      <th>X156</th>\n",
              "      <th>X157</th>\n",
              "      <th>X158</th>\n",
              "      <th>X159</th>\n",
              "      <th>X160</th>\n",
              "      <th>X161</th>\n",
              "      <th>X162</th>\n",
              "      <th>X163</th>\n",
              "      <th>X164</th>\n",
              "      <th>X165</th>\n",
              "      <th>X166</th>\n",
              "      <th>X167</th>\n",
              "      <th>X168</th>\n",
              "      <th>X169</th>\n",
              "      <th>X170</th>\n",
              "      <th>X171</th>\n",
              "      <th>X172</th>\n",
              "      <th>X173</th>\n",
              "      <th>X174</th>\n",
              "      <th>X175</th>\n",
              "      <th>X176</th>\n",
              "      <th>X177</th>\n",
              "      <th>X178</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>135</td>\n",
              "      <td>190</td>\n",
              "      <td>229</td>\n",
              "      <td>223</td>\n",
              "      <td>192</td>\n",
              "      <td>125</td>\n",
              "      <td>55</td>\n",
              "      <td>-9</td>\n",
              "      <td>-33</td>\n",
              "      <td>-38</td>\n",
              "      <td>-10</td>\n",
              "      <td>35</td>\n",
              "      <td>64</td>\n",
              "      <td>113</td>\n",
              "      <td>152</td>\n",
              "      <td>164</td>\n",
              "      <td>127</td>\n",
              "      <td>50</td>\n",
              "      <td>-47</td>\n",
              "      <td>-121</td>\n",
              "      <td>-138</td>\n",
              "      <td>-125</td>\n",
              "      <td>-101</td>\n",
              "      <td>-50</td>\n",
              "      <td>11</td>\n",
              "      <td>39</td>\n",
              "      <td>24</td>\n",
              "      <td>48</td>\n",
              "      <td>64</td>\n",
              "      <td>46</td>\n",
              "      <td>13</td>\n",
              "      <td>-19</td>\n",
              "      <td>-61</td>\n",
              "      <td>-96</td>\n",
              "      <td>-130</td>\n",
              "      <td>-132</td>\n",
              "      <td>-116</td>\n",
              "      <td>-115</td>\n",
              "      <td>-71</td>\n",
              "      <td>-14</td>\n",
              "      <td>...</td>\n",
              "      <td>54</td>\n",
              "      <td>90</td>\n",
              "      <td>111</td>\n",
              "      <td>107</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>18</td>\n",
              "      <td>-25</td>\n",
              "      <td>-69</td>\n",
              "      <td>-65</td>\n",
              "      <td>-44</td>\n",
              "      <td>-33</td>\n",
              "      <td>-57</td>\n",
              "      <td>-88</td>\n",
              "      <td>-114</td>\n",
              "      <td>-130</td>\n",
              "      <td>-114</td>\n",
              "      <td>-83</td>\n",
              "      <td>-53</td>\n",
              "      <td>-79</td>\n",
              "      <td>-72</td>\n",
              "      <td>-85</td>\n",
              "      <td>-109</td>\n",
              "      <td>-98</td>\n",
              "      <td>-72</td>\n",
              "      <td>-65</td>\n",
              "      <td>-63</td>\n",
              "      <td>-11</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>-17</td>\n",
              "      <td>-15</td>\n",
              "      <td>-31</td>\n",
              "      <td>-77</td>\n",
              "      <td>-103</td>\n",
              "      <td>-127</td>\n",
              "      <td>-116</td>\n",
              "      <td>-83</td>\n",
              "      <td>-51</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>386</td>\n",
              "      <td>382</td>\n",
              "      <td>356</td>\n",
              "      <td>331</td>\n",
              "      <td>320</td>\n",
              "      <td>315</td>\n",
              "      <td>307</td>\n",
              "      <td>272</td>\n",
              "      <td>244</td>\n",
              "      <td>232</td>\n",
              "      <td>237</td>\n",
              "      <td>258</td>\n",
              "      <td>212</td>\n",
              "      <td>2</td>\n",
              "      <td>-267</td>\n",
              "      <td>-605</td>\n",
              "      <td>-850</td>\n",
              "      <td>-1001</td>\n",
              "      <td>-1109</td>\n",
              "      <td>-1090</td>\n",
              "      <td>-967</td>\n",
              "      <td>-746</td>\n",
              "      <td>-464</td>\n",
              "      <td>-152</td>\n",
              "      <td>118</td>\n",
              "      <td>318</td>\n",
              "      <td>427</td>\n",
              "      <td>473</td>\n",
              "      <td>485</td>\n",
              "      <td>447</td>\n",
              "      <td>397</td>\n",
              "      <td>339</td>\n",
              "      <td>312</td>\n",
              "      <td>314</td>\n",
              "      <td>326</td>\n",
              "      <td>335</td>\n",
              "      <td>332</td>\n",
              "      <td>324</td>\n",
              "      <td>310</td>\n",
              "      <td>312</td>\n",
              "      <td>...</td>\n",
              "      <td>27</td>\n",
              "      <td>146</td>\n",
              "      <td>229</td>\n",
              "      <td>269</td>\n",
              "      <td>297</td>\n",
              "      <td>307</td>\n",
              "      <td>303</td>\n",
              "      <td>305</td>\n",
              "      <td>306</td>\n",
              "      <td>307</td>\n",
              "      <td>280</td>\n",
              "      <td>231</td>\n",
              "      <td>159</td>\n",
              "      <td>85</td>\n",
              "      <td>51</td>\n",
              "      <td>43</td>\n",
              "      <td>62</td>\n",
              "      <td>63</td>\n",
              "      <td>63</td>\n",
              "      <td>69</td>\n",
              "      <td>89</td>\n",
              "      <td>123</td>\n",
              "      <td>136</td>\n",
              "      <td>127</td>\n",
              "      <td>102</td>\n",
              "      <td>95</td>\n",
              "      <td>105</td>\n",
              "      <td>131</td>\n",
              "      <td>163</td>\n",
              "      <td>168</td>\n",
              "      <td>164</td>\n",
              "      <td>150</td>\n",
              "      <td>146</td>\n",
              "      <td>152</td>\n",
              "      <td>157</td>\n",
              "      <td>156</td>\n",
              "      <td>154</td>\n",
              "      <td>143</td>\n",
              "      <td>129</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-32</td>\n",
              "      <td>-39</td>\n",
              "      <td>-47</td>\n",
              "      <td>-37</td>\n",
              "      <td>-32</td>\n",
              "      <td>-36</td>\n",
              "      <td>-57</td>\n",
              "      <td>-73</td>\n",
              "      <td>-85</td>\n",
              "      <td>-94</td>\n",
              "      <td>-99</td>\n",
              "      <td>-94</td>\n",
              "      <td>-96</td>\n",
              "      <td>-104</td>\n",
              "      <td>-103</td>\n",
              "      <td>-92</td>\n",
              "      <td>-75</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-53</td>\n",
              "      <td>-37</td>\n",
              "      <td>-14</td>\n",
              "      <td>-10</td>\n",
              "      <td>-39</td>\n",
              "      <td>-78</td>\n",
              "      <td>-102</td>\n",
              "      <td>-98</td>\n",
              "      <td>-80</td>\n",
              "      <td>-54</td>\n",
              "      <td>-40</td>\n",
              "      <td>-35</td>\n",
              "      <td>-39</td>\n",
              "      <td>-32</td>\n",
              "      <td>-13</td>\n",
              "      <td>7</td>\n",
              "      <td>34</td>\n",
              "      <td>41</td>\n",
              "      <td>33</td>\n",
              "      <td>6</td>\n",
              "      <td>-15</td>\n",
              "      <td>...</td>\n",
              "      <td>-82</td>\n",
              "      <td>-107</td>\n",
              "      <td>-126</td>\n",
              "      <td>-124</td>\n",
              "      <td>-108</td>\n",
              "      <td>-84</td>\n",
              "      <td>-68</td>\n",
              "      <td>-61</td>\n",
              "      <td>-56</td>\n",
              "      <td>-63</td>\n",
              "      <td>-62</td>\n",
              "      <td>-33</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>45</td>\n",
              "      <td>37</td>\n",
              "      <td>48</td>\n",
              "      <td>62</td>\n",
              "      <td>80</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>-11</td>\n",
              "      <td>-39</td>\n",
              "      <td>-44</td>\n",
              "      <td>-42</td>\n",
              "      <td>-45</td>\n",
              "      <td>-48</td>\n",
              "      <td>-42</td>\n",
              "      <td>-6</td>\n",
              "      <td>29</td>\n",
              "      <td>57</td>\n",
              "      <td>64</td>\n",
              "      <td>48</td>\n",
              "      <td>19</td>\n",
              "      <td>-12</td>\n",
              "      <td>-30</td>\n",
              "      <td>-35</td>\n",
              "      <td>-35</td>\n",
              "      <td>-36</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-105</td>\n",
              "      <td>-101</td>\n",
              "      <td>-96</td>\n",
              "      <td>-92</td>\n",
              "      <td>-89</td>\n",
              "      <td>-95</td>\n",
              "      <td>-102</td>\n",
              "      <td>-100</td>\n",
              "      <td>-87</td>\n",
              "      <td>-79</td>\n",
              "      <td>-72</td>\n",
              "      <td>-68</td>\n",
              "      <td>-74</td>\n",
              "      <td>-80</td>\n",
              "      <td>-83</td>\n",
              "      <td>-73</td>\n",
              "      <td>-68</td>\n",
              "      <td>-61</td>\n",
              "      <td>-58</td>\n",
              "      <td>-59</td>\n",
              "      <td>-64</td>\n",
              "      <td>-79</td>\n",
              "      <td>-84</td>\n",
              "      <td>-97</td>\n",
              "      <td>-94</td>\n",
              "      <td>-84</td>\n",
              "      <td>-77</td>\n",
              "      <td>-75</td>\n",
              "      <td>-72</td>\n",
              "      <td>-68</td>\n",
              "      <td>-76</td>\n",
              "      <td>-76</td>\n",
              "      <td>-72</td>\n",
              "      <td>-67</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-67</td>\n",
              "      <td>-68</td>\n",
              "      <td>-69</td>\n",
              "      <td>...</td>\n",
              "      <td>-69</td>\n",
              "      <td>-66</td>\n",
              "      <td>-74</td>\n",
              "      <td>-69</td>\n",
              "      <td>-61</td>\n",
              "      <td>-51</td>\n",
              "      <td>-45</td>\n",
              "      <td>-45</td>\n",
              "      <td>-49</td>\n",
              "      <td>-58</td>\n",
              "      <td>-64</td>\n",
              "      <td>-78</td>\n",
              "      <td>-80</td>\n",
              "      <td>-90</td>\n",
              "      <td>-87</td>\n",
              "      <td>-83</td>\n",
              "      <td>-78</td>\n",
              "      <td>-64</td>\n",
              "      <td>-38</td>\n",
              "      <td>-22</td>\n",
              "      <td>-29</td>\n",
              "      <td>-42</td>\n",
              "      <td>-51</td>\n",
              "      <td>-68</td>\n",
              "      <td>-71</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-74</td>\n",
              "      <td>-74</td>\n",
              "      <td>-80</td>\n",
              "      <td>-82</td>\n",
              "      <td>-81</td>\n",
              "      <td>-80</td>\n",
              "      <td>-77</td>\n",
              "      <td>-85</td>\n",
              "      <td>-77</td>\n",
              "      <td>-72</td>\n",
              "      <td>-69</td>\n",
              "      <td>-65</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-9</td>\n",
              "      <td>-65</td>\n",
              "      <td>-98</td>\n",
              "      <td>-102</td>\n",
              "      <td>-78</td>\n",
              "      <td>-48</td>\n",
              "      <td>-16</td>\n",
              "      <td>0</td>\n",
              "      <td>-21</td>\n",
              "      <td>-59</td>\n",
              "      <td>-90</td>\n",
              "      <td>-103</td>\n",
              "      <td>-84</td>\n",
              "      <td>-43</td>\n",
              "      <td>-9</td>\n",
              "      <td>3</td>\n",
              "      <td>-21</td>\n",
              "      <td>-60</td>\n",
              "      <td>-96</td>\n",
              "      <td>-103</td>\n",
              "      <td>-75</td>\n",
              "      <td>-29</td>\n",
              "      <td>14</td>\n",
              "      <td>55</td>\n",
              "      <td>78</td>\n",
              "      <td>73</td>\n",
              "      <td>28</td>\n",
              "      <td>-13</td>\n",
              "      <td>-43</td>\n",
              "      <td>-68</td>\n",
              "      <td>-78</td>\n",
              "      <td>-75</td>\n",
              "      <td>-55</td>\n",
              "      <td>-41</td>\n",
              "      <td>-19</td>\n",
              "      <td>-20</td>\n",
              "      <td>-29</td>\n",
              "      <td>-36</td>\n",
              "      <td>-20</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>38</td>\n",
              "      <td>26</td>\n",
              "      <td>10</td>\n",
              "      <td>-4</td>\n",
              "      <td>-13</td>\n",
              "      <td>-8</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>19</td>\n",
              "      <td>29</td>\n",
              "      <td>57</td>\n",
              "      <td>63</td>\n",
              "      <td>45</td>\n",
              "      <td>7</td>\n",
              "      <td>-13</td>\n",
              "      <td>-23</td>\n",
              "      <td>-9</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>-2</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>27</td>\n",
              "      <td>27</td>\n",
              "      <td>14</td>\n",
              "      <td>15</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>-12</td>\n",
              "      <td>-32</td>\n",
              "      <td>-41</td>\n",
              "      <td>-65</td>\n",
              "      <td>-83</td>\n",
              "      <td>-89</td>\n",
              "      <td>-73</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 179 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    X1   X2   X3   X4   X5   X6   X7  ...  X173  X174  X175  X176  X177  X178  y\n",
              "0  135  190  229  223  192  125   55  ...   -77  -103  -127  -116   -83   -51  0\n",
              "1  386  382  356  331  320  315  307  ...   152   157   156   154   143   129  1\n",
              "2  -32  -39  -47  -37  -32  -36  -57  ...    19   -12   -30   -35   -35   -36  0\n",
              "3 -105 -101  -96  -92  -89  -95 -102  ...   -77   -85   -77   -72   -69   -65  0\n",
              "4   -9  -65  -98 -102  -78  -48  -16  ...   -32   -41   -65   -83   -89   -73  0\n",
              "\n",
              "[5 rows x 179 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWLep66fiRnp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "127005f1-4314-4a17-93c3-a06bc07c4a22"
      },
      "source": [
        "feature_data = df.iloc[:,0:-1]\n",
        "feature_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "      <th>X4</th>\n",
              "      <th>X5</th>\n",
              "      <th>X6</th>\n",
              "      <th>X7</th>\n",
              "      <th>X8</th>\n",
              "      <th>X9</th>\n",
              "      <th>X10</th>\n",
              "      <th>X11</th>\n",
              "      <th>X12</th>\n",
              "      <th>X13</th>\n",
              "      <th>X14</th>\n",
              "      <th>X15</th>\n",
              "      <th>X16</th>\n",
              "      <th>X17</th>\n",
              "      <th>X18</th>\n",
              "      <th>X19</th>\n",
              "      <th>X20</th>\n",
              "      <th>X21</th>\n",
              "      <th>X22</th>\n",
              "      <th>X23</th>\n",
              "      <th>X24</th>\n",
              "      <th>X25</th>\n",
              "      <th>X26</th>\n",
              "      <th>X27</th>\n",
              "      <th>X28</th>\n",
              "      <th>X29</th>\n",
              "      <th>X30</th>\n",
              "      <th>X31</th>\n",
              "      <th>X32</th>\n",
              "      <th>X33</th>\n",
              "      <th>X34</th>\n",
              "      <th>X35</th>\n",
              "      <th>X36</th>\n",
              "      <th>X37</th>\n",
              "      <th>X38</th>\n",
              "      <th>X39</th>\n",
              "      <th>X40</th>\n",
              "      <th>...</th>\n",
              "      <th>X139</th>\n",
              "      <th>X140</th>\n",
              "      <th>X141</th>\n",
              "      <th>X142</th>\n",
              "      <th>X143</th>\n",
              "      <th>X144</th>\n",
              "      <th>X145</th>\n",
              "      <th>X146</th>\n",
              "      <th>X147</th>\n",
              "      <th>X148</th>\n",
              "      <th>X149</th>\n",
              "      <th>X150</th>\n",
              "      <th>X151</th>\n",
              "      <th>X152</th>\n",
              "      <th>X153</th>\n",
              "      <th>X154</th>\n",
              "      <th>X155</th>\n",
              "      <th>X156</th>\n",
              "      <th>X157</th>\n",
              "      <th>X158</th>\n",
              "      <th>X159</th>\n",
              "      <th>X160</th>\n",
              "      <th>X161</th>\n",
              "      <th>X162</th>\n",
              "      <th>X163</th>\n",
              "      <th>X164</th>\n",
              "      <th>X165</th>\n",
              "      <th>X166</th>\n",
              "      <th>X167</th>\n",
              "      <th>X168</th>\n",
              "      <th>X169</th>\n",
              "      <th>X170</th>\n",
              "      <th>X171</th>\n",
              "      <th>X172</th>\n",
              "      <th>X173</th>\n",
              "      <th>X174</th>\n",
              "      <th>X175</th>\n",
              "      <th>X176</th>\n",
              "      <th>X177</th>\n",
              "      <th>X178</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>135</td>\n",
              "      <td>190</td>\n",
              "      <td>229</td>\n",
              "      <td>223</td>\n",
              "      <td>192</td>\n",
              "      <td>125</td>\n",
              "      <td>55</td>\n",
              "      <td>-9</td>\n",
              "      <td>-33</td>\n",
              "      <td>-38</td>\n",
              "      <td>-10</td>\n",
              "      <td>35</td>\n",
              "      <td>64</td>\n",
              "      <td>113</td>\n",
              "      <td>152</td>\n",
              "      <td>164</td>\n",
              "      <td>127</td>\n",
              "      <td>50</td>\n",
              "      <td>-47</td>\n",
              "      <td>-121</td>\n",
              "      <td>-138</td>\n",
              "      <td>-125</td>\n",
              "      <td>-101</td>\n",
              "      <td>-50</td>\n",
              "      <td>11</td>\n",
              "      <td>39</td>\n",
              "      <td>24</td>\n",
              "      <td>48</td>\n",
              "      <td>64</td>\n",
              "      <td>46</td>\n",
              "      <td>13</td>\n",
              "      <td>-19</td>\n",
              "      <td>-61</td>\n",
              "      <td>-96</td>\n",
              "      <td>-130</td>\n",
              "      <td>-132</td>\n",
              "      <td>-116</td>\n",
              "      <td>-115</td>\n",
              "      <td>-71</td>\n",
              "      <td>-14</td>\n",
              "      <td>...</td>\n",
              "      <td>43</td>\n",
              "      <td>54</td>\n",
              "      <td>90</td>\n",
              "      <td>111</td>\n",
              "      <td>107</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>18</td>\n",
              "      <td>-25</td>\n",
              "      <td>-69</td>\n",
              "      <td>-65</td>\n",
              "      <td>-44</td>\n",
              "      <td>-33</td>\n",
              "      <td>-57</td>\n",
              "      <td>-88</td>\n",
              "      <td>-114</td>\n",
              "      <td>-130</td>\n",
              "      <td>-114</td>\n",
              "      <td>-83</td>\n",
              "      <td>-53</td>\n",
              "      <td>-79</td>\n",
              "      <td>-72</td>\n",
              "      <td>-85</td>\n",
              "      <td>-109</td>\n",
              "      <td>-98</td>\n",
              "      <td>-72</td>\n",
              "      <td>-65</td>\n",
              "      <td>-63</td>\n",
              "      <td>-11</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>-17</td>\n",
              "      <td>-15</td>\n",
              "      <td>-31</td>\n",
              "      <td>-77</td>\n",
              "      <td>-103</td>\n",
              "      <td>-127</td>\n",
              "      <td>-116</td>\n",
              "      <td>-83</td>\n",
              "      <td>-51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>386</td>\n",
              "      <td>382</td>\n",
              "      <td>356</td>\n",
              "      <td>331</td>\n",
              "      <td>320</td>\n",
              "      <td>315</td>\n",
              "      <td>307</td>\n",
              "      <td>272</td>\n",
              "      <td>244</td>\n",
              "      <td>232</td>\n",
              "      <td>237</td>\n",
              "      <td>258</td>\n",
              "      <td>212</td>\n",
              "      <td>2</td>\n",
              "      <td>-267</td>\n",
              "      <td>-605</td>\n",
              "      <td>-850</td>\n",
              "      <td>-1001</td>\n",
              "      <td>-1109</td>\n",
              "      <td>-1090</td>\n",
              "      <td>-967</td>\n",
              "      <td>-746</td>\n",
              "      <td>-464</td>\n",
              "      <td>-152</td>\n",
              "      <td>118</td>\n",
              "      <td>318</td>\n",
              "      <td>427</td>\n",
              "      <td>473</td>\n",
              "      <td>485</td>\n",
              "      <td>447</td>\n",
              "      <td>397</td>\n",
              "      <td>339</td>\n",
              "      <td>312</td>\n",
              "      <td>314</td>\n",
              "      <td>326</td>\n",
              "      <td>335</td>\n",
              "      <td>332</td>\n",
              "      <td>324</td>\n",
              "      <td>310</td>\n",
              "      <td>312</td>\n",
              "      <td>...</td>\n",
              "      <td>-136</td>\n",
              "      <td>27</td>\n",
              "      <td>146</td>\n",
              "      <td>229</td>\n",
              "      <td>269</td>\n",
              "      <td>297</td>\n",
              "      <td>307</td>\n",
              "      <td>303</td>\n",
              "      <td>305</td>\n",
              "      <td>306</td>\n",
              "      <td>307</td>\n",
              "      <td>280</td>\n",
              "      <td>231</td>\n",
              "      <td>159</td>\n",
              "      <td>85</td>\n",
              "      <td>51</td>\n",
              "      <td>43</td>\n",
              "      <td>62</td>\n",
              "      <td>63</td>\n",
              "      <td>63</td>\n",
              "      <td>69</td>\n",
              "      <td>89</td>\n",
              "      <td>123</td>\n",
              "      <td>136</td>\n",
              "      <td>127</td>\n",
              "      <td>102</td>\n",
              "      <td>95</td>\n",
              "      <td>105</td>\n",
              "      <td>131</td>\n",
              "      <td>163</td>\n",
              "      <td>168</td>\n",
              "      <td>164</td>\n",
              "      <td>150</td>\n",
              "      <td>146</td>\n",
              "      <td>152</td>\n",
              "      <td>157</td>\n",
              "      <td>156</td>\n",
              "      <td>154</td>\n",
              "      <td>143</td>\n",
              "      <td>129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-32</td>\n",
              "      <td>-39</td>\n",
              "      <td>-47</td>\n",
              "      <td>-37</td>\n",
              "      <td>-32</td>\n",
              "      <td>-36</td>\n",
              "      <td>-57</td>\n",
              "      <td>-73</td>\n",
              "      <td>-85</td>\n",
              "      <td>-94</td>\n",
              "      <td>-99</td>\n",
              "      <td>-94</td>\n",
              "      <td>-96</td>\n",
              "      <td>-104</td>\n",
              "      <td>-103</td>\n",
              "      <td>-92</td>\n",
              "      <td>-75</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-53</td>\n",
              "      <td>-37</td>\n",
              "      <td>-14</td>\n",
              "      <td>-10</td>\n",
              "      <td>-39</td>\n",
              "      <td>-78</td>\n",
              "      <td>-102</td>\n",
              "      <td>-98</td>\n",
              "      <td>-80</td>\n",
              "      <td>-54</td>\n",
              "      <td>-40</td>\n",
              "      <td>-35</td>\n",
              "      <td>-39</td>\n",
              "      <td>-32</td>\n",
              "      <td>-13</td>\n",
              "      <td>7</td>\n",
              "      <td>34</td>\n",
              "      <td>41</td>\n",
              "      <td>33</td>\n",
              "      <td>6</td>\n",
              "      <td>-15</td>\n",
              "      <td>...</td>\n",
              "      <td>-54</td>\n",
              "      <td>-82</td>\n",
              "      <td>-107</td>\n",
              "      <td>-126</td>\n",
              "      <td>-124</td>\n",
              "      <td>-108</td>\n",
              "      <td>-84</td>\n",
              "      <td>-68</td>\n",
              "      <td>-61</td>\n",
              "      <td>-56</td>\n",
              "      <td>-63</td>\n",
              "      <td>-62</td>\n",
              "      <td>-33</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>45</td>\n",
              "      <td>37</td>\n",
              "      <td>48</td>\n",
              "      <td>62</td>\n",
              "      <td>80</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>-11</td>\n",
              "      <td>-39</td>\n",
              "      <td>-44</td>\n",
              "      <td>-42</td>\n",
              "      <td>-45</td>\n",
              "      <td>-48</td>\n",
              "      <td>-42</td>\n",
              "      <td>-6</td>\n",
              "      <td>29</td>\n",
              "      <td>57</td>\n",
              "      <td>64</td>\n",
              "      <td>48</td>\n",
              "      <td>19</td>\n",
              "      <td>-12</td>\n",
              "      <td>-30</td>\n",
              "      <td>-35</td>\n",
              "      <td>-35</td>\n",
              "      <td>-36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-105</td>\n",
              "      <td>-101</td>\n",
              "      <td>-96</td>\n",
              "      <td>-92</td>\n",
              "      <td>-89</td>\n",
              "      <td>-95</td>\n",
              "      <td>-102</td>\n",
              "      <td>-100</td>\n",
              "      <td>-87</td>\n",
              "      <td>-79</td>\n",
              "      <td>-72</td>\n",
              "      <td>-68</td>\n",
              "      <td>-74</td>\n",
              "      <td>-80</td>\n",
              "      <td>-83</td>\n",
              "      <td>-73</td>\n",
              "      <td>-68</td>\n",
              "      <td>-61</td>\n",
              "      <td>-58</td>\n",
              "      <td>-59</td>\n",
              "      <td>-64</td>\n",
              "      <td>-79</td>\n",
              "      <td>-84</td>\n",
              "      <td>-97</td>\n",
              "      <td>-94</td>\n",
              "      <td>-84</td>\n",
              "      <td>-77</td>\n",
              "      <td>-75</td>\n",
              "      <td>-72</td>\n",
              "      <td>-68</td>\n",
              "      <td>-76</td>\n",
              "      <td>-76</td>\n",
              "      <td>-72</td>\n",
              "      <td>-67</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-67</td>\n",
              "      <td>-68</td>\n",
              "      <td>-69</td>\n",
              "      <td>...</td>\n",
              "      <td>-61</td>\n",
              "      <td>-69</td>\n",
              "      <td>-66</td>\n",
              "      <td>-74</td>\n",
              "      <td>-69</td>\n",
              "      <td>-61</td>\n",
              "      <td>-51</td>\n",
              "      <td>-45</td>\n",
              "      <td>-45</td>\n",
              "      <td>-49</td>\n",
              "      <td>-58</td>\n",
              "      <td>-64</td>\n",
              "      <td>-78</td>\n",
              "      <td>-80</td>\n",
              "      <td>-90</td>\n",
              "      <td>-87</td>\n",
              "      <td>-83</td>\n",
              "      <td>-78</td>\n",
              "      <td>-64</td>\n",
              "      <td>-38</td>\n",
              "      <td>-22</td>\n",
              "      <td>-29</td>\n",
              "      <td>-42</td>\n",
              "      <td>-51</td>\n",
              "      <td>-68</td>\n",
              "      <td>-71</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-74</td>\n",
              "      <td>-74</td>\n",
              "      <td>-80</td>\n",
              "      <td>-82</td>\n",
              "      <td>-81</td>\n",
              "      <td>-80</td>\n",
              "      <td>-77</td>\n",
              "      <td>-85</td>\n",
              "      <td>-77</td>\n",
              "      <td>-72</td>\n",
              "      <td>-69</td>\n",
              "      <td>-65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-9</td>\n",
              "      <td>-65</td>\n",
              "      <td>-98</td>\n",
              "      <td>-102</td>\n",
              "      <td>-78</td>\n",
              "      <td>-48</td>\n",
              "      <td>-16</td>\n",
              "      <td>0</td>\n",
              "      <td>-21</td>\n",
              "      <td>-59</td>\n",
              "      <td>-90</td>\n",
              "      <td>-103</td>\n",
              "      <td>-84</td>\n",
              "      <td>-43</td>\n",
              "      <td>-9</td>\n",
              "      <td>3</td>\n",
              "      <td>-21</td>\n",
              "      <td>-60</td>\n",
              "      <td>-96</td>\n",
              "      <td>-103</td>\n",
              "      <td>-75</td>\n",
              "      <td>-29</td>\n",
              "      <td>14</td>\n",
              "      <td>55</td>\n",
              "      <td>78</td>\n",
              "      <td>73</td>\n",
              "      <td>28</td>\n",
              "      <td>-13</td>\n",
              "      <td>-43</td>\n",
              "      <td>-68</td>\n",
              "      <td>-78</td>\n",
              "      <td>-75</td>\n",
              "      <td>-55</td>\n",
              "      <td>-41</td>\n",
              "      <td>-19</td>\n",
              "      <td>-20</td>\n",
              "      <td>-29</td>\n",
              "      <td>-36</td>\n",
              "      <td>-20</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>37</td>\n",
              "      <td>38</td>\n",
              "      <td>26</td>\n",
              "      <td>10</td>\n",
              "      <td>-4</td>\n",
              "      <td>-13</td>\n",
              "      <td>-8</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>19</td>\n",
              "      <td>29</td>\n",
              "      <td>57</td>\n",
              "      <td>63</td>\n",
              "      <td>45</td>\n",
              "      <td>7</td>\n",
              "      <td>-13</td>\n",
              "      <td>-23</td>\n",
              "      <td>-9</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>-2</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>27</td>\n",
              "      <td>27</td>\n",
              "      <td>14</td>\n",
              "      <td>15</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>-12</td>\n",
              "      <td>-32</td>\n",
              "      <td>-41</td>\n",
              "      <td>-65</td>\n",
              "      <td>-83</td>\n",
              "      <td>-89</td>\n",
              "      <td>-73</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 178 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    X1   X2   X3   X4   X5   X6   X7  ...  X172  X173  X174  X175  X176  X177  X178\n",
              "0  135  190  229  223  192  125   55  ...   -31   -77  -103  -127  -116   -83   -51\n",
              "1  386  382  356  331  320  315  307  ...   146   152   157   156   154   143   129\n",
              "2  -32  -39  -47  -37  -32  -36  -57  ...    48    19   -12   -30   -35   -35   -36\n",
              "3 -105 -101  -96  -92  -89  -95 -102  ...   -80   -77   -85   -77   -72   -69   -65\n",
              "4   -9  -65  -98 -102  -78  -48  -16  ...   -12   -32   -41   -65   -83   -89   -73\n",
              "\n",
              "[5 rows x 178 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gH_Xe88iRnp"
      },
      "source": [
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "X_train_minmax = min_max_scaler.fit_transform(feature_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLt-G762iRnp"
      },
      "source": [
        "y = df[\"y\"]\n",
        "x_train, x_test, y_train, y_test = SplitData(X_train_minmax,y ,0.20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce7603ldiRnp"
      },
      "source": [
        "theta = np.array([0, 0, 0])                                           # Initilize theta to 0\n",
        "m,n=np.shape(x_train)                                                 # Get the row and col count of trainging features\n",
        "y_train=np.array(y_train)\n",
        "y_train=y_train.reshape(m,1)                                          # Create a vector of train target data\n",
        "theta=np.zeros((n,1))                                                 # Create an vector of zeros\n",
        "(theta, cost) = gradient_descent(x_train,y_train,theta,0.001,100)     # New parameter value set and cost history\n",
        "    \n",
        "MSE_train=cost[-1]\n",
        "       \n",
        "#test\n",
        "m,n=np.shape(x_test)\n",
        "y_test=np.array(y_test)\n",
        "y_test=y_test.reshape(m,1)\n",
        "theta=np.zeros((n,1))\n",
        "(theta, cost) = gradient_descent(x_test,y_test,theta,0.001,100)\n",
        "MSE_test=cost[-1]\n",
        "Accuracy(theta)\n",
        "print(\"Training MSE  :\",MSE_train)\n",
        "print('Test MSE  :',MSE_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUhTRdJViRnq"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cfm_test = confusion_matrix(y_test, y_pred_s1)\n",
        "cfm_test "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUN6C-0GiRnq",
        "outputId": "bf615c93-4632-4bcb-ed78-4ad60a949586"
      },
      "source": [
        "tp, fn, fp, tn = confusion_matrix(y_test, y_pred_s1,labels=[1,0]).reshape(-1)\n",
        "print('Outcome values : \\n', tp, fn, fp, tn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Outcome values : \n",
            " 29 661 0 2760\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s04dIHPKiRnr"
      },
      "source": [
        "##### For boston dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFdlVgpSiRnr",
        "outputId": "bcf6ff94-5a2b-4895-85de-fd0ed746599c"
      },
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"boston.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.887621</td>\n",
              "      <td>0.185875</td>\n",
              "      <td>2.299474</td>\n",
              "      <td>2.299474</td>\n",
              "      <td>0.581396</td>\n",
              "      <td>healthy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.783744</td>\n",
              "      <td>0.163794</td>\n",
              "      <td>2.299480</td>\n",
              "      <td>2.299480</td>\n",
              "      <td>0.588340</td>\n",
              "      <td>healthy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.875873</td>\n",
              "      <td>0.154714</td>\n",
              "      <td>2.299477</td>\n",
              "      <td>2.299477</td>\n",
              "      <td>0.589802</td>\n",
              "      <td>healthy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.881293</td>\n",
              "      <td>0.138396</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>0.592911</td>\n",
              "      <td>healthy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.798116</td>\n",
              "      <td>0.125291</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>0.595338</td>\n",
              "      <td>healthy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0        f1        f2        f3        f4        f5    class\n",
              "0           0  0.887621  0.185875  2.299474  2.299474  0.581396  healthy\n",
              "1           1  0.783744  0.163794  2.299480  2.299480  0.588340  healthy\n",
              "2           2  0.875873  0.154714  2.299477  2.299477  0.589802  healthy\n",
              "3           3  0.881293  0.138396  2.299481  2.299481  0.592911  healthy\n",
              "4           4  0.798116  0.125291  2.299481  2.299481  0.595338  healthy"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUevgVMGiRnr"
      },
      "source": [
        "df=df.iloc[:,1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZLIyDKkiRnr"
      },
      "source": [
        "df[\"Target\"] = df[\"class\"]\n",
        "\n",
        "df['Target'] = df.Target.map({\"seizure\": 1, \"healthy\": 0, \"transation\": 0}) # Checking for presence of CLass 1\n",
        "df.head(5)\n",
        "df.drop(df.columns[5], axis=1, inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2R5jkM6iRnr",
        "outputId": "8e708ef3-0c9d-4816-ca00-4592ebc35f1b"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.887621</td>\n",
              "      <td>0.185875</td>\n",
              "      <td>2.299474</td>\n",
              "      <td>2.299474</td>\n",
              "      <td>0.581396</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.783744</td>\n",
              "      <td>0.163794</td>\n",
              "      <td>2.299480</td>\n",
              "      <td>2.299480</td>\n",
              "      <td>0.588340</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.875873</td>\n",
              "      <td>0.154714</td>\n",
              "      <td>2.299477</td>\n",
              "      <td>2.299477</td>\n",
              "      <td>0.589802</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.881293</td>\n",
              "      <td>0.138396</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>0.592911</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.798116</td>\n",
              "      <td>0.125291</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>0.595338</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         f1        f2        f3        f4        f5  Target\n",
              "0  0.887621  0.185875  2.299474  2.299474  0.581396       0\n",
              "1  0.783744  0.163794  2.299480  2.299480  0.588340       0\n",
              "2  0.875873  0.154714  2.299477  2.299477  0.589802       0\n",
              "3  0.881293  0.138396  2.299481  2.299481  0.592911       0\n",
              "4  0.798116  0.125291  2.299481  2.299481  0.595338       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 285
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVpu7IJ0iRns",
        "outputId": "6706fef0-51ff-4723-ac66-a8161ce87b59"
      },
      "source": [
        "feature_data = df.iloc[:,0:5]\n",
        "feature_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.887621</td>\n",
              "      <td>0.185875</td>\n",
              "      <td>2.299474</td>\n",
              "      <td>2.299474</td>\n",
              "      <td>0.581396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.783744</td>\n",
              "      <td>0.163794</td>\n",
              "      <td>2.299480</td>\n",
              "      <td>2.299480</td>\n",
              "      <td>0.588340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.875873</td>\n",
              "      <td>0.154714</td>\n",
              "      <td>2.299477</td>\n",
              "      <td>2.299477</td>\n",
              "      <td>0.589802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.881293</td>\n",
              "      <td>0.138396</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>0.592911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.798116</td>\n",
              "      <td>0.125291</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>0.595338</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         f1        f2        f3        f4        f5\n",
              "0  0.887621  0.185875  2.299474  2.299474  0.581396\n",
              "1  0.783744  0.163794  2.299480  2.299480  0.588340\n",
              "2  0.875873  0.154714  2.299477  2.299477  0.589802\n",
              "3  0.881293  0.138396  2.299481  2.299481  0.592911\n",
              "4  0.798116  0.125291  2.299481  2.299481  0.595338"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt44RgLBiRns"
      },
      "source": [
        "#dataprocessing\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "X_train_minmax = min_max_scaler.fit_transform(feature_data)\n",
        "#X_train_minmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VteebvHQiRns"
      },
      "source": [
        "y = df[\"Target\"]\n",
        "x_train, x_test, y_train, y_test = SplitData(X_train_minmax,y, 0.30 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PrHStdaiRns",
        "outputId": "48a5807c-d080-4e49-8afe-131ca83e7874"
      },
      "source": [
        "theta = np.array([0, 0, 0])\n",
        "m,n=np.shape(x_train)\n",
        "y_train=np.array(y_train)\n",
        "y_train=y_train.reshape(m,1)\n",
        "theta=np.zeros((n,1))\n",
        "(theta, cost) = gradient_descent(x_train,y_train,theta,0.001,100)\n",
        "    \n",
        "MSE_train=cost[-1]\n",
        "       \n",
        "#test\n",
        "m,n=np.shape(x_test)\n",
        "y_test=np.array(y_test)\n",
        "y_test=y_test.reshape(m,1)\n",
        "theta=np.zeros((n,1))\n",
        "(theta, cost) = gradient_descent(x_test,y_test,theta,0.001,100)\n",
        "MSE_test=cost[-1]\n",
        "accuracy=Accuracy(theta)\n",
        "print(\"Training MSE  :\",MSE_train)\n",
        "print('Test MSE  :',MSE_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR Accuracy %:  82.22222222222221\n",
            "Training MSE  : 0.6926763939481365\n",
            "Test MSE  : 0.6927433437204163\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx4jgPG9iRns"
      },
      "source": [
        "##### For CHB MIT dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5AdeVyAiRns",
        "outputId": "3c871f96-2d46-415c-dfcb-f0fec8394bd9"
      },
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"chb.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>T8-P8-1_n_peaks</th>\n",
              "      <th>T8-P8-1_n_crossings</th>\n",
              "      <th>T8-P8-1_hfd</th>\n",
              "      <th>T8-P8-1_pfd</th>\n",
              "      <th>T8-P8-1_hurst_exp</th>\n",
              "      <th>T8-P8-1_spectral_entropy</th>\n",
              "      <th>T8-P8-1_total_power</th>\n",
              "      <th>T8-P8-1_median_freq</th>\n",
              "      <th>T8-P8-1_peak_freq</th>\n",
              "      <th>T8-P8-1_hjorth_mobility</th>\n",
              "      <th>T8-P8-1_hjorth_complexity</th>\n",
              "      <th>T8-P8-1_power_1hz</th>\n",
              "      <th>T8-P8-1_power_5hz</th>\n",
              "      <th>T8-P8-1_power_10hz</th>\n",
              "      <th>T8-P8-1_power_15hz</th>\n",
              "      <th>T8-P8-1_power_20hz</th>\n",
              "      <th>seizure</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>183</td>\n",
              "      <td>153</td>\n",
              "      <td>0.094016</td>\n",
              "      <td>0.611024</td>\n",
              "      <td>0.879401</td>\n",
              "      <td>0.747444</td>\n",
              "      <td>5.090000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.002766</td>\n",
              "      <td>171.222337</td>\n",
              "      <td>0.420823</td>\n",
              "      <td>0.226051</td>\n",
              "      <td>0.128492</td>\n",
              "      <td>0.106838</td>\n",
              "      <td>0.117797</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>183</td>\n",
              "      <td>165</td>\n",
              "      <td>0.092275</td>\n",
              "      <td>0.611024</td>\n",
              "      <td>0.746361</td>\n",
              "      <td>0.765824</td>\n",
              "      <td>5.560000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003211</td>\n",
              "      <td>148.493351</td>\n",
              "      <td>0.384135</td>\n",
              "      <td>0.234295</td>\n",
              "      <td>0.140616</td>\n",
              "      <td>0.115382</td>\n",
              "      <td>0.125572</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>186</td>\n",
              "      <td>170</td>\n",
              "      <td>0.098374</td>\n",
              "      <td>0.610385</td>\n",
              "      <td>0.687973</td>\n",
              "      <td>0.752050</td>\n",
              "      <td>5.450000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003564</td>\n",
              "      <td>142.209197</td>\n",
              "      <td>0.389655</td>\n",
              "      <td>0.238327</td>\n",
              "      <td>0.124545</td>\n",
              "      <td>0.109260</td>\n",
              "      <td>0.138213</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>189</td>\n",
              "      <td>166</td>\n",
              "      <td>0.099552</td>\n",
              "      <td>0.609756</td>\n",
              "      <td>0.726399</td>\n",
              "      <td>0.754101</td>\n",
              "      <td>5.920000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003514</td>\n",
              "      <td>140.840795</td>\n",
              "      <td>0.396989</td>\n",
              "      <td>0.232559</td>\n",
              "      <td>0.120853</td>\n",
              "      <td>0.118132</td>\n",
              "      <td>0.131467</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>188</td>\n",
              "      <td>167</td>\n",
              "      <td>0.101196</td>\n",
              "      <td>0.609756</td>\n",
              "      <td>0.755442</td>\n",
              "      <td>0.758187</td>\n",
              "      <td>5.830000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003494</td>\n",
              "      <td>142.434220</td>\n",
              "      <td>0.390794</td>\n",
              "      <td>0.230665</td>\n",
              "      <td>0.125717</td>\n",
              "      <td>0.119646</td>\n",
              "      <td>0.133177</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   T8-P8-1_n_peaks  T8-P8-1_n_crossings  T8-P8-1_hfd  T8-P8-1_pfd  \\\n",
              "0              183                  153     0.094016     0.611024   \n",
              "1              183                  165     0.092275     0.611024   \n",
              "2              186                  170     0.098374     0.610385   \n",
              "3              189                  166     0.099552     0.609756   \n",
              "4              188                  167     0.101196     0.609756   \n",
              "\n",
              "   T8-P8-1_hurst_exp  T8-P8-1_spectral_entropy  T8-P8-1_total_power  \\\n",
              "0           0.879401                  0.747444         5.090000e-10   \n",
              "1           0.746361                  0.765824         5.560000e-10   \n",
              "2           0.687973                  0.752050         5.450000e-10   \n",
              "3           0.726399                  0.754101         5.920000e-10   \n",
              "4           0.755442                  0.758187         5.830000e-10   \n",
              "\n",
              "   T8-P8-1_median_freq  T8-P8-1_peak_freq  T8-P8-1_hjorth_mobility  \\\n",
              "0                    3                  1                 0.002766   \n",
              "1                    3                  1                 0.003211   \n",
              "2                    3                  1                 0.003564   \n",
              "3                    3                  1                 0.003514   \n",
              "4                    3                  1                 0.003494   \n",
              "\n",
              "   T8-P8-1_hjorth_complexity  T8-P8-1_power_1hz  T8-P8-1_power_5hz  \\\n",
              "0                 171.222337           0.420823           0.226051   \n",
              "1                 148.493351           0.384135           0.234295   \n",
              "2                 142.209197           0.389655           0.238327   \n",
              "3                 140.840795           0.396989           0.232559   \n",
              "4                 142.434220           0.390794           0.230665   \n",
              "\n",
              "   T8-P8-1_power_10hz  T8-P8-1_power_15hz  T8-P8-1_power_20hz  seizure  \n",
              "0            0.128492            0.106838            0.117797        0  \n",
              "1            0.140616            0.115382            0.125572        0  \n",
              "2            0.124545            0.109260            0.138213        0  \n",
              "3            0.120853            0.118132            0.131467        0  \n",
              "4            0.125717            0.119646            0.133177        0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v6FIpRdiRnt",
        "outputId": "95a4e472-ae2b-4615-da9d-d6cd0d773aa6"
      },
      "source": [
        "feature_data = df.iloc[:,0:-1]\n",
        "feature_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>T8-P8-1_n_peaks</th>\n",
              "      <th>T8-P8-1_n_crossings</th>\n",
              "      <th>T8-P8-1_hfd</th>\n",
              "      <th>T8-P8-1_pfd</th>\n",
              "      <th>T8-P8-1_hurst_exp</th>\n",
              "      <th>T8-P8-1_spectral_entropy</th>\n",
              "      <th>T8-P8-1_total_power</th>\n",
              "      <th>T8-P8-1_median_freq</th>\n",
              "      <th>T8-P8-1_peak_freq</th>\n",
              "      <th>T8-P8-1_hjorth_mobility</th>\n",
              "      <th>T8-P8-1_hjorth_complexity</th>\n",
              "      <th>T8-P8-1_power_1hz</th>\n",
              "      <th>T8-P8-1_power_5hz</th>\n",
              "      <th>T8-P8-1_power_10hz</th>\n",
              "      <th>T8-P8-1_power_15hz</th>\n",
              "      <th>T8-P8-1_power_20hz</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>183</td>\n",
              "      <td>153</td>\n",
              "      <td>0.094016</td>\n",
              "      <td>0.611024</td>\n",
              "      <td>0.879401</td>\n",
              "      <td>0.747444</td>\n",
              "      <td>5.090000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.002766</td>\n",
              "      <td>171.222337</td>\n",
              "      <td>0.420823</td>\n",
              "      <td>0.226051</td>\n",
              "      <td>0.128492</td>\n",
              "      <td>0.106838</td>\n",
              "      <td>0.117797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>183</td>\n",
              "      <td>165</td>\n",
              "      <td>0.092275</td>\n",
              "      <td>0.611024</td>\n",
              "      <td>0.746361</td>\n",
              "      <td>0.765824</td>\n",
              "      <td>5.560000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003211</td>\n",
              "      <td>148.493351</td>\n",
              "      <td>0.384135</td>\n",
              "      <td>0.234295</td>\n",
              "      <td>0.140616</td>\n",
              "      <td>0.115382</td>\n",
              "      <td>0.125572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>186</td>\n",
              "      <td>170</td>\n",
              "      <td>0.098374</td>\n",
              "      <td>0.610385</td>\n",
              "      <td>0.687973</td>\n",
              "      <td>0.752050</td>\n",
              "      <td>5.450000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003564</td>\n",
              "      <td>142.209197</td>\n",
              "      <td>0.389655</td>\n",
              "      <td>0.238327</td>\n",
              "      <td>0.124545</td>\n",
              "      <td>0.109260</td>\n",
              "      <td>0.138213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>189</td>\n",
              "      <td>166</td>\n",
              "      <td>0.099552</td>\n",
              "      <td>0.609756</td>\n",
              "      <td>0.726399</td>\n",
              "      <td>0.754101</td>\n",
              "      <td>5.920000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003514</td>\n",
              "      <td>140.840795</td>\n",
              "      <td>0.396989</td>\n",
              "      <td>0.232559</td>\n",
              "      <td>0.120853</td>\n",
              "      <td>0.118132</td>\n",
              "      <td>0.131467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>188</td>\n",
              "      <td>167</td>\n",
              "      <td>0.101196</td>\n",
              "      <td>0.609756</td>\n",
              "      <td>0.755442</td>\n",
              "      <td>0.758187</td>\n",
              "      <td>5.830000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003494</td>\n",
              "      <td>142.434220</td>\n",
              "      <td>0.390794</td>\n",
              "      <td>0.230665</td>\n",
              "      <td>0.125717</td>\n",
              "      <td>0.119646</td>\n",
              "      <td>0.133177</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   T8-P8-1_n_peaks  T8-P8-1_n_crossings  T8-P8-1_hfd  T8-P8-1_pfd  \\\n",
              "0              183                  153     0.094016     0.611024   \n",
              "1              183                  165     0.092275     0.611024   \n",
              "2              186                  170     0.098374     0.610385   \n",
              "3              189                  166     0.099552     0.609756   \n",
              "4              188                  167     0.101196     0.609756   \n",
              "\n",
              "   T8-P8-1_hurst_exp  T8-P8-1_spectral_entropy  T8-P8-1_total_power  \\\n",
              "0           0.879401                  0.747444         5.090000e-10   \n",
              "1           0.746361                  0.765824         5.560000e-10   \n",
              "2           0.687973                  0.752050         5.450000e-10   \n",
              "3           0.726399                  0.754101         5.920000e-10   \n",
              "4           0.755442                  0.758187         5.830000e-10   \n",
              "\n",
              "   T8-P8-1_median_freq  T8-P8-1_peak_freq  T8-P8-1_hjorth_mobility  \\\n",
              "0                    3                  1                 0.002766   \n",
              "1                    3                  1                 0.003211   \n",
              "2                    3                  1                 0.003564   \n",
              "3                    3                  1                 0.003514   \n",
              "4                    3                  1                 0.003494   \n",
              "\n",
              "   T8-P8-1_hjorth_complexity  T8-P8-1_power_1hz  T8-P8-1_power_5hz  \\\n",
              "0                 171.222337           0.420823           0.226051   \n",
              "1                 148.493351           0.384135           0.234295   \n",
              "2                 142.209197           0.389655           0.238327   \n",
              "3                 140.840795           0.396989           0.232559   \n",
              "4                 142.434220           0.390794           0.230665   \n",
              "\n",
              "   T8-P8-1_power_10hz  T8-P8-1_power_15hz  T8-P8-1_power_20hz  \n",
              "0            0.128492            0.106838            0.117797  \n",
              "1            0.140616            0.115382            0.125572  \n",
              "2            0.124545            0.109260            0.138213  \n",
              "3            0.120853            0.118132            0.131467  \n",
              "4            0.125717            0.119646            0.133177  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iBcle5eiRnt"
      },
      "source": [
        "#dataprocessing\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "X_train_minmax = min_max_scaler.fit_transform(feature_data)\n",
        "#X_train_minmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9u7XxqqiRnt"
      },
      "source": [
        "y = df[\"seizure\"]\n",
        "x_train, x_test, y_train, y_test = SplitData(X_train_minmax,y , 0.30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RF6sufKiRnt",
        "outputId": "e18292ad-b05b-4916-fba3-1dcaaada42d1"
      },
      "source": [
        "theta = np.array([0, 0, 0])\n",
        "m,n=np.shape(x_train)\n",
        "y_train=np.array(y_train)\n",
        "y_train=y_train.reshape(m,1)\n",
        "theta=np.zeros((n,1))\n",
        "(theta, cost) = gradient_descent(x_train,y_train,theta,0.001,100)\n",
        "    \n",
        "MSE_train=cost[-1]\n",
        "       \n",
        "#test\n",
        "m,n=np.shape(x_test)\n",
        "y_test=np.array(y_test)\n",
        "y_test=y_test.reshape(m,1)\n",
        "theta=np.zeros((n,1))\n",
        "(theta, cost) = gradient_descent(x_test,y_test,theta,0.001,100)\n",
        "MSE_test=cost[-1]\n",
        "accuracy=Accuracy(theta)\n",
        "print(\"Training MSE  :\",MSE_train)\n",
        "print('Test MSE  :',MSE_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR Accuracy %:  98.0\n",
            "Training MSE  : 0.6854484778200003\n",
            "Test MSE  : 0.6854520328004717\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxj4tOi17CvI"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoaSJIcDQtZ-"
      },
      "source": [
        "##### EEG dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "SAqJJt_yG3SR",
        "outputId": "0c123921-062e-4850-8b86-79ec6a0707a1"
      },
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "      <th>X4</th>\n",
              "      <th>X5</th>\n",
              "      <th>X6</th>\n",
              "      <th>X7</th>\n",
              "      <th>X8</th>\n",
              "      <th>X9</th>\n",
              "      <th>X10</th>\n",
              "      <th>X11</th>\n",
              "      <th>X12</th>\n",
              "      <th>X13</th>\n",
              "      <th>X14</th>\n",
              "      <th>X15</th>\n",
              "      <th>X16</th>\n",
              "      <th>X17</th>\n",
              "      <th>X18</th>\n",
              "      <th>X19</th>\n",
              "      <th>X20</th>\n",
              "      <th>X21</th>\n",
              "      <th>X22</th>\n",
              "      <th>X23</th>\n",
              "      <th>X24</th>\n",
              "      <th>X25</th>\n",
              "      <th>X26</th>\n",
              "      <th>X27</th>\n",
              "      <th>X28</th>\n",
              "      <th>X29</th>\n",
              "      <th>X30</th>\n",
              "      <th>X31</th>\n",
              "      <th>X32</th>\n",
              "      <th>X33</th>\n",
              "      <th>X34</th>\n",
              "      <th>X35</th>\n",
              "      <th>X36</th>\n",
              "      <th>X37</th>\n",
              "      <th>X38</th>\n",
              "      <th>X39</th>\n",
              "      <th>...</th>\n",
              "      <th>X140</th>\n",
              "      <th>X141</th>\n",
              "      <th>X142</th>\n",
              "      <th>X143</th>\n",
              "      <th>X144</th>\n",
              "      <th>X145</th>\n",
              "      <th>X146</th>\n",
              "      <th>X147</th>\n",
              "      <th>X148</th>\n",
              "      <th>X149</th>\n",
              "      <th>X150</th>\n",
              "      <th>X151</th>\n",
              "      <th>X152</th>\n",
              "      <th>X153</th>\n",
              "      <th>X154</th>\n",
              "      <th>X155</th>\n",
              "      <th>X156</th>\n",
              "      <th>X157</th>\n",
              "      <th>X158</th>\n",
              "      <th>X159</th>\n",
              "      <th>X160</th>\n",
              "      <th>X161</th>\n",
              "      <th>X162</th>\n",
              "      <th>X163</th>\n",
              "      <th>X164</th>\n",
              "      <th>X165</th>\n",
              "      <th>X166</th>\n",
              "      <th>X167</th>\n",
              "      <th>X168</th>\n",
              "      <th>X169</th>\n",
              "      <th>X170</th>\n",
              "      <th>X171</th>\n",
              "      <th>X172</th>\n",
              "      <th>X173</th>\n",
              "      <th>X174</th>\n",
              "      <th>X175</th>\n",
              "      <th>X176</th>\n",
              "      <th>X177</th>\n",
              "      <th>X178</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>X21.V1.791</td>\n",
              "      <td>135</td>\n",
              "      <td>190</td>\n",
              "      <td>229</td>\n",
              "      <td>223</td>\n",
              "      <td>192</td>\n",
              "      <td>125</td>\n",
              "      <td>55</td>\n",
              "      <td>-9</td>\n",
              "      <td>-33</td>\n",
              "      <td>-38</td>\n",
              "      <td>-10</td>\n",
              "      <td>35</td>\n",
              "      <td>64</td>\n",
              "      <td>113</td>\n",
              "      <td>152</td>\n",
              "      <td>164</td>\n",
              "      <td>127</td>\n",
              "      <td>50</td>\n",
              "      <td>-47</td>\n",
              "      <td>-121</td>\n",
              "      <td>-138</td>\n",
              "      <td>-125</td>\n",
              "      <td>-101</td>\n",
              "      <td>-50</td>\n",
              "      <td>11</td>\n",
              "      <td>39</td>\n",
              "      <td>24</td>\n",
              "      <td>48</td>\n",
              "      <td>64</td>\n",
              "      <td>46</td>\n",
              "      <td>13</td>\n",
              "      <td>-19</td>\n",
              "      <td>-61</td>\n",
              "      <td>-96</td>\n",
              "      <td>-130</td>\n",
              "      <td>-132</td>\n",
              "      <td>-116</td>\n",
              "      <td>-115</td>\n",
              "      <td>-71</td>\n",
              "      <td>...</td>\n",
              "      <td>54</td>\n",
              "      <td>90</td>\n",
              "      <td>111</td>\n",
              "      <td>107</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>18</td>\n",
              "      <td>-25</td>\n",
              "      <td>-69</td>\n",
              "      <td>-65</td>\n",
              "      <td>-44</td>\n",
              "      <td>-33</td>\n",
              "      <td>-57</td>\n",
              "      <td>-88</td>\n",
              "      <td>-114</td>\n",
              "      <td>-130</td>\n",
              "      <td>-114</td>\n",
              "      <td>-83</td>\n",
              "      <td>-53</td>\n",
              "      <td>-79</td>\n",
              "      <td>-72</td>\n",
              "      <td>-85</td>\n",
              "      <td>-109</td>\n",
              "      <td>-98</td>\n",
              "      <td>-72</td>\n",
              "      <td>-65</td>\n",
              "      <td>-63</td>\n",
              "      <td>-11</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>-17</td>\n",
              "      <td>-15</td>\n",
              "      <td>-31</td>\n",
              "      <td>-77</td>\n",
              "      <td>-103</td>\n",
              "      <td>-127</td>\n",
              "      <td>-116</td>\n",
              "      <td>-83</td>\n",
              "      <td>-51</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>X15.V1.924</td>\n",
              "      <td>386</td>\n",
              "      <td>382</td>\n",
              "      <td>356</td>\n",
              "      <td>331</td>\n",
              "      <td>320</td>\n",
              "      <td>315</td>\n",
              "      <td>307</td>\n",
              "      <td>272</td>\n",
              "      <td>244</td>\n",
              "      <td>232</td>\n",
              "      <td>237</td>\n",
              "      <td>258</td>\n",
              "      <td>212</td>\n",
              "      <td>2</td>\n",
              "      <td>-267</td>\n",
              "      <td>-605</td>\n",
              "      <td>-850</td>\n",
              "      <td>-1001</td>\n",
              "      <td>-1109</td>\n",
              "      <td>-1090</td>\n",
              "      <td>-967</td>\n",
              "      <td>-746</td>\n",
              "      <td>-464</td>\n",
              "      <td>-152</td>\n",
              "      <td>118</td>\n",
              "      <td>318</td>\n",
              "      <td>427</td>\n",
              "      <td>473</td>\n",
              "      <td>485</td>\n",
              "      <td>447</td>\n",
              "      <td>397</td>\n",
              "      <td>339</td>\n",
              "      <td>312</td>\n",
              "      <td>314</td>\n",
              "      <td>326</td>\n",
              "      <td>335</td>\n",
              "      <td>332</td>\n",
              "      <td>324</td>\n",
              "      <td>310</td>\n",
              "      <td>...</td>\n",
              "      <td>27</td>\n",
              "      <td>146</td>\n",
              "      <td>229</td>\n",
              "      <td>269</td>\n",
              "      <td>297</td>\n",
              "      <td>307</td>\n",
              "      <td>303</td>\n",
              "      <td>305</td>\n",
              "      <td>306</td>\n",
              "      <td>307</td>\n",
              "      <td>280</td>\n",
              "      <td>231</td>\n",
              "      <td>159</td>\n",
              "      <td>85</td>\n",
              "      <td>51</td>\n",
              "      <td>43</td>\n",
              "      <td>62</td>\n",
              "      <td>63</td>\n",
              "      <td>63</td>\n",
              "      <td>69</td>\n",
              "      <td>89</td>\n",
              "      <td>123</td>\n",
              "      <td>136</td>\n",
              "      <td>127</td>\n",
              "      <td>102</td>\n",
              "      <td>95</td>\n",
              "      <td>105</td>\n",
              "      <td>131</td>\n",
              "      <td>163</td>\n",
              "      <td>168</td>\n",
              "      <td>164</td>\n",
              "      <td>150</td>\n",
              "      <td>146</td>\n",
              "      <td>152</td>\n",
              "      <td>157</td>\n",
              "      <td>156</td>\n",
              "      <td>154</td>\n",
              "      <td>143</td>\n",
              "      <td>129</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>X8.V1.1</td>\n",
              "      <td>-32</td>\n",
              "      <td>-39</td>\n",
              "      <td>-47</td>\n",
              "      <td>-37</td>\n",
              "      <td>-32</td>\n",
              "      <td>-36</td>\n",
              "      <td>-57</td>\n",
              "      <td>-73</td>\n",
              "      <td>-85</td>\n",
              "      <td>-94</td>\n",
              "      <td>-99</td>\n",
              "      <td>-94</td>\n",
              "      <td>-96</td>\n",
              "      <td>-104</td>\n",
              "      <td>-103</td>\n",
              "      <td>-92</td>\n",
              "      <td>-75</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-53</td>\n",
              "      <td>-37</td>\n",
              "      <td>-14</td>\n",
              "      <td>-10</td>\n",
              "      <td>-39</td>\n",
              "      <td>-78</td>\n",
              "      <td>-102</td>\n",
              "      <td>-98</td>\n",
              "      <td>-80</td>\n",
              "      <td>-54</td>\n",
              "      <td>-40</td>\n",
              "      <td>-35</td>\n",
              "      <td>-39</td>\n",
              "      <td>-32</td>\n",
              "      <td>-13</td>\n",
              "      <td>7</td>\n",
              "      <td>34</td>\n",
              "      <td>41</td>\n",
              "      <td>33</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>-82</td>\n",
              "      <td>-107</td>\n",
              "      <td>-126</td>\n",
              "      <td>-124</td>\n",
              "      <td>-108</td>\n",
              "      <td>-84</td>\n",
              "      <td>-68</td>\n",
              "      <td>-61</td>\n",
              "      <td>-56</td>\n",
              "      <td>-63</td>\n",
              "      <td>-62</td>\n",
              "      <td>-33</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>45</td>\n",
              "      <td>37</td>\n",
              "      <td>48</td>\n",
              "      <td>62</td>\n",
              "      <td>80</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>-11</td>\n",
              "      <td>-39</td>\n",
              "      <td>-44</td>\n",
              "      <td>-42</td>\n",
              "      <td>-45</td>\n",
              "      <td>-48</td>\n",
              "      <td>-42</td>\n",
              "      <td>-6</td>\n",
              "      <td>29</td>\n",
              "      <td>57</td>\n",
              "      <td>64</td>\n",
              "      <td>48</td>\n",
              "      <td>19</td>\n",
              "      <td>-12</td>\n",
              "      <td>-30</td>\n",
              "      <td>-35</td>\n",
              "      <td>-35</td>\n",
              "      <td>-36</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>X16.V1.60</td>\n",
              "      <td>-105</td>\n",
              "      <td>-101</td>\n",
              "      <td>-96</td>\n",
              "      <td>-92</td>\n",
              "      <td>-89</td>\n",
              "      <td>-95</td>\n",
              "      <td>-102</td>\n",
              "      <td>-100</td>\n",
              "      <td>-87</td>\n",
              "      <td>-79</td>\n",
              "      <td>-72</td>\n",
              "      <td>-68</td>\n",
              "      <td>-74</td>\n",
              "      <td>-80</td>\n",
              "      <td>-83</td>\n",
              "      <td>-73</td>\n",
              "      <td>-68</td>\n",
              "      <td>-61</td>\n",
              "      <td>-58</td>\n",
              "      <td>-59</td>\n",
              "      <td>-64</td>\n",
              "      <td>-79</td>\n",
              "      <td>-84</td>\n",
              "      <td>-97</td>\n",
              "      <td>-94</td>\n",
              "      <td>-84</td>\n",
              "      <td>-77</td>\n",
              "      <td>-75</td>\n",
              "      <td>-72</td>\n",
              "      <td>-68</td>\n",
              "      <td>-76</td>\n",
              "      <td>-76</td>\n",
              "      <td>-72</td>\n",
              "      <td>-67</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-67</td>\n",
              "      <td>-68</td>\n",
              "      <td>...</td>\n",
              "      <td>-69</td>\n",
              "      <td>-66</td>\n",
              "      <td>-74</td>\n",
              "      <td>-69</td>\n",
              "      <td>-61</td>\n",
              "      <td>-51</td>\n",
              "      <td>-45</td>\n",
              "      <td>-45</td>\n",
              "      <td>-49</td>\n",
              "      <td>-58</td>\n",
              "      <td>-64</td>\n",
              "      <td>-78</td>\n",
              "      <td>-80</td>\n",
              "      <td>-90</td>\n",
              "      <td>-87</td>\n",
              "      <td>-83</td>\n",
              "      <td>-78</td>\n",
              "      <td>-64</td>\n",
              "      <td>-38</td>\n",
              "      <td>-22</td>\n",
              "      <td>-29</td>\n",
              "      <td>-42</td>\n",
              "      <td>-51</td>\n",
              "      <td>-68</td>\n",
              "      <td>-71</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-74</td>\n",
              "      <td>-74</td>\n",
              "      <td>-80</td>\n",
              "      <td>-82</td>\n",
              "      <td>-81</td>\n",
              "      <td>-80</td>\n",
              "      <td>-77</td>\n",
              "      <td>-85</td>\n",
              "      <td>-77</td>\n",
              "      <td>-72</td>\n",
              "      <td>-69</td>\n",
              "      <td>-65</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>X20.V1.54</td>\n",
              "      <td>-9</td>\n",
              "      <td>-65</td>\n",
              "      <td>-98</td>\n",
              "      <td>-102</td>\n",
              "      <td>-78</td>\n",
              "      <td>-48</td>\n",
              "      <td>-16</td>\n",
              "      <td>0</td>\n",
              "      <td>-21</td>\n",
              "      <td>-59</td>\n",
              "      <td>-90</td>\n",
              "      <td>-103</td>\n",
              "      <td>-84</td>\n",
              "      <td>-43</td>\n",
              "      <td>-9</td>\n",
              "      <td>3</td>\n",
              "      <td>-21</td>\n",
              "      <td>-60</td>\n",
              "      <td>-96</td>\n",
              "      <td>-103</td>\n",
              "      <td>-75</td>\n",
              "      <td>-29</td>\n",
              "      <td>14</td>\n",
              "      <td>55</td>\n",
              "      <td>78</td>\n",
              "      <td>73</td>\n",
              "      <td>28</td>\n",
              "      <td>-13</td>\n",
              "      <td>-43</td>\n",
              "      <td>-68</td>\n",
              "      <td>-78</td>\n",
              "      <td>-75</td>\n",
              "      <td>-55</td>\n",
              "      <td>-41</td>\n",
              "      <td>-19</td>\n",
              "      <td>-20</td>\n",
              "      <td>-29</td>\n",
              "      <td>-36</td>\n",
              "      <td>-20</td>\n",
              "      <td>...</td>\n",
              "      <td>38</td>\n",
              "      <td>26</td>\n",
              "      <td>10</td>\n",
              "      <td>-4</td>\n",
              "      <td>-13</td>\n",
              "      <td>-8</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>19</td>\n",
              "      <td>29</td>\n",
              "      <td>57</td>\n",
              "      <td>63</td>\n",
              "      <td>45</td>\n",
              "      <td>7</td>\n",
              "      <td>-13</td>\n",
              "      <td>-23</td>\n",
              "      <td>-9</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>-2</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>27</td>\n",
              "      <td>27</td>\n",
              "      <td>14</td>\n",
              "      <td>15</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>-12</td>\n",
              "      <td>-32</td>\n",
              "      <td>-41</td>\n",
              "      <td>-65</td>\n",
              "      <td>-83</td>\n",
              "      <td>-89</td>\n",
              "      <td>-73</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 180 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0   X1   X2   X3   X4   X5  ...  X174  X175  X176  X177  X178  y\n",
              "0  X21.V1.791  135  190  229  223  192  ...  -103  -127  -116   -83   -51  4\n",
              "1  X15.V1.924  386  382  356  331  320  ...   157   156   154   143   129  1\n",
              "2     X8.V1.1  -32  -39  -47  -37  -32  ...   -12   -30   -35   -35   -36  5\n",
              "3   X16.V1.60 -105 -101  -96  -92  -89  ...   -85   -77   -72   -69   -65  5\n",
              "4   X20.V1.54   -9  -65  -98 -102  -78  ...   -41   -65   -83   -89   -73  5\n",
              "\n",
              "[5 rows x 180 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kUjv__RHCYM"
      },
      "source": [
        "df.drop(df.columns[0], axis=1, inplace=True) # Drop the 1st column from the table"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "-OIjtFD-HG-I",
        "outputId": "0d5417cc-8ea5-405e-80ae-47ff2044521a"
      },
      "source": [
        "# Created a features dataframe from the original dataframe\n",
        "feature_data = df.iloc[:,0:-1]\n",
        "feature_data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "      <th>X4</th>\n",
              "      <th>X5</th>\n",
              "      <th>X6</th>\n",
              "      <th>X7</th>\n",
              "      <th>X8</th>\n",
              "      <th>X9</th>\n",
              "      <th>X10</th>\n",
              "      <th>X11</th>\n",
              "      <th>X12</th>\n",
              "      <th>X13</th>\n",
              "      <th>X14</th>\n",
              "      <th>X15</th>\n",
              "      <th>X16</th>\n",
              "      <th>X17</th>\n",
              "      <th>X18</th>\n",
              "      <th>X19</th>\n",
              "      <th>X20</th>\n",
              "      <th>X21</th>\n",
              "      <th>X22</th>\n",
              "      <th>X23</th>\n",
              "      <th>X24</th>\n",
              "      <th>X25</th>\n",
              "      <th>X26</th>\n",
              "      <th>X27</th>\n",
              "      <th>X28</th>\n",
              "      <th>X29</th>\n",
              "      <th>X30</th>\n",
              "      <th>X31</th>\n",
              "      <th>X32</th>\n",
              "      <th>X33</th>\n",
              "      <th>X34</th>\n",
              "      <th>X35</th>\n",
              "      <th>X36</th>\n",
              "      <th>X37</th>\n",
              "      <th>X38</th>\n",
              "      <th>X39</th>\n",
              "      <th>X40</th>\n",
              "      <th>...</th>\n",
              "      <th>X139</th>\n",
              "      <th>X140</th>\n",
              "      <th>X141</th>\n",
              "      <th>X142</th>\n",
              "      <th>X143</th>\n",
              "      <th>X144</th>\n",
              "      <th>X145</th>\n",
              "      <th>X146</th>\n",
              "      <th>X147</th>\n",
              "      <th>X148</th>\n",
              "      <th>X149</th>\n",
              "      <th>X150</th>\n",
              "      <th>X151</th>\n",
              "      <th>X152</th>\n",
              "      <th>X153</th>\n",
              "      <th>X154</th>\n",
              "      <th>X155</th>\n",
              "      <th>X156</th>\n",
              "      <th>X157</th>\n",
              "      <th>X158</th>\n",
              "      <th>X159</th>\n",
              "      <th>X160</th>\n",
              "      <th>X161</th>\n",
              "      <th>X162</th>\n",
              "      <th>X163</th>\n",
              "      <th>X164</th>\n",
              "      <th>X165</th>\n",
              "      <th>X166</th>\n",
              "      <th>X167</th>\n",
              "      <th>X168</th>\n",
              "      <th>X169</th>\n",
              "      <th>X170</th>\n",
              "      <th>X171</th>\n",
              "      <th>X172</th>\n",
              "      <th>X173</th>\n",
              "      <th>X174</th>\n",
              "      <th>X175</th>\n",
              "      <th>X176</th>\n",
              "      <th>X177</th>\n",
              "      <th>X178</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>135</td>\n",
              "      <td>190</td>\n",
              "      <td>229</td>\n",
              "      <td>223</td>\n",
              "      <td>192</td>\n",
              "      <td>125</td>\n",
              "      <td>55</td>\n",
              "      <td>-9</td>\n",
              "      <td>-33</td>\n",
              "      <td>-38</td>\n",
              "      <td>-10</td>\n",
              "      <td>35</td>\n",
              "      <td>64</td>\n",
              "      <td>113</td>\n",
              "      <td>152</td>\n",
              "      <td>164</td>\n",
              "      <td>127</td>\n",
              "      <td>50</td>\n",
              "      <td>-47</td>\n",
              "      <td>-121</td>\n",
              "      <td>-138</td>\n",
              "      <td>-125</td>\n",
              "      <td>-101</td>\n",
              "      <td>-50</td>\n",
              "      <td>11</td>\n",
              "      <td>39</td>\n",
              "      <td>24</td>\n",
              "      <td>48</td>\n",
              "      <td>64</td>\n",
              "      <td>46</td>\n",
              "      <td>13</td>\n",
              "      <td>-19</td>\n",
              "      <td>-61</td>\n",
              "      <td>-96</td>\n",
              "      <td>-130</td>\n",
              "      <td>-132</td>\n",
              "      <td>-116</td>\n",
              "      <td>-115</td>\n",
              "      <td>-71</td>\n",
              "      <td>-14</td>\n",
              "      <td>...</td>\n",
              "      <td>43</td>\n",
              "      <td>54</td>\n",
              "      <td>90</td>\n",
              "      <td>111</td>\n",
              "      <td>107</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>18</td>\n",
              "      <td>-25</td>\n",
              "      <td>-69</td>\n",
              "      <td>-65</td>\n",
              "      <td>-44</td>\n",
              "      <td>-33</td>\n",
              "      <td>-57</td>\n",
              "      <td>-88</td>\n",
              "      <td>-114</td>\n",
              "      <td>-130</td>\n",
              "      <td>-114</td>\n",
              "      <td>-83</td>\n",
              "      <td>-53</td>\n",
              "      <td>-79</td>\n",
              "      <td>-72</td>\n",
              "      <td>-85</td>\n",
              "      <td>-109</td>\n",
              "      <td>-98</td>\n",
              "      <td>-72</td>\n",
              "      <td>-65</td>\n",
              "      <td>-63</td>\n",
              "      <td>-11</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>-17</td>\n",
              "      <td>-15</td>\n",
              "      <td>-31</td>\n",
              "      <td>-77</td>\n",
              "      <td>-103</td>\n",
              "      <td>-127</td>\n",
              "      <td>-116</td>\n",
              "      <td>-83</td>\n",
              "      <td>-51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>386</td>\n",
              "      <td>382</td>\n",
              "      <td>356</td>\n",
              "      <td>331</td>\n",
              "      <td>320</td>\n",
              "      <td>315</td>\n",
              "      <td>307</td>\n",
              "      <td>272</td>\n",
              "      <td>244</td>\n",
              "      <td>232</td>\n",
              "      <td>237</td>\n",
              "      <td>258</td>\n",
              "      <td>212</td>\n",
              "      <td>2</td>\n",
              "      <td>-267</td>\n",
              "      <td>-605</td>\n",
              "      <td>-850</td>\n",
              "      <td>-1001</td>\n",
              "      <td>-1109</td>\n",
              "      <td>-1090</td>\n",
              "      <td>-967</td>\n",
              "      <td>-746</td>\n",
              "      <td>-464</td>\n",
              "      <td>-152</td>\n",
              "      <td>118</td>\n",
              "      <td>318</td>\n",
              "      <td>427</td>\n",
              "      <td>473</td>\n",
              "      <td>485</td>\n",
              "      <td>447</td>\n",
              "      <td>397</td>\n",
              "      <td>339</td>\n",
              "      <td>312</td>\n",
              "      <td>314</td>\n",
              "      <td>326</td>\n",
              "      <td>335</td>\n",
              "      <td>332</td>\n",
              "      <td>324</td>\n",
              "      <td>310</td>\n",
              "      <td>312</td>\n",
              "      <td>...</td>\n",
              "      <td>-136</td>\n",
              "      <td>27</td>\n",
              "      <td>146</td>\n",
              "      <td>229</td>\n",
              "      <td>269</td>\n",
              "      <td>297</td>\n",
              "      <td>307</td>\n",
              "      <td>303</td>\n",
              "      <td>305</td>\n",
              "      <td>306</td>\n",
              "      <td>307</td>\n",
              "      <td>280</td>\n",
              "      <td>231</td>\n",
              "      <td>159</td>\n",
              "      <td>85</td>\n",
              "      <td>51</td>\n",
              "      <td>43</td>\n",
              "      <td>62</td>\n",
              "      <td>63</td>\n",
              "      <td>63</td>\n",
              "      <td>69</td>\n",
              "      <td>89</td>\n",
              "      <td>123</td>\n",
              "      <td>136</td>\n",
              "      <td>127</td>\n",
              "      <td>102</td>\n",
              "      <td>95</td>\n",
              "      <td>105</td>\n",
              "      <td>131</td>\n",
              "      <td>163</td>\n",
              "      <td>168</td>\n",
              "      <td>164</td>\n",
              "      <td>150</td>\n",
              "      <td>146</td>\n",
              "      <td>152</td>\n",
              "      <td>157</td>\n",
              "      <td>156</td>\n",
              "      <td>154</td>\n",
              "      <td>143</td>\n",
              "      <td>129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-32</td>\n",
              "      <td>-39</td>\n",
              "      <td>-47</td>\n",
              "      <td>-37</td>\n",
              "      <td>-32</td>\n",
              "      <td>-36</td>\n",
              "      <td>-57</td>\n",
              "      <td>-73</td>\n",
              "      <td>-85</td>\n",
              "      <td>-94</td>\n",
              "      <td>-99</td>\n",
              "      <td>-94</td>\n",
              "      <td>-96</td>\n",
              "      <td>-104</td>\n",
              "      <td>-103</td>\n",
              "      <td>-92</td>\n",
              "      <td>-75</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-53</td>\n",
              "      <td>-37</td>\n",
              "      <td>-14</td>\n",
              "      <td>-10</td>\n",
              "      <td>-39</td>\n",
              "      <td>-78</td>\n",
              "      <td>-102</td>\n",
              "      <td>-98</td>\n",
              "      <td>-80</td>\n",
              "      <td>-54</td>\n",
              "      <td>-40</td>\n",
              "      <td>-35</td>\n",
              "      <td>-39</td>\n",
              "      <td>-32</td>\n",
              "      <td>-13</td>\n",
              "      <td>7</td>\n",
              "      <td>34</td>\n",
              "      <td>41</td>\n",
              "      <td>33</td>\n",
              "      <td>6</td>\n",
              "      <td>-15</td>\n",
              "      <td>...</td>\n",
              "      <td>-54</td>\n",
              "      <td>-82</td>\n",
              "      <td>-107</td>\n",
              "      <td>-126</td>\n",
              "      <td>-124</td>\n",
              "      <td>-108</td>\n",
              "      <td>-84</td>\n",
              "      <td>-68</td>\n",
              "      <td>-61</td>\n",
              "      <td>-56</td>\n",
              "      <td>-63</td>\n",
              "      <td>-62</td>\n",
              "      <td>-33</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>45</td>\n",
              "      <td>37</td>\n",
              "      <td>48</td>\n",
              "      <td>62</td>\n",
              "      <td>80</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>-11</td>\n",
              "      <td>-39</td>\n",
              "      <td>-44</td>\n",
              "      <td>-42</td>\n",
              "      <td>-45</td>\n",
              "      <td>-48</td>\n",
              "      <td>-42</td>\n",
              "      <td>-6</td>\n",
              "      <td>29</td>\n",
              "      <td>57</td>\n",
              "      <td>64</td>\n",
              "      <td>48</td>\n",
              "      <td>19</td>\n",
              "      <td>-12</td>\n",
              "      <td>-30</td>\n",
              "      <td>-35</td>\n",
              "      <td>-35</td>\n",
              "      <td>-36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-105</td>\n",
              "      <td>-101</td>\n",
              "      <td>-96</td>\n",
              "      <td>-92</td>\n",
              "      <td>-89</td>\n",
              "      <td>-95</td>\n",
              "      <td>-102</td>\n",
              "      <td>-100</td>\n",
              "      <td>-87</td>\n",
              "      <td>-79</td>\n",
              "      <td>-72</td>\n",
              "      <td>-68</td>\n",
              "      <td>-74</td>\n",
              "      <td>-80</td>\n",
              "      <td>-83</td>\n",
              "      <td>-73</td>\n",
              "      <td>-68</td>\n",
              "      <td>-61</td>\n",
              "      <td>-58</td>\n",
              "      <td>-59</td>\n",
              "      <td>-64</td>\n",
              "      <td>-79</td>\n",
              "      <td>-84</td>\n",
              "      <td>-97</td>\n",
              "      <td>-94</td>\n",
              "      <td>-84</td>\n",
              "      <td>-77</td>\n",
              "      <td>-75</td>\n",
              "      <td>-72</td>\n",
              "      <td>-68</td>\n",
              "      <td>-76</td>\n",
              "      <td>-76</td>\n",
              "      <td>-72</td>\n",
              "      <td>-67</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-67</td>\n",
              "      <td>-68</td>\n",
              "      <td>-69</td>\n",
              "      <td>...</td>\n",
              "      <td>-61</td>\n",
              "      <td>-69</td>\n",
              "      <td>-66</td>\n",
              "      <td>-74</td>\n",
              "      <td>-69</td>\n",
              "      <td>-61</td>\n",
              "      <td>-51</td>\n",
              "      <td>-45</td>\n",
              "      <td>-45</td>\n",
              "      <td>-49</td>\n",
              "      <td>-58</td>\n",
              "      <td>-64</td>\n",
              "      <td>-78</td>\n",
              "      <td>-80</td>\n",
              "      <td>-90</td>\n",
              "      <td>-87</td>\n",
              "      <td>-83</td>\n",
              "      <td>-78</td>\n",
              "      <td>-64</td>\n",
              "      <td>-38</td>\n",
              "      <td>-22</td>\n",
              "      <td>-29</td>\n",
              "      <td>-42</td>\n",
              "      <td>-51</td>\n",
              "      <td>-68</td>\n",
              "      <td>-71</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-74</td>\n",
              "      <td>-74</td>\n",
              "      <td>-80</td>\n",
              "      <td>-82</td>\n",
              "      <td>-81</td>\n",
              "      <td>-80</td>\n",
              "      <td>-77</td>\n",
              "      <td>-85</td>\n",
              "      <td>-77</td>\n",
              "      <td>-72</td>\n",
              "      <td>-69</td>\n",
              "      <td>-65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-9</td>\n",
              "      <td>-65</td>\n",
              "      <td>-98</td>\n",
              "      <td>-102</td>\n",
              "      <td>-78</td>\n",
              "      <td>-48</td>\n",
              "      <td>-16</td>\n",
              "      <td>0</td>\n",
              "      <td>-21</td>\n",
              "      <td>-59</td>\n",
              "      <td>-90</td>\n",
              "      <td>-103</td>\n",
              "      <td>-84</td>\n",
              "      <td>-43</td>\n",
              "      <td>-9</td>\n",
              "      <td>3</td>\n",
              "      <td>-21</td>\n",
              "      <td>-60</td>\n",
              "      <td>-96</td>\n",
              "      <td>-103</td>\n",
              "      <td>-75</td>\n",
              "      <td>-29</td>\n",
              "      <td>14</td>\n",
              "      <td>55</td>\n",
              "      <td>78</td>\n",
              "      <td>73</td>\n",
              "      <td>28</td>\n",
              "      <td>-13</td>\n",
              "      <td>-43</td>\n",
              "      <td>-68</td>\n",
              "      <td>-78</td>\n",
              "      <td>-75</td>\n",
              "      <td>-55</td>\n",
              "      <td>-41</td>\n",
              "      <td>-19</td>\n",
              "      <td>-20</td>\n",
              "      <td>-29</td>\n",
              "      <td>-36</td>\n",
              "      <td>-20</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>37</td>\n",
              "      <td>38</td>\n",
              "      <td>26</td>\n",
              "      <td>10</td>\n",
              "      <td>-4</td>\n",
              "      <td>-13</td>\n",
              "      <td>-8</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>19</td>\n",
              "      <td>29</td>\n",
              "      <td>57</td>\n",
              "      <td>63</td>\n",
              "      <td>45</td>\n",
              "      <td>7</td>\n",
              "      <td>-13</td>\n",
              "      <td>-23</td>\n",
              "      <td>-9</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>-2</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>27</td>\n",
              "      <td>27</td>\n",
              "      <td>14</td>\n",
              "      <td>15</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>-12</td>\n",
              "      <td>-32</td>\n",
              "      <td>-41</td>\n",
              "      <td>-65</td>\n",
              "      <td>-83</td>\n",
              "      <td>-89</td>\n",
              "      <td>-73</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 178 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    X1   X2   X3   X4   X5   X6   X7  ...  X172  X173  X174  X175  X176  X177  X178\n",
              "0  135  190  229  223  192  125   55  ...   -31   -77  -103  -127  -116   -83   -51\n",
              "1  386  382  356  331  320  315  307  ...   146   152   157   156   154   143   129\n",
              "2  -32  -39  -47  -37  -32  -36  -57  ...    48    19   -12   -30   -35   -35   -36\n",
              "3 -105 -101  -96  -92  -89  -95 -102  ...   -80   -77   -85   -77   -72   -69   -65\n",
              "4   -9  -65  -98 -102  -78  -48  -16  ...   -12   -32   -41   -65   -83   -89   -73\n",
              "\n",
              "[5 rows x 178 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "eF-TaBT9HHBA",
        "outputId": "b4d45dac-6a1c-460a-eb21-2e4c1201e3e6"
      },
      "source": [
        "#Mapping Target for one V/S all logistic classification\n",
        "df['y'] = df.y.map({1: 1, 2: 0, 3: 0, 4: 0, 5:0}) # CLass 1-  Recording of seizure activity, Class 0 - No seizure activity recorded\n",
        "df.head(5)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "      <th>X4</th>\n",
              "      <th>X5</th>\n",
              "      <th>X6</th>\n",
              "      <th>X7</th>\n",
              "      <th>X8</th>\n",
              "      <th>X9</th>\n",
              "      <th>X10</th>\n",
              "      <th>X11</th>\n",
              "      <th>X12</th>\n",
              "      <th>X13</th>\n",
              "      <th>X14</th>\n",
              "      <th>X15</th>\n",
              "      <th>X16</th>\n",
              "      <th>X17</th>\n",
              "      <th>X18</th>\n",
              "      <th>X19</th>\n",
              "      <th>X20</th>\n",
              "      <th>X21</th>\n",
              "      <th>X22</th>\n",
              "      <th>X23</th>\n",
              "      <th>X24</th>\n",
              "      <th>X25</th>\n",
              "      <th>X26</th>\n",
              "      <th>X27</th>\n",
              "      <th>X28</th>\n",
              "      <th>X29</th>\n",
              "      <th>X30</th>\n",
              "      <th>X31</th>\n",
              "      <th>X32</th>\n",
              "      <th>X33</th>\n",
              "      <th>X34</th>\n",
              "      <th>X35</th>\n",
              "      <th>X36</th>\n",
              "      <th>X37</th>\n",
              "      <th>X38</th>\n",
              "      <th>X39</th>\n",
              "      <th>X40</th>\n",
              "      <th>...</th>\n",
              "      <th>X140</th>\n",
              "      <th>X141</th>\n",
              "      <th>X142</th>\n",
              "      <th>X143</th>\n",
              "      <th>X144</th>\n",
              "      <th>X145</th>\n",
              "      <th>X146</th>\n",
              "      <th>X147</th>\n",
              "      <th>X148</th>\n",
              "      <th>X149</th>\n",
              "      <th>X150</th>\n",
              "      <th>X151</th>\n",
              "      <th>X152</th>\n",
              "      <th>X153</th>\n",
              "      <th>X154</th>\n",
              "      <th>X155</th>\n",
              "      <th>X156</th>\n",
              "      <th>X157</th>\n",
              "      <th>X158</th>\n",
              "      <th>X159</th>\n",
              "      <th>X160</th>\n",
              "      <th>X161</th>\n",
              "      <th>X162</th>\n",
              "      <th>X163</th>\n",
              "      <th>X164</th>\n",
              "      <th>X165</th>\n",
              "      <th>X166</th>\n",
              "      <th>X167</th>\n",
              "      <th>X168</th>\n",
              "      <th>X169</th>\n",
              "      <th>X170</th>\n",
              "      <th>X171</th>\n",
              "      <th>X172</th>\n",
              "      <th>X173</th>\n",
              "      <th>X174</th>\n",
              "      <th>X175</th>\n",
              "      <th>X176</th>\n",
              "      <th>X177</th>\n",
              "      <th>X178</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>135</td>\n",
              "      <td>190</td>\n",
              "      <td>229</td>\n",
              "      <td>223</td>\n",
              "      <td>192</td>\n",
              "      <td>125</td>\n",
              "      <td>55</td>\n",
              "      <td>-9</td>\n",
              "      <td>-33</td>\n",
              "      <td>-38</td>\n",
              "      <td>-10</td>\n",
              "      <td>35</td>\n",
              "      <td>64</td>\n",
              "      <td>113</td>\n",
              "      <td>152</td>\n",
              "      <td>164</td>\n",
              "      <td>127</td>\n",
              "      <td>50</td>\n",
              "      <td>-47</td>\n",
              "      <td>-121</td>\n",
              "      <td>-138</td>\n",
              "      <td>-125</td>\n",
              "      <td>-101</td>\n",
              "      <td>-50</td>\n",
              "      <td>11</td>\n",
              "      <td>39</td>\n",
              "      <td>24</td>\n",
              "      <td>48</td>\n",
              "      <td>64</td>\n",
              "      <td>46</td>\n",
              "      <td>13</td>\n",
              "      <td>-19</td>\n",
              "      <td>-61</td>\n",
              "      <td>-96</td>\n",
              "      <td>-130</td>\n",
              "      <td>-132</td>\n",
              "      <td>-116</td>\n",
              "      <td>-115</td>\n",
              "      <td>-71</td>\n",
              "      <td>-14</td>\n",
              "      <td>...</td>\n",
              "      <td>54</td>\n",
              "      <td>90</td>\n",
              "      <td>111</td>\n",
              "      <td>107</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "      <td>18</td>\n",
              "      <td>-25</td>\n",
              "      <td>-69</td>\n",
              "      <td>-65</td>\n",
              "      <td>-44</td>\n",
              "      <td>-33</td>\n",
              "      <td>-57</td>\n",
              "      <td>-88</td>\n",
              "      <td>-114</td>\n",
              "      <td>-130</td>\n",
              "      <td>-114</td>\n",
              "      <td>-83</td>\n",
              "      <td>-53</td>\n",
              "      <td>-79</td>\n",
              "      <td>-72</td>\n",
              "      <td>-85</td>\n",
              "      <td>-109</td>\n",
              "      <td>-98</td>\n",
              "      <td>-72</td>\n",
              "      <td>-65</td>\n",
              "      <td>-63</td>\n",
              "      <td>-11</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>-17</td>\n",
              "      <td>-15</td>\n",
              "      <td>-31</td>\n",
              "      <td>-77</td>\n",
              "      <td>-103</td>\n",
              "      <td>-127</td>\n",
              "      <td>-116</td>\n",
              "      <td>-83</td>\n",
              "      <td>-51</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>386</td>\n",
              "      <td>382</td>\n",
              "      <td>356</td>\n",
              "      <td>331</td>\n",
              "      <td>320</td>\n",
              "      <td>315</td>\n",
              "      <td>307</td>\n",
              "      <td>272</td>\n",
              "      <td>244</td>\n",
              "      <td>232</td>\n",
              "      <td>237</td>\n",
              "      <td>258</td>\n",
              "      <td>212</td>\n",
              "      <td>2</td>\n",
              "      <td>-267</td>\n",
              "      <td>-605</td>\n",
              "      <td>-850</td>\n",
              "      <td>-1001</td>\n",
              "      <td>-1109</td>\n",
              "      <td>-1090</td>\n",
              "      <td>-967</td>\n",
              "      <td>-746</td>\n",
              "      <td>-464</td>\n",
              "      <td>-152</td>\n",
              "      <td>118</td>\n",
              "      <td>318</td>\n",
              "      <td>427</td>\n",
              "      <td>473</td>\n",
              "      <td>485</td>\n",
              "      <td>447</td>\n",
              "      <td>397</td>\n",
              "      <td>339</td>\n",
              "      <td>312</td>\n",
              "      <td>314</td>\n",
              "      <td>326</td>\n",
              "      <td>335</td>\n",
              "      <td>332</td>\n",
              "      <td>324</td>\n",
              "      <td>310</td>\n",
              "      <td>312</td>\n",
              "      <td>...</td>\n",
              "      <td>27</td>\n",
              "      <td>146</td>\n",
              "      <td>229</td>\n",
              "      <td>269</td>\n",
              "      <td>297</td>\n",
              "      <td>307</td>\n",
              "      <td>303</td>\n",
              "      <td>305</td>\n",
              "      <td>306</td>\n",
              "      <td>307</td>\n",
              "      <td>280</td>\n",
              "      <td>231</td>\n",
              "      <td>159</td>\n",
              "      <td>85</td>\n",
              "      <td>51</td>\n",
              "      <td>43</td>\n",
              "      <td>62</td>\n",
              "      <td>63</td>\n",
              "      <td>63</td>\n",
              "      <td>69</td>\n",
              "      <td>89</td>\n",
              "      <td>123</td>\n",
              "      <td>136</td>\n",
              "      <td>127</td>\n",
              "      <td>102</td>\n",
              "      <td>95</td>\n",
              "      <td>105</td>\n",
              "      <td>131</td>\n",
              "      <td>163</td>\n",
              "      <td>168</td>\n",
              "      <td>164</td>\n",
              "      <td>150</td>\n",
              "      <td>146</td>\n",
              "      <td>152</td>\n",
              "      <td>157</td>\n",
              "      <td>156</td>\n",
              "      <td>154</td>\n",
              "      <td>143</td>\n",
              "      <td>129</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-32</td>\n",
              "      <td>-39</td>\n",
              "      <td>-47</td>\n",
              "      <td>-37</td>\n",
              "      <td>-32</td>\n",
              "      <td>-36</td>\n",
              "      <td>-57</td>\n",
              "      <td>-73</td>\n",
              "      <td>-85</td>\n",
              "      <td>-94</td>\n",
              "      <td>-99</td>\n",
              "      <td>-94</td>\n",
              "      <td>-96</td>\n",
              "      <td>-104</td>\n",
              "      <td>-103</td>\n",
              "      <td>-92</td>\n",
              "      <td>-75</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-53</td>\n",
              "      <td>-37</td>\n",
              "      <td>-14</td>\n",
              "      <td>-10</td>\n",
              "      <td>-39</td>\n",
              "      <td>-78</td>\n",
              "      <td>-102</td>\n",
              "      <td>-98</td>\n",
              "      <td>-80</td>\n",
              "      <td>-54</td>\n",
              "      <td>-40</td>\n",
              "      <td>-35</td>\n",
              "      <td>-39</td>\n",
              "      <td>-32</td>\n",
              "      <td>-13</td>\n",
              "      <td>7</td>\n",
              "      <td>34</td>\n",
              "      <td>41</td>\n",
              "      <td>33</td>\n",
              "      <td>6</td>\n",
              "      <td>-15</td>\n",
              "      <td>...</td>\n",
              "      <td>-82</td>\n",
              "      <td>-107</td>\n",
              "      <td>-126</td>\n",
              "      <td>-124</td>\n",
              "      <td>-108</td>\n",
              "      <td>-84</td>\n",
              "      <td>-68</td>\n",
              "      <td>-61</td>\n",
              "      <td>-56</td>\n",
              "      <td>-63</td>\n",
              "      <td>-62</td>\n",
              "      <td>-33</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>45</td>\n",
              "      <td>37</td>\n",
              "      <td>48</td>\n",
              "      <td>62</td>\n",
              "      <td>80</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>-11</td>\n",
              "      <td>-39</td>\n",
              "      <td>-44</td>\n",
              "      <td>-42</td>\n",
              "      <td>-45</td>\n",
              "      <td>-48</td>\n",
              "      <td>-42</td>\n",
              "      <td>-6</td>\n",
              "      <td>29</td>\n",
              "      <td>57</td>\n",
              "      <td>64</td>\n",
              "      <td>48</td>\n",
              "      <td>19</td>\n",
              "      <td>-12</td>\n",
              "      <td>-30</td>\n",
              "      <td>-35</td>\n",
              "      <td>-35</td>\n",
              "      <td>-36</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-105</td>\n",
              "      <td>-101</td>\n",
              "      <td>-96</td>\n",
              "      <td>-92</td>\n",
              "      <td>-89</td>\n",
              "      <td>-95</td>\n",
              "      <td>-102</td>\n",
              "      <td>-100</td>\n",
              "      <td>-87</td>\n",
              "      <td>-79</td>\n",
              "      <td>-72</td>\n",
              "      <td>-68</td>\n",
              "      <td>-74</td>\n",
              "      <td>-80</td>\n",
              "      <td>-83</td>\n",
              "      <td>-73</td>\n",
              "      <td>-68</td>\n",
              "      <td>-61</td>\n",
              "      <td>-58</td>\n",
              "      <td>-59</td>\n",
              "      <td>-64</td>\n",
              "      <td>-79</td>\n",
              "      <td>-84</td>\n",
              "      <td>-97</td>\n",
              "      <td>-94</td>\n",
              "      <td>-84</td>\n",
              "      <td>-77</td>\n",
              "      <td>-75</td>\n",
              "      <td>-72</td>\n",
              "      <td>-68</td>\n",
              "      <td>-76</td>\n",
              "      <td>-76</td>\n",
              "      <td>-72</td>\n",
              "      <td>-67</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-67</td>\n",
              "      <td>-68</td>\n",
              "      <td>-69</td>\n",
              "      <td>...</td>\n",
              "      <td>-69</td>\n",
              "      <td>-66</td>\n",
              "      <td>-74</td>\n",
              "      <td>-69</td>\n",
              "      <td>-61</td>\n",
              "      <td>-51</td>\n",
              "      <td>-45</td>\n",
              "      <td>-45</td>\n",
              "      <td>-49</td>\n",
              "      <td>-58</td>\n",
              "      <td>-64</td>\n",
              "      <td>-78</td>\n",
              "      <td>-80</td>\n",
              "      <td>-90</td>\n",
              "      <td>-87</td>\n",
              "      <td>-83</td>\n",
              "      <td>-78</td>\n",
              "      <td>-64</td>\n",
              "      <td>-38</td>\n",
              "      <td>-22</td>\n",
              "      <td>-29</td>\n",
              "      <td>-42</td>\n",
              "      <td>-51</td>\n",
              "      <td>-68</td>\n",
              "      <td>-71</td>\n",
              "      <td>-69</td>\n",
              "      <td>-69</td>\n",
              "      <td>-74</td>\n",
              "      <td>-74</td>\n",
              "      <td>-80</td>\n",
              "      <td>-82</td>\n",
              "      <td>-81</td>\n",
              "      <td>-80</td>\n",
              "      <td>-77</td>\n",
              "      <td>-85</td>\n",
              "      <td>-77</td>\n",
              "      <td>-72</td>\n",
              "      <td>-69</td>\n",
              "      <td>-65</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-9</td>\n",
              "      <td>-65</td>\n",
              "      <td>-98</td>\n",
              "      <td>-102</td>\n",
              "      <td>-78</td>\n",
              "      <td>-48</td>\n",
              "      <td>-16</td>\n",
              "      <td>0</td>\n",
              "      <td>-21</td>\n",
              "      <td>-59</td>\n",
              "      <td>-90</td>\n",
              "      <td>-103</td>\n",
              "      <td>-84</td>\n",
              "      <td>-43</td>\n",
              "      <td>-9</td>\n",
              "      <td>3</td>\n",
              "      <td>-21</td>\n",
              "      <td>-60</td>\n",
              "      <td>-96</td>\n",
              "      <td>-103</td>\n",
              "      <td>-75</td>\n",
              "      <td>-29</td>\n",
              "      <td>14</td>\n",
              "      <td>55</td>\n",
              "      <td>78</td>\n",
              "      <td>73</td>\n",
              "      <td>28</td>\n",
              "      <td>-13</td>\n",
              "      <td>-43</td>\n",
              "      <td>-68</td>\n",
              "      <td>-78</td>\n",
              "      <td>-75</td>\n",
              "      <td>-55</td>\n",
              "      <td>-41</td>\n",
              "      <td>-19</td>\n",
              "      <td>-20</td>\n",
              "      <td>-29</td>\n",
              "      <td>-36</td>\n",
              "      <td>-20</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>38</td>\n",
              "      <td>26</td>\n",
              "      <td>10</td>\n",
              "      <td>-4</td>\n",
              "      <td>-13</td>\n",
              "      <td>-8</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>19</td>\n",
              "      <td>29</td>\n",
              "      <td>57</td>\n",
              "      <td>63</td>\n",
              "      <td>45</td>\n",
              "      <td>7</td>\n",
              "      <td>-13</td>\n",
              "      <td>-23</td>\n",
              "      <td>-9</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>-2</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>27</td>\n",
              "      <td>27</td>\n",
              "      <td>14</td>\n",
              "      <td>15</td>\n",
              "      <td>11</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>-12</td>\n",
              "      <td>-32</td>\n",
              "      <td>-41</td>\n",
              "      <td>-65</td>\n",
              "      <td>-83</td>\n",
              "      <td>-89</td>\n",
              "      <td>-73</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 179 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    X1   X2   X3   X4   X5   X6   X7  ...  X173  X174  X175  X176  X177  X178  y\n",
              "0  135  190  229  223  192  125   55  ...   -77  -103  -127  -116   -83   -51  0\n",
              "1  386  382  356  331  320  315  307  ...   152   157   156   154   143   129  1\n",
              "2  -32  -39  -47  -37  -32  -36  -57  ...    19   -12   -30   -35   -35   -36  0\n",
              "3 -105 -101  -96  -92  -89  -95 -102  ...   -77   -85   -77   -72   -69   -65  0\n",
              "4   -9  -65  -98 -102  -78  -48  -16  ...   -32   -41   -65   -83   -89   -73  0\n",
              "\n",
              "[5 rows x 179 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hRpTczaHLE9"
      },
      "source": [
        "#dataprocessing\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "X_train_minmax = min_max_scaler.fit_transform(feature_data)\n",
        "#X_train_minmax"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfYN8VUOHLIF"
      },
      "source": [
        "y = df[\"y\"]\n",
        "x_train, x_test, y_train, y_test = SplitData(X_train_minmax,y, 0.30 )"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgADEfSTLVwe"
      },
      "source": [
        "##### CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLS3-JoeTDRL"
      },
      "source": [
        "num_folds=10\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfTap75nNl1I"
      },
      "source": [
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "X_train_tf = np.expand_dims(x_train, axis=2)\n",
        "X_valid_tf = np.expand_dims(x_test, axis=2)\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train , num_classes=2)\n",
        "y_valid = keras.utils.to_categorical(y_test , num_classes=2)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYwekY1QGk5N",
        "outputId": "87e26c79-6f21-4535-f473-b78e159cf9be"
      },
      "source": [
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(X_train_tf, y_train):\n",
        "\n",
        "  # Define the model architecture\n",
        "  model = Sequential()\n",
        "  model.add(Convolution1D(64, 10, strides=2, padding='same', activation='relu',  input_shape=(178, 1)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(MaxPooling1D(3))\n",
        "  model.add(Convolution1D(40, 5, strides=2, padding='same', activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(MaxPooling1D(3))\n",
        "  model.add(Convolution1D(32, 4, strides=1, padding='same', activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(MaxPooling1D(3))\n",
        "  model.add(GlobalAveragePooling1D())\n",
        "  model.add(Dense(50, activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "  \n",
        "  # Compile the model\n",
        "  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "  data_train=model.fit(X_train_tf, y_train, epochs= 15, validation_data=(X_valid_tf, y_valid), batch_size=20)\n",
        "\n",
        "  # Generate generalization metrics\n",
        "  scores = model.evaluate(X_train_tf[test], y_train[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "# == Provide average scores ==\n",
        "\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/15\n",
            "403/403 [==============================] - 5s 10ms/step - loss: 0.5376 - accuracy: 0.7861 - val_loss: 0.3180 - val_accuracy: 0.9310\n",
            "Epoch 2/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.2435 - accuracy: 0.9272 - val_loss: 0.2426 - val_accuracy: 0.9516\n",
            "Epoch 3/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1745 - accuracy: 0.9434 - val_loss: 0.1583 - val_accuracy: 0.9478\n",
            "Epoch 4/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1699 - accuracy: 0.9444 - val_loss: 0.2027 - val_accuracy: 0.9304\n",
            "Epoch 5/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1518 - accuracy: 0.9489 - val_loss: 0.1326 - val_accuracy: 0.9562\n",
            "Epoch 6/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1355 - accuracy: 0.9521 - val_loss: 0.1476 - val_accuracy: 0.9467\n",
            "Epoch 7/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1341 - accuracy: 0.9548 - val_loss: 0.1065 - val_accuracy: 0.9597\n",
            "Epoch 8/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1142 - accuracy: 0.9608 - val_loss: 0.1383 - val_accuracy: 0.9504\n",
            "Epoch 9/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1138 - accuracy: 0.9587 - val_loss: 0.1615 - val_accuracy: 0.9478\n",
            "Epoch 10/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1073 - accuracy: 0.9589 - val_loss: 0.0958 - val_accuracy: 0.9606\n",
            "Epoch 11/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1030 - accuracy: 0.9615 - val_loss: 0.1013 - val_accuracy: 0.9591\n",
            "Epoch 12/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.0929 - accuracy: 0.9664 - val_loss: 0.0962 - val_accuracy: 0.9617\n",
            "Epoch 13/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.0993 - accuracy: 0.9665 - val_loss: 0.1249 - val_accuracy: 0.9545\n",
            "Epoch 14/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.0936 - accuracy: 0.9667 - val_loss: 0.1120 - val_accuracy: 0.9583\n",
            "Epoch 15/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.0938 - accuracy: 0.9651 - val_loss: 0.1045 - val_accuracy: 0.9600\n",
            "Score for fold 1: loss of 0.1071377769112587; accuracy of 95.5279529094696%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/15\n",
            "403/403 [==============================] - 5s 9ms/step - loss: 0.5183 - accuracy: 0.7941 - val_loss: 0.2850 - val_accuracy: 0.9006\n",
            "Epoch 2/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.2494 - accuracy: 0.9154 - val_loss: 0.2165 - val_accuracy: 0.9304\n",
            "Epoch 3/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1973 - accuracy: 0.9351 - val_loss: 0.1843 - val_accuracy: 0.9380\n",
            "Epoch 4/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1755 - accuracy: 0.9435 - val_loss: 0.1875 - val_accuracy: 0.9443\n",
            "Epoch 5/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1679 - accuracy: 0.9433 - val_loss: 0.1725 - val_accuracy: 0.9496\n",
            "Epoch 6/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1603 - accuracy: 0.9449 - val_loss: 0.1863 - val_accuracy: 0.9345\n",
            "Epoch 7/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1629 - accuracy: 0.9450 - val_loss: 0.1612 - val_accuracy: 0.9432\n",
            "Epoch 8/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1517 - accuracy: 0.9469 - val_loss: 0.1499 - val_accuracy: 0.9452\n",
            "Epoch 9/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1462 - accuracy: 0.9438 - val_loss: 0.1307 - val_accuracy: 0.9493\n",
            "Epoch 10/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1408 - accuracy: 0.9483 - val_loss: 0.1289 - val_accuracy: 0.9484\n",
            "Epoch 11/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1237 - accuracy: 0.9539 - val_loss: 0.1235 - val_accuracy: 0.9499\n",
            "Epoch 12/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1210 - accuracy: 0.9561 - val_loss: 0.1272 - val_accuracy: 0.9519\n",
            "Epoch 13/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1252 - accuracy: 0.9563 - val_loss: 0.1096 - val_accuracy: 0.9551\n",
            "Epoch 14/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1146 - accuracy: 0.9582 - val_loss: 0.1304 - val_accuracy: 0.9522\n",
            "Epoch 15/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1144 - accuracy: 0.9619 - val_loss: 0.1366 - val_accuracy: 0.9510\n",
            "Score for fold 2: loss of 0.1346045732498169; accuracy of 94.7826087474823%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/15\n",
            "403/403 [==============================] - 5s 10ms/step - loss: 0.5337 - accuracy: 0.7923 - val_loss: 0.4671 - val_accuracy: 0.8000\n",
            "Epoch 2/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.3204 - accuracy: 0.8845 - val_loss: 0.2055 - val_accuracy: 0.9357\n",
            "Epoch 3/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1963 - accuracy: 0.9386 - val_loss: 0.2210 - val_accuracy: 0.9446\n",
            "Epoch 4/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1703 - accuracy: 0.9406 - val_loss: 0.1658 - val_accuracy: 0.9464\n",
            "Epoch 5/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1724 - accuracy: 0.9388 - val_loss: 0.1671 - val_accuracy: 0.9504\n",
            "Epoch 6/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1567 - accuracy: 0.9471 - val_loss: 0.2135 - val_accuracy: 0.9429\n",
            "Epoch 7/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1442 - accuracy: 0.9504 - val_loss: 0.1405 - val_accuracy: 0.9513\n",
            "Epoch 8/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1377 - accuracy: 0.9514 - val_loss: 0.1226 - val_accuracy: 0.9588\n",
            "Epoch 9/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1286 - accuracy: 0.9533 - val_loss: 0.1185 - val_accuracy: 0.9565\n",
            "Epoch 10/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1131 - accuracy: 0.9612 - val_loss: 0.1306 - val_accuracy: 0.9539\n",
            "Epoch 11/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1178 - accuracy: 0.9574 - val_loss: 0.1562 - val_accuracy: 0.9496\n",
            "Epoch 12/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1091 - accuracy: 0.9588 - val_loss: 0.1373 - val_accuracy: 0.9528\n",
            "Epoch 13/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1190 - accuracy: 0.9620 - val_loss: 0.1165 - val_accuracy: 0.9574\n",
            "Epoch 14/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1057 - accuracy: 0.9598 - val_loss: 0.1398 - val_accuracy: 0.9533\n",
            "Epoch 15/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1013 - accuracy: 0.9616 - val_loss: 0.1336 - val_accuracy: 0.9557\n",
            "Score for fold 3: loss of 0.11113450676202774; accuracy of 95.652174949646%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/15\n",
            "403/403 [==============================] - 5s 10ms/step - loss: 0.5084 - accuracy: 0.8061 - val_loss: 0.3280 - val_accuracy: 0.8890\n",
            "Epoch 2/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.2271 - accuracy: 0.9268 - val_loss: 0.1777 - val_accuracy: 0.9458\n",
            "Epoch 3/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1901 - accuracy: 0.9392 - val_loss: 0.1514 - val_accuracy: 0.9470\n",
            "Epoch 4/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1595 - accuracy: 0.9475 - val_loss: 0.1651 - val_accuracy: 0.9484\n",
            "Epoch 5/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1490 - accuracy: 0.9522 - val_loss: 0.1546 - val_accuracy: 0.9452\n",
            "Epoch 6/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1434 - accuracy: 0.9514 - val_loss: 0.1110 - val_accuracy: 0.9600\n",
            "Epoch 7/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1291 - accuracy: 0.9581 - val_loss: 0.1202 - val_accuracy: 0.9557\n",
            "Epoch 8/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1209 - accuracy: 0.9595 - val_loss: 0.1223 - val_accuracy: 0.9554\n",
            "Epoch 9/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1127 - accuracy: 0.9611 - val_loss: 0.1387 - val_accuracy: 0.9499\n",
            "Epoch 10/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1107 - accuracy: 0.9634 - val_loss: 0.1254 - val_accuracy: 0.9562\n",
            "Epoch 11/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1166 - accuracy: 0.9570 - val_loss: 0.0999 - val_accuracy: 0.9632\n",
            "Epoch 12/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.0997 - accuracy: 0.9658 - val_loss: 0.1143 - val_accuracy: 0.9580\n",
            "Epoch 13/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1029 - accuracy: 0.9611 - val_loss: 0.1494 - val_accuracy: 0.9501\n",
            "Epoch 14/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.0947 - accuracy: 0.9637 - val_loss: 0.1428 - val_accuracy: 0.9536\n",
            "Epoch 15/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.0999 - accuracy: 0.9646 - val_loss: 0.1329 - val_accuracy: 0.9507\n",
            "Score for fold 4: loss of 0.1231784075498581; accuracy of 95.5279529094696%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/15\n",
            "403/403 [==============================] - 5s 10ms/step - loss: 0.5137 - accuracy: 0.8060 - val_loss: 0.2708 - val_accuracy: 0.9243\n",
            "Epoch 2/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.2175 - accuracy: 0.9322 - val_loss: 0.1887 - val_accuracy: 0.9458\n",
            "Epoch 3/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1889 - accuracy: 0.9378 - val_loss: 0.1979 - val_accuracy: 0.9362\n",
            "Epoch 4/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1704 - accuracy: 0.9420 - val_loss: 0.1731 - val_accuracy: 0.9383\n",
            "Epoch 5/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1507 - accuracy: 0.9497 - val_loss: 0.1223 - val_accuracy: 0.9603\n",
            "Epoch 6/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1412 - accuracy: 0.9536 - val_loss: 0.1331 - val_accuracy: 0.9519\n",
            "Epoch 7/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1199 - accuracy: 0.9570 - val_loss: 0.1251 - val_accuracy: 0.9638\n",
            "Epoch 8/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1108 - accuracy: 0.9583 - val_loss: 0.1015 - val_accuracy: 0.9652\n",
            "Epoch 9/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1041 - accuracy: 0.9623 - val_loss: 0.0907 - val_accuracy: 0.9699\n",
            "Epoch 10/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1062 - accuracy: 0.9666 - val_loss: 0.0962 - val_accuracy: 0.9670\n",
            "Epoch 11/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.0986 - accuracy: 0.9681 - val_loss: 0.1411 - val_accuracy: 0.9504\n",
            "Epoch 12/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1040 - accuracy: 0.9633 - val_loss: 0.0941 - val_accuracy: 0.9652\n",
            "Epoch 13/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1020 - accuracy: 0.9639 - val_loss: 0.1092 - val_accuracy: 0.9612\n",
            "Epoch 14/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.0934 - accuracy: 0.9638 - val_loss: 0.1525 - val_accuracy: 0.9513\n",
            "Epoch 15/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1010 - accuracy: 0.9649 - val_loss: 0.0919 - val_accuracy: 0.9658\n",
            "Score for fold 5: loss of 0.08625219762325287; accuracy of 96.89440727233887%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 6 ...\n",
            "Epoch 1/15\n",
            "403/403 [==============================] - 5s 10ms/step - loss: 0.5147 - accuracy: 0.8011 - val_loss: 0.3071 - val_accuracy: 0.9501\n",
            "Epoch 2/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.2109 - accuracy: 0.9342 - val_loss: 0.1901 - val_accuracy: 0.9441\n",
            "Epoch 3/15\n",
            "403/403 [==============================] - 5s 12ms/step - loss: 0.1717 - accuracy: 0.9447 - val_loss: 0.1595 - val_accuracy: 0.9420\n",
            "Epoch 4/15\n",
            "403/403 [==============================] - 5s 13ms/step - loss: 0.1488 - accuracy: 0.9516 - val_loss: 0.1237 - val_accuracy: 0.9603\n",
            "Epoch 5/15\n",
            "403/403 [==============================] - 4s 11ms/step - loss: 0.1313 - accuracy: 0.9548 - val_loss: 0.1155 - val_accuracy: 0.9574\n",
            "Epoch 6/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1181 - accuracy: 0.9608 - val_loss: 0.1167 - val_accuracy: 0.9571\n",
            "Epoch 7/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.1112 - accuracy: 0.9615 - val_loss: 0.0884 - val_accuracy: 0.9690\n",
            "Epoch 8/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1065 - accuracy: 0.9612 - val_loss: 0.1294 - val_accuracy: 0.9516\n",
            "Epoch 9/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1116 - accuracy: 0.9599 - val_loss: 0.0953 - val_accuracy: 0.9626\n",
            "Epoch 10/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.0995 - accuracy: 0.9647 - val_loss: 0.1173 - val_accuracy: 0.9600\n",
            "Epoch 11/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.0999 - accuracy: 0.9644 - val_loss: 0.0801 - val_accuracy: 0.9719\n",
            "Epoch 12/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1003 - accuracy: 0.9624 - val_loss: 0.1270 - val_accuracy: 0.9539\n",
            "Epoch 13/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.0921 - accuracy: 0.9657 - val_loss: 0.1561 - val_accuracy: 0.9525\n",
            "Epoch 14/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.0980 - accuracy: 0.9645 - val_loss: 0.0856 - val_accuracy: 0.9699\n",
            "Epoch 15/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.0854 - accuracy: 0.9681 - val_loss: 0.1401 - val_accuracy: 0.9559\n",
            "Score for fold 6: loss of 0.1276426762342453; accuracy of 95.652174949646%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 7 ...\n",
            "Epoch 1/15\n",
            "403/403 [==============================] - 5s 10ms/step - loss: 0.5128 - accuracy: 0.8041 - val_loss: 0.3339 - val_accuracy: 0.9446\n",
            "Epoch 2/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.2257 - accuracy: 0.9289 - val_loss: 0.1878 - val_accuracy: 0.9504\n",
            "Epoch 3/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1791 - accuracy: 0.9403 - val_loss: 0.1613 - val_accuracy: 0.9452\n",
            "Epoch 4/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1584 - accuracy: 0.9487 - val_loss: 0.1514 - val_accuracy: 0.9461\n",
            "Epoch 5/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1467 - accuracy: 0.9531 - val_loss: 0.1615 - val_accuracy: 0.9409\n",
            "Epoch 6/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1322 - accuracy: 0.9568 - val_loss: 0.1055 - val_accuracy: 0.9629\n",
            "Epoch 7/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1222 - accuracy: 0.9590 - val_loss: 0.1188 - val_accuracy: 0.9571\n",
            "Epoch 8/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1127 - accuracy: 0.9595 - val_loss: 0.1695 - val_accuracy: 0.9449\n",
            "Epoch 9/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1071 - accuracy: 0.9642 - val_loss: 0.1300 - val_accuracy: 0.9545\n",
            "Epoch 10/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1009 - accuracy: 0.9651 - val_loss: 0.1333 - val_accuracy: 0.9536\n",
            "Epoch 11/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1117 - accuracy: 0.9579 - val_loss: 0.1632 - val_accuracy: 0.9470\n",
            "Epoch 12/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.0979 - accuracy: 0.9629 - val_loss: 0.0857 - val_accuracy: 0.9693\n",
            "Epoch 13/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1031 - accuracy: 0.9645 - val_loss: 0.1182 - val_accuracy: 0.9554\n",
            "Epoch 14/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1007 - accuracy: 0.9650 - val_loss: 0.1137 - val_accuracy: 0.9600\n",
            "Epoch 15/15\n",
            "403/403 [==============================] - 4s 9ms/step - loss: 0.0897 - accuracy: 0.9678 - val_loss: 0.1030 - val_accuracy: 0.9641\n",
            "Score for fold 7: loss of 0.10841074585914612; accuracy of 96.02484703063965%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 8 ...\n",
            "Epoch 1/15\n",
            "403/403 [==============================] - 5s 10ms/step - loss: 0.5138 - accuracy: 0.7972 - val_loss: 0.2550 - val_accuracy: 0.9194\n",
            "Epoch 2/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.2159 - accuracy: 0.9325 - val_loss: 0.1920 - val_accuracy: 0.9322\n",
            "Epoch 3/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1807 - accuracy: 0.9392 - val_loss: 0.1975 - val_accuracy: 0.9319\n",
            "Epoch 4/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1527 - accuracy: 0.9494 - val_loss: 0.1366 - val_accuracy: 0.9583\n",
            "Epoch 5/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1427 - accuracy: 0.9520 - val_loss: 0.1634 - val_accuracy: 0.9403\n",
            "Epoch 6/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1268 - accuracy: 0.9566 - val_loss: 0.1130 - val_accuracy: 0.9638\n",
            "Epoch 7/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1280 - accuracy: 0.9543 - val_loss: 0.1122 - val_accuracy: 0.9583\n",
            "Epoch 8/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1097 - accuracy: 0.9621 - val_loss: 0.1269 - val_accuracy: 0.9551\n",
            "Epoch 9/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1028 - accuracy: 0.9628 - val_loss: 0.1236 - val_accuracy: 0.9571\n",
            "Epoch 10/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1138 - accuracy: 0.9572 - val_loss: 0.1163 - val_accuracy: 0.9603\n",
            "Epoch 11/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.0954 - accuracy: 0.9656 - val_loss: 0.1466 - val_accuracy: 0.9525\n",
            "Epoch 12/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1018 - accuracy: 0.9631 - val_loss: 0.1121 - val_accuracy: 0.9597\n",
            "Epoch 13/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.0982 - accuracy: 0.9637 - val_loss: 0.1010 - val_accuracy: 0.9632\n",
            "Epoch 14/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.0967 - accuracy: 0.9645 - val_loss: 0.0812 - val_accuracy: 0.9699\n",
            "Epoch 15/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.0963 - accuracy: 0.9660 - val_loss: 0.1060 - val_accuracy: 0.9612\n",
            "Score for fold 8: loss of 0.09285303950309753; accuracy of 96.27329111099243%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 9 ...\n",
            "Epoch 1/15\n",
            "403/403 [==============================] - 5s 10ms/step - loss: 0.5134 - accuracy: 0.7999 - val_loss: 0.3021 - val_accuracy: 0.9432\n",
            "Epoch 2/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.2238 - accuracy: 0.9291 - val_loss: 0.2395 - val_accuracy: 0.9478\n",
            "Epoch 3/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1908 - accuracy: 0.9386 - val_loss: 0.1671 - val_accuracy: 0.9403\n",
            "Epoch 4/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1569 - accuracy: 0.9472 - val_loss: 0.1485 - val_accuracy: 0.9530\n",
            "Epoch 5/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1570 - accuracy: 0.9478 - val_loss: 0.1735 - val_accuracy: 0.9484\n",
            "Epoch 6/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1604 - accuracy: 0.9481 - val_loss: 0.1319 - val_accuracy: 0.9620\n",
            "Epoch 7/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1389 - accuracy: 0.9566 - val_loss: 0.1228 - val_accuracy: 0.9551\n",
            "Epoch 8/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1167 - accuracy: 0.9602 - val_loss: 0.1335 - val_accuracy: 0.9554\n",
            "Epoch 9/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1158 - accuracy: 0.9577 - val_loss: 0.1313 - val_accuracy: 0.9522\n",
            "Epoch 10/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1244 - accuracy: 0.9586 - val_loss: 0.1504 - val_accuracy: 0.9501\n",
            "Epoch 11/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1075 - accuracy: 0.9621 - val_loss: 0.1025 - val_accuracy: 0.9614\n",
            "Epoch 12/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.0935 - accuracy: 0.9661 - val_loss: 0.1145 - val_accuracy: 0.9559\n",
            "Epoch 13/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1081 - accuracy: 0.9613 - val_loss: 0.1300 - val_accuracy: 0.9539\n",
            "Epoch 14/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.0976 - accuracy: 0.9664 - val_loss: 0.1046 - val_accuracy: 0.9606\n",
            "Epoch 15/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.0988 - accuracy: 0.9647 - val_loss: 0.0971 - val_accuracy: 0.9643\n",
            "Score for fold 9: loss of 0.1085113063454628; accuracy of 95.90061902999878%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 10 ...\n",
            "Epoch 1/15\n",
            "403/403 [==============================] - 5s 10ms/step - loss: 0.4818 - accuracy: 0.8195 - val_loss: 0.2472 - val_accuracy: 0.9214\n",
            "Epoch 2/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.2100 - accuracy: 0.9324 - val_loss: 0.2040 - val_accuracy: 0.9339\n",
            "Epoch 3/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1867 - accuracy: 0.9398 - val_loss: 0.2062 - val_accuracy: 0.9345\n",
            "Epoch 4/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1928 - accuracy: 0.9375 - val_loss: 0.1806 - val_accuracy: 0.9496\n",
            "Epoch 5/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1738 - accuracy: 0.9436 - val_loss: 0.1927 - val_accuracy: 0.9345\n",
            "Epoch 6/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1621 - accuracy: 0.9473 - val_loss: 0.2022 - val_accuracy: 0.9310\n",
            "Epoch 7/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1653 - accuracy: 0.9440 - val_loss: 0.1409 - val_accuracy: 0.9481\n",
            "Epoch 8/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1418 - accuracy: 0.9481 - val_loss: 0.1354 - val_accuracy: 0.9490\n",
            "Epoch 9/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1374 - accuracy: 0.9512 - val_loss: 0.1145 - val_accuracy: 0.9548\n",
            "Epoch 10/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1240 - accuracy: 0.9550 - val_loss: 0.1189 - val_accuracy: 0.9536\n",
            "Epoch 11/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1327 - accuracy: 0.9561 - val_loss: 0.1611 - val_accuracy: 0.9461\n",
            "Epoch 12/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1150 - accuracy: 0.9596 - val_loss: 0.1032 - val_accuracy: 0.9623\n",
            "Epoch 13/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1089 - accuracy: 0.9625 - val_loss: 0.1196 - val_accuracy: 0.9565\n",
            "Epoch 14/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1111 - accuracy: 0.9600 - val_loss: 0.0896 - val_accuracy: 0.9687\n",
            "Epoch 15/15\n",
            "403/403 [==============================] - 4s 10ms/step - loss: 0.1053 - accuracy: 0.9631 - val_loss: 0.1061 - val_accuracy: 0.9609\n",
            "Score for fold 10: loss of 0.0855531319975853; accuracy of 96.64596319198608%\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.1071377769112587 - Accuracy: 95.5279529094696%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.1346045732498169 - Accuracy: 94.7826087474823%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.11113450676202774 - Accuracy: 95.652174949646%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.1231784075498581 - Accuracy: 95.5279529094696%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.08625219762325287 - Accuracy: 96.89440727233887%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.1276426762342453 - Accuracy: 95.652174949646%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.10841074585914612 - Accuracy: 96.02484703063965%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.09285303950309753 - Accuracy: 96.27329111099243%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.1085113063454628 - Accuracy: 95.90061902999878%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.0855531319975853 - Accuracy: 96.64596319198608%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 95.88819921016693 (+- 0.5785398461634739)\n",
            "> Loss: 0.10852783620357513\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5I_bZndY1DG",
        "outputId": "dd9b505c-72ac-43d6-b4bf-96b1fbfcb0a1"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_51\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_153 (Conv1D)          (None, 89, 64)            704       \n",
            "_________________________________________________________________\n",
            "dropout_204 (Dropout)        (None, 89, 64)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_153 (MaxPoolin (None, 29, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_154 (Conv1D)          (None, 15, 40)            12840     \n",
            "_________________________________________________________________\n",
            "dropout_205 (Dropout)        (None, 15, 40)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_154 (MaxPoolin (None, 5, 40)             0         \n",
            "_________________________________________________________________\n",
            "conv1d_155 (Conv1D)          (None, 5, 32)             5152      \n",
            "_________________________________________________________________\n",
            "dropout_206 (Dropout)        (None, 5, 32)             0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_155 (MaxPoolin (None, 1, 32)             0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_51  (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_102 (Dense)            (None, 50)                1650      \n",
            "_________________________________________________________________\n",
            "dropout_207 (Dropout)        (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_103 (Dense)            (None, 2)                 102       \n",
            "=================================================================\n",
            "Total params: 20,448\n",
            "Trainable params: 20,448\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "id": "hUc-6lj488xD",
        "outputId": "79f5418b-ca3e-417f-b3a5-d1a51ccfaad0"
      },
      "source": [
        "# The history.history[\"loss\"] entry is a dictionary with as many values as epochs that the\n",
        "# model was trained on. \n",
        "df_loss_acc = pd.DataFrame(data_train.history)\n",
        "df_loss= df_loss_acc[['loss','val_loss']]\n",
        "df_loss.rename(columns={'loss':'train','val_loss':'validation'},inplace=True)\n",
        "df_acc= df_loss_acc[['accuracy','val_accuracy']]\n",
        "df_acc.rename(columns={'accuracy':'train','val_accuracy':'validation'},inplace=True)\n",
        "df_loss.plot(title='Model loss',figsize=(7,7)).set(xlabel='Epoch',ylabel='Loss')\n",
        "df_acc.plot(title='Model Accuracy',figsize=(7,7)).set(xlabel='Epoch',ylabel='Accuracy')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0, 0.5, 'Accuracy'), Text(0.5, 0, 'Epoch')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m15sZ6T-Pdfx"
      },
      "source": [
        "###### Bonn dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "cQGbytAxPhtt",
        "outputId": "59b5c33c-350e-49e9-943a-7b8d27c05bfd"
      },
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"boston.csv\")\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.887621</td>\n",
              "      <td>0.185875</td>\n",
              "      <td>2.299474</td>\n",
              "      <td>2.299474</td>\n",
              "      <td>0.581396</td>\n",
              "      <td>healthy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.783744</td>\n",
              "      <td>0.163794</td>\n",
              "      <td>2.299480</td>\n",
              "      <td>2.299480</td>\n",
              "      <td>0.588340</td>\n",
              "      <td>healthy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.875873</td>\n",
              "      <td>0.154714</td>\n",
              "      <td>2.299477</td>\n",
              "      <td>2.299477</td>\n",
              "      <td>0.589802</td>\n",
              "      <td>healthy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.881293</td>\n",
              "      <td>0.138396</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>0.592911</td>\n",
              "      <td>healthy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.798116</td>\n",
              "      <td>0.125291</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>0.595338</td>\n",
              "      <td>healthy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0        f1        f2        f3        f4        f5    class\n",
              "0           0  0.887621  0.185875  2.299474  2.299474  0.581396  healthy\n",
              "1           1  0.783744  0.163794  2.299480  2.299480  0.588340  healthy\n",
              "2           2  0.875873  0.154714  2.299477  2.299477  0.589802  healthy\n",
              "3           3  0.881293  0.138396  2.299481  2.299481  0.592911  healthy\n",
              "4           4  0.798116  0.125291  2.299481  2.299481  0.595338  healthy"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmIoqEdGPhw2"
      },
      "source": [
        "df=df.iloc[:,1:]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM1welfgPhzU"
      },
      "source": [
        "df[\"Target\"] = df[\"class\"]\n",
        "\n",
        "df['Target'] = df.Target.map({\"seizure\": 1, \"healthy\": 0, \"transation\": 0}) # Checking for presence of CLass 1\n",
        "df.head(5)\n",
        "df.drop(df.columns[5], axis=1, inplace=True)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haEUS859mbkg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "de8df5e3-794b-49e6-d035-008a0ab360ea"
      },
      "source": [
        "feature_data = df.iloc[:,0:5]\n",
        "feature_data.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.887621</td>\n",
              "      <td>0.185875</td>\n",
              "      <td>2.299474</td>\n",
              "      <td>2.299474</td>\n",
              "      <td>0.581396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.783744</td>\n",
              "      <td>0.163794</td>\n",
              "      <td>2.299480</td>\n",
              "      <td>2.299480</td>\n",
              "      <td>0.588340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.875873</td>\n",
              "      <td>0.154714</td>\n",
              "      <td>2.299477</td>\n",
              "      <td>2.299477</td>\n",
              "      <td>0.589802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.881293</td>\n",
              "      <td>0.138396</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>0.592911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.798116</td>\n",
              "      <td>0.125291</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>0.595338</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         f1        f2        f3        f4        f5\n",
              "0  0.887621  0.185875  2.299474  2.299474  0.581396\n",
              "1  0.783744  0.163794  2.299480  2.299480  0.588340\n",
              "2  0.875873  0.154714  2.299477  2.299477  0.589802\n",
              "3  0.881293  0.138396  2.299481  2.299481  0.592911\n",
              "4  0.798116  0.125291  2.299481  2.299481  0.595338"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_h48PAUmB7va",
        "outputId": "4eafef1d-2cc7-4c51-87cd-c0f369a6f443"
      },
      "source": [
        "feature_data.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UHao7H6QTHt"
      },
      "source": [
        "#dataprocessing\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "X_train_minmax = min_max_scaler.fit_transform(feature_data)\n",
        "#X_train_minmax"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTiMotxBQ5Zc"
      },
      "source": [
        "y = df[\"Target\"]\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_train_minmax,y,test_size = 0.30 ,stratify= y, random_state = 0)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9IZCnr3Q1IG"
      },
      "source": [
        "num_folds=10\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRFVTYdPQ1IG"
      },
      "source": [
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "X_train_tf = np.expand_dims(x_train, axis=2)\n",
        "X_valid_tf = np.expand_dims(x_test, axis=2)\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train , num_classes=2)\n",
        "y_valid = keras.utils.to_categorical(y_test , num_classes=2)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8GRnO3HQ1IH",
        "outputId": "7e9bc192-335a-430d-e667-506a5e9c9858"
      },
      "source": [
        "print(x_test.shape)\n",
        "print(y_valid.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(90, 5)\n",
            "(90, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lODUlSc_Q1IH",
        "outputId": "3c6f025e-232d-463d-a490-ac3a71ec7cbe"
      },
      "source": [
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(X_train_tf, y_train):\n",
        "\n",
        "  # Define the model architecture\n",
        "  model = Sequential()\n",
        "  model.add(Convolution1D(64, 10, strides=2, padding='same', activation='relu',  input_shape=(5, 1)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(MaxPooling1D(3))\n",
        "  model.add(Convolution1D(40, 5, strides=2, padding='same', activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Convolution1D(32, 4, strides=1, padding='same', activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(GlobalAveragePooling1D())\n",
        "  model.add(Dense(50, activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "  \n",
        "  # Compile the model\n",
        "  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "  data_train=model.fit(X_train_tf, y_train, epochs= 15, validation_data=(X_valid_tf, y_valid), batch_size=20)\n",
        "\n",
        "  # Generate generalization metrics\n",
        "  scores = model.evaluate(X_train_tf[test], y_train[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "# == Provide average scores ==\n",
        "\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/15\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_46_input'), name='conv1d_46_input', description=\"created by layer 'conv1d_46_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_46_input'), name='conv1d_46_input', description=\"created by layer 'conv1d_46_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "18/35 [==============>...............] - ETA: 0s - loss: 0.6851 - accuracy: 0.7472 WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_46_input'), name='conv1d_46_input', description=\"created by layer 'conv1d_46_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "35/35 [==============================] - 1s 11ms/step - loss: 0.6649 - accuracy: 0.8391 - val_loss: 0.4214 - val_accuracy: 0.9800\n",
            "Epoch 2/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.2635 - accuracy: 0.9774 - val_loss: 0.1024 - val_accuracy: 0.9800\n",
            "Epoch 3/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0976 - accuracy: 0.9851 - val_loss: 0.1003 - val_accuracy: 0.9800\n",
            "Epoch 4/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0904 - accuracy: 0.9878 - val_loss: 0.1012 - val_accuracy: 0.9800\n",
            "Epoch 5/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0963 - accuracy: 0.9867 - val_loss: 0.1054 - val_accuracy: 0.9800\n",
            "Epoch 6/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1057 - accuracy: 0.9832 - val_loss: 0.0987 - val_accuracy: 0.9800\n",
            "Epoch 7/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1035 - accuracy: 0.9825 - val_loss: 0.1027 - val_accuracy: 0.9800\n",
            "Epoch 8/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0904 - accuracy: 0.9847 - val_loss: 0.0998 - val_accuracy: 0.9800\n",
            "Epoch 9/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0826 - accuracy: 0.9857 - val_loss: 0.1002 - val_accuracy: 0.9800\n",
            "Epoch 10/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1657 - accuracy: 0.9662 - val_loss: 0.1090 - val_accuracy: 0.9800\n",
            "Epoch 11/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0678 - accuracy: 0.9878 - val_loss: 0.1065 - val_accuracy: 0.9800\n",
            "Epoch 12/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0805 - accuracy: 0.9886 - val_loss: 0.1042 - val_accuracy: 0.9800\n",
            "Epoch 13/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0870 - accuracy: 0.9839 - val_loss: 0.0971 - val_accuracy: 0.9800\n",
            "Epoch 14/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0566 - accuracy: 0.9909 - val_loss: 0.0989 - val_accuracy: 0.9800\n",
            "Epoch 15/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0775 - accuracy: 0.9892 - val_loss: 0.1023 - val_accuracy: 0.9800\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_46_input'), name='conv1d_46_input', description=\"created by layer 'conv1d_46_input'\"), but it was called on an input with incompatible shape (None, 16, 1).\n",
            "Score for fold 1: loss of 0.08265713602304459; accuracy of 98.57142567634583%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/15\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_49_input'), name='conv1d_49_input', description=\"created by layer 'conv1d_49_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_49_input'), name='conv1d_49_input', description=\"created by layer 'conv1d_49_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "25/35 [====================>.........] - ETA: 0s - loss: 0.6427 - accuracy: 0.8659 WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_49_input'), name='conv1d_49_input', description=\"created by layer 'conv1d_49_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "35/35 [==============================] - 1s 9ms/step - loss: 0.5983 - accuracy: 0.8938 - val_loss: 0.1239 - val_accuracy: 0.9800\n",
            "Epoch 2/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1254 - accuracy: 0.9760 - val_loss: 0.0986 - val_accuracy: 0.9800\n",
            "Epoch 3/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0940 - accuracy: 0.9836 - val_loss: 0.0979 - val_accuracy: 0.9800\n",
            "Epoch 4/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1259 - accuracy: 0.9747 - val_loss: 0.0975 - val_accuracy: 0.9800\n",
            "Epoch 5/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1041 - accuracy: 0.9807 - val_loss: 0.0980 - val_accuracy: 0.9800\n",
            "Epoch 6/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1126 - accuracy: 0.9824 - val_loss: 0.0993 - val_accuracy: 0.9800\n",
            "Epoch 7/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0951 - accuracy: 0.9849 - val_loss: 0.1035 - val_accuracy: 0.9800\n",
            "Epoch 8/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1476 - accuracy: 0.9665 - val_loss: 0.0973 - val_accuracy: 0.9800\n",
            "Epoch 9/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0962 - accuracy: 0.9847 - val_loss: 0.0992 - val_accuracy: 0.9800\n",
            "Epoch 10/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1010 - accuracy: 0.9829 - val_loss: 0.0995 - val_accuracy: 0.9800\n",
            "Epoch 11/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0790 - accuracy: 0.9874 - val_loss: 0.0977 - val_accuracy: 0.9800\n",
            "Epoch 12/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0857 - accuracy: 0.9857 - val_loss: 0.0989 - val_accuracy: 0.9800\n",
            "Epoch 13/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0671 - accuracy: 0.9892 - val_loss: 0.1004 - val_accuracy: 0.9800\n",
            "Epoch 14/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0755 - accuracy: 0.9885 - val_loss: 0.0977 - val_accuracy: 0.9800\n",
            "Epoch 15/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1523 - accuracy: 0.9718 - val_loss: 0.0962 - val_accuracy: 0.9800\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_49_input'), name='conv1d_49_input', description=\"created by layer 'conv1d_49_input'\"), but it was called on an input with incompatible shape (None, 16, 1).\n",
            "Score for fold 2: loss of 0.02019042707979679; accuracy of 100.0%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/15\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_52_input'), name='conv1d_52_input', description=\"created by layer 'conv1d_52_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_52_input'), name='conv1d_52_input', description=\"created by layer 'conv1d_52_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "18/35 [==============>...............] - ETA: 0s - loss: 0.6882 - accuracy: 0.6099     WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_52_input'), name='conv1d_52_input', description=\"created by layer 'conv1d_52_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "35/35 [==============================] - 1s 10ms/step - loss: 0.6662 - accuracy: 0.7510 - val_loss: 0.3908 - val_accuracy: 0.9800\n",
            "Epoch 2/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.2266 - accuracy: 0.9804 - val_loss: 0.0981 - val_accuracy: 0.9800\n",
            "Epoch 3/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1123 - accuracy: 0.9819 - val_loss: 0.1017 - val_accuracy: 0.9800\n",
            "Epoch 4/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1576 - accuracy: 0.9711 - val_loss: 0.0973 - val_accuracy: 0.9800\n",
            "Epoch 5/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0696 - accuracy: 0.9905 - val_loss: 0.0997 - val_accuracy: 0.9800\n",
            "Epoch 6/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1210 - accuracy: 0.9791 - val_loss: 0.0971 - val_accuracy: 0.9800\n",
            "Epoch 7/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1290 - accuracy: 0.9776 - val_loss: 0.0975 - val_accuracy: 0.9800\n",
            "Epoch 8/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1268 - accuracy: 0.9713 - val_loss: 0.0973 - val_accuracy: 0.9800\n",
            "Epoch 9/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1168 - accuracy: 0.9772 - val_loss: 0.0974 - val_accuracy: 0.9800\n",
            "Epoch 10/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1215 - accuracy: 0.9774 - val_loss: 0.0965 - val_accuracy: 0.9800\n",
            "Epoch 11/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0924 - accuracy: 0.9847 - val_loss: 0.1018 - val_accuracy: 0.9800\n",
            "Epoch 12/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0942 - accuracy: 0.9829 - val_loss: 0.0965 - val_accuracy: 0.9800\n",
            "Epoch 13/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1044 - accuracy: 0.9796 - val_loss: 0.0966 - val_accuracy: 0.9800\n",
            "Epoch 14/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1274 - accuracy: 0.9732 - val_loss: 0.0992 - val_accuracy: 0.9800\n",
            "Epoch 15/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0918 - accuracy: 0.9843 - val_loss: 0.0996 - val_accuracy: 0.9800\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_52_input'), name='conv1d_52_input', description=\"created by layer 'conv1d_52_input'\"), but it was called on an input with incompatible shape (None, 16, 1).\n",
            "Score for fold 3: loss of 0.13092733919620514; accuracy of 97.14285731315613%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/15\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_55_input'), name='conv1d_55_input', description=\"created by layer 'conv1d_55_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_55_input'), name='conv1d_55_input', description=\"created by layer 'conv1d_55_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "21/35 [=================>............] - ETA: 0s - loss: 0.6407 - accuracy: 0.9010 WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_55_input'), name='conv1d_55_input', description=\"created by layer 'conv1d_55_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "35/35 [==============================] - 1s 10ms/step - loss: 0.5723 - accuracy: 0.9280 - val_loss: 0.0991 - val_accuracy: 0.9800\n",
            "Epoch 2/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1234 - accuracy: 0.9814 - val_loss: 0.0979 - val_accuracy: 0.9800\n",
            "Epoch 3/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0997 - accuracy: 0.9818 - val_loss: 0.1038 - val_accuracy: 0.9800\n",
            "Epoch 4/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0991 - accuracy: 0.9841 - val_loss: 0.0976 - val_accuracy: 0.9800\n",
            "Epoch 5/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1045 - accuracy: 0.9797 - val_loss: 0.0986 - val_accuracy: 0.9800\n",
            "Epoch 6/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1086 - accuracy: 0.9802 - val_loss: 0.0973 - val_accuracy: 0.9800\n",
            "Epoch 7/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0768 - accuracy: 0.9867 - val_loss: 0.1015 - val_accuracy: 0.9800\n",
            "Epoch 8/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1113 - accuracy: 0.9807 - val_loss: 0.0987 - val_accuracy: 0.9800\n",
            "Epoch 9/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1194 - accuracy: 0.9756 - val_loss: 0.0976 - val_accuracy: 0.9800\n",
            "Epoch 10/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1081 - accuracy: 0.9792 - val_loss: 0.0968 - val_accuracy: 0.9800\n",
            "Epoch 11/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1198 - accuracy: 0.9788 - val_loss: 0.0975 - val_accuracy: 0.9800\n",
            "Epoch 12/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0734 - accuracy: 0.9881 - val_loss: 0.1096 - val_accuracy: 0.9800\n",
            "Epoch 13/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1223 - accuracy: 0.9765 - val_loss: 0.0983 - val_accuracy: 0.9800\n",
            "Epoch 14/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0936 - accuracy: 0.9821 - val_loss: 0.1073 - val_accuracy: 0.9800\n",
            "Epoch 15/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0979 - accuracy: 0.9819 - val_loss: 0.1019 - val_accuracy: 0.9800\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_55_input'), name='conv1d_55_input', description=\"created by layer 'conv1d_55_input'\"), but it was called on an input with incompatible shape (None, 16, 1).\n",
            "Score for fold 4: loss of 0.17258477210998535; accuracy of 95.71428298950195%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/15\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_58_input'), name='conv1d_58_input', description=\"created by layer 'conv1d_58_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_58_input'), name='conv1d_58_input', description=\"created by layer 'conv1d_58_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "17/35 [=============>................] - ETA: 0s - loss: 0.6635 - accuracy: 0.8980 WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_58_input'), name='conv1d_58_input', description=\"created by layer 'conv1d_58_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "35/35 [==============================] - 1s 10ms/step - loss: 0.5934 - accuracy: 0.9325 - val_loss: 0.1160 - val_accuracy: 0.9800\n",
            "Epoch 2/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1276 - accuracy: 0.9783 - val_loss: 0.0975 - val_accuracy: 0.9800\n",
            "Epoch 3/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0732 - accuracy: 0.9878 - val_loss: 0.1009 - val_accuracy: 0.9800\n",
            "Epoch 4/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1138 - accuracy: 0.9803 - val_loss: 0.0978 - val_accuracy: 0.9800\n",
            "Epoch 5/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0991 - accuracy: 0.9806 - val_loss: 0.0993 - val_accuracy: 0.9800\n",
            "Epoch 6/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0806 - accuracy: 0.9861 - val_loss: 0.1003 - val_accuracy: 0.9800\n",
            "Epoch 7/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1527 - accuracy: 0.9700 - val_loss: 0.0976 - val_accuracy: 0.9800\n",
            "Epoch 8/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0810 - accuracy: 0.9876 - val_loss: 0.1140 - val_accuracy: 0.9800\n",
            "Epoch 9/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0916 - accuracy: 0.9859 - val_loss: 0.1005 - val_accuracy: 0.9800\n",
            "Epoch 10/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0605 - accuracy: 0.9920 - val_loss: 0.1056 - val_accuracy: 0.9800\n",
            "Epoch 11/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1046 - accuracy: 0.9802 - val_loss: 0.0986 - val_accuracy: 0.9800\n",
            "Epoch 12/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0782 - accuracy: 0.9885 - val_loss: 0.0995 - val_accuracy: 0.9800\n",
            "Epoch 13/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0899 - accuracy: 0.9849 - val_loss: 0.0978 - val_accuracy: 0.9800\n",
            "Epoch 14/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0919 - accuracy: 0.9855 - val_loss: 0.0978 - val_accuracy: 0.9800\n",
            "Epoch 15/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1180 - accuracy: 0.9795 - val_loss: 0.0983 - val_accuracy: 0.9800\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_58_input'), name='conv1d_58_input', description=\"created by layer 'conv1d_58_input'\"), but it was called on an input with incompatible shape (None, 16, 1).\n",
            "Score for fold 5: loss of 0.03220674395561218; accuracy of 100.0%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 6 ...\n",
            "Epoch 1/15\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_61_input'), name='conv1d_61_input', description=\"created by layer 'conv1d_61_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_61_input'), name='conv1d_61_input', description=\"created by layer 'conv1d_61_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "21/35 [=================>............] - ETA: 0s - loss: 0.6534 - accuracy: 0.9037 WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_61_input'), name='conv1d_61_input', description=\"created by layer 'conv1d_61_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "35/35 [==============================] - 1s 10ms/step - loss: 0.5956 - accuracy: 0.9300 - val_loss: 0.1107 - val_accuracy: 0.9800\n",
            "Epoch 2/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0696 - accuracy: 0.9917 - val_loss: 0.0981 - val_accuracy: 0.9800\n",
            "Epoch 3/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0829 - accuracy: 0.9873 - val_loss: 0.1018 - val_accuracy: 0.9800\n",
            "Epoch 4/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0788 - accuracy: 0.9880 - val_loss: 0.1005 - val_accuracy: 0.9800\n",
            "Epoch 5/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0970 - accuracy: 0.9832 - val_loss: 0.0992 - val_accuracy: 0.9800\n",
            "Epoch 6/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1337 - accuracy: 0.9758 - val_loss: 0.0969 - val_accuracy: 0.9800\n",
            "Epoch 7/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0885 - accuracy: 0.9848 - val_loss: 0.0980 - val_accuracy: 0.9800\n",
            "Epoch 8/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1588 - accuracy: 0.9711 - val_loss: 0.0979 - val_accuracy: 0.9800\n",
            "Epoch 9/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1211 - accuracy: 0.9784 - val_loss: 0.0973 - val_accuracy: 0.9800\n",
            "Epoch 10/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1560 - accuracy: 0.9719 - val_loss: 0.0971 - val_accuracy: 0.9800\n",
            "Epoch 11/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0593 - accuracy: 0.9895 - val_loss: 0.1095 - val_accuracy: 0.9800\n",
            "Epoch 12/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0711 - accuracy: 0.9897 - val_loss: 0.0998 - val_accuracy: 0.9800\n",
            "Epoch 13/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0852 - accuracy: 0.9846 - val_loss: 0.0964 - val_accuracy: 0.9800\n",
            "Epoch 14/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0902 - accuracy: 0.9842 - val_loss: 0.1065 - val_accuracy: 0.9800\n",
            "Epoch 15/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1240 - accuracy: 0.9781 - val_loss: 0.0958 - val_accuracy: 0.9800\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_61_input'), name='conv1d_61_input', description=\"created by layer 'conv1d_61_input'\"), but it was called on an input with incompatible shape (None, 16, 1).\n",
            "Score for fold 6: loss of 0.1274092197418213; accuracy of 97.14285731315613%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 7 ...\n",
            "Epoch 1/15\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_64_input'), name='conv1d_64_input', description=\"created by layer 'conv1d_64_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_64_input'), name='conv1d_64_input', description=\"created by layer 'conv1d_64_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "24/35 [===================>..........] - ETA: 0s - loss: 0.6506 - accuracy: 0.8020 WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_64_input'), name='conv1d_64_input', description=\"created by layer 'conv1d_64_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "35/35 [==============================] - 1s 10ms/step - loss: 0.6038 - accuracy: 0.8485 - val_loss: 0.1150 - val_accuracy: 0.9800\n",
            "Epoch 2/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1029 - accuracy: 0.9856 - val_loss: 0.0992 - val_accuracy: 0.9800\n",
            "Epoch 3/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1059 - accuracy: 0.9823 - val_loss: 0.0988 - val_accuracy: 0.9800\n",
            "Epoch 4/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0847 - accuracy: 0.9847 - val_loss: 0.1037 - val_accuracy: 0.9800\n",
            "Epoch 5/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0907 - accuracy: 0.9858 - val_loss: 0.1011 - val_accuracy: 0.9800\n",
            "Epoch 6/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1265 - accuracy: 0.9734 - val_loss: 0.1010 - val_accuracy: 0.9800\n",
            "Epoch 7/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1463 - accuracy: 0.9773 - val_loss: 0.0977 - val_accuracy: 0.9800\n",
            "Epoch 8/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0960 - accuracy: 0.9831 - val_loss: 0.0982 - val_accuracy: 0.9800\n",
            "Epoch 9/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1110 - accuracy: 0.9815 - val_loss: 0.0989 - val_accuracy: 0.9800\n",
            "Epoch 10/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1153 - accuracy: 0.9792 - val_loss: 0.0989 - val_accuracy: 0.9800\n",
            "Epoch 11/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0957 - accuracy: 0.9825 - val_loss: 0.0971 - val_accuracy: 0.9800\n",
            "Epoch 12/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1283 - accuracy: 0.9757 - val_loss: 0.0995 - val_accuracy: 0.9800\n",
            "Epoch 13/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0880 - accuracy: 0.9850 - val_loss: 0.1015 - val_accuracy: 0.9800\n",
            "Epoch 14/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1003 - accuracy: 0.9818 - val_loss: 0.0961 - val_accuracy: 0.9800\n",
            "Epoch 15/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1374 - accuracy: 0.9716 - val_loss: 0.0974 - val_accuracy: 0.9800\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_64_input'), name='conv1d_64_input', description=\"created by layer 'conv1d_64_input'\"), but it was called on an input with incompatible shape (None, 16, 1).\n",
            "Score for fold 7: loss of 0.0730312168598175; accuracy of 98.57142567634583%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 8 ...\n",
            "Epoch 1/15\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_67_input'), name='conv1d_67_input', description=\"created by layer 'conv1d_67_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_67_input'), name='conv1d_67_input', description=\"created by layer 'conv1d_67_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "21/35 [=================>............] - ETA: 0s - loss: 0.6521 - accuracy: 0.8938 WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_67_input'), name='conv1d_67_input', description=\"created by layer 'conv1d_67_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "35/35 [==============================] - 1s 10ms/step - loss: 0.5950 - accuracy: 0.9227 - val_loss: 0.1110 - val_accuracy: 0.9800\n",
            "Epoch 2/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1231 - accuracy: 0.9816 - val_loss: 0.0973 - val_accuracy: 0.9800\n",
            "Epoch 3/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1476 - accuracy: 0.9714 - val_loss: 0.0979 - val_accuracy: 0.9800\n",
            "Epoch 4/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0805 - accuracy: 0.9847 - val_loss: 0.0980 - val_accuracy: 0.9800\n",
            "Epoch 5/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1069 - accuracy: 0.9806 - val_loss: 0.0971 - val_accuracy: 0.9800\n",
            "Epoch 6/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1088 - accuracy: 0.9821 - val_loss: 0.1040 - val_accuracy: 0.9800\n",
            "Epoch 7/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0921 - accuracy: 0.9842 - val_loss: 0.0998 - val_accuracy: 0.9800\n",
            "Epoch 8/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1071 - accuracy: 0.9803 - val_loss: 0.0977 - val_accuracy: 0.9800\n",
            "Epoch 9/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0965 - accuracy: 0.9816 - val_loss: 0.0982 - val_accuracy: 0.9800\n",
            "Epoch 10/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1594 - accuracy: 0.9706 - val_loss: 0.0968 - val_accuracy: 0.9800\n",
            "Epoch 11/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1135 - accuracy: 0.9802 - val_loss: 0.0965 - val_accuracy: 0.9800\n",
            "Epoch 12/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1082 - accuracy: 0.9807 - val_loss: 0.0959 - val_accuracy: 0.9800\n",
            "Epoch 13/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1110 - accuracy: 0.9792 - val_loss: 0.0985 - val_accuracy: 0.9800\n",
            "Epoch 14/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1259 - accuracy: 0.9773 - val_loss: 0.0966 - val_accuracy: 0.9800\n",
            "Epoch 15/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1338 - accuracy: 0.9756 - val_loss: 0.0972 - val_accuracy: 0.9800\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_67_input'), name='conv1d_67_input', description=\"created by layer 'conv1d_67_input'\"), but it was called on an input with incompatible shape (None, 16, 1).\n",
            "Score for fold 8: loss of 0.13006731867790222; accuracy of 97.14285731315613%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 9 ...\n",
            "Epoch 1/15\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_70_input'), name='conv1d_70_input', description=\"created by layer 'conv1d_70_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_70_input'), name='conv1d_70_input', description=\"created by layer 'conv1d_70_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "24/35 [===================>..........] - ETA: 0s - loss: 0.6389 - accuracy: 0.8868 WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_70_input'), name='conv1d_70_input', description=\"created by layer 'conv1d_70_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "35/35 [==============================] - 1s 9ms/step - loss: 0.5888 - accuracy: 0.9106 - val_loss: 0.1070 - val_accuracy: 0.9800\n",
            "Epoch 2/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1204 - accuracy: 0.9800 - val_loss: 0.0982 - val_accuracy: 0.9800\n",
            "Epoch 3/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1327 - accuracy: 0.9708 - val_loss: 0.0973 - val_accuracy: 0.9800\n",
            "Epoch 4/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1460 - accuracy: 0.9746 - val_loss: 0.0971 - val_accuracy: 0.9800\n",
            "Epoch 5/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0729 - accuracy: 0.9881 - val_loss: 0.1018 - val_accuracy: 0.9800\n",
            "Epoch 6/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0931 - accuracy: 0.9837 - val_loss: 0.0983 - val_accuracy: 0.9800\n",
            "Epoch 7/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1181 - accuracy: 0.9764 - val_loss: 0.0991 - val_accuracy: 0.9800\n",
            "Epoch 8/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0934 - accuracy: 0.9852 - val_loss: 0.1045 - val_accuracy: 0.9800\n",
            "Epoch 9/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1034 - accuracy: 0.9788 - val_loss: 0.0971 - val_accuracy: 0.9800\n",
            "Epoch 10/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0981 - accuracy: 0.9807 - val_loss: 0.0966 - val_accuracy: 0.9800\n",
            "Epoch 11/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0737 - accuracy: 0.9853 - val_loss: 0.1071 - val_accuracy: 0.9800\n",
            "Epoch 12/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1017 - accuracy: 0.9828 - val_loss: 0.0967 - val_accuracy: 0.9800\n",
            "Epoch 13/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1183 - accuracy: 0.9780 - val_loss: 0.0983 - val_accuracy: 0.9800\n",
            "Epoch 14/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0962 - accuracy: 0.9832 - val_loss: 0.0959 - val_accuracy: 0.9800\n",
            "Epoch 15/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1010 - accuracy: 0.9800 - val_loss: 0.0956 - val_accuracy: 0.9800\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_70_input'), name='conv1d_70_input', description=\"created by layer 'conv1d_70_input'\"), but it was called on an input with incompatible shape (None, 16, 1).\n",
            "Score for fold 9: loss of 0.021197576075792313; accuracy of 100.0%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 10 ...\n",
            "Epoch 1/15\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_73_input'), name='conv1d_73_input', description=\"created by layer 'conv1d_73_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_73_input'), name='conv1d_73_input', description=\"created by layer 'conv1d_73_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "23/35 [==================>...........] - ETA: 0s - loss: 0.6511 - accuracy: 0.9519 WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_73_input'), name='conv1d_73_input', description=\"created by layer 'conv1d_73_input'\"), but it was called on an input with incompatible shape (20, 16, 1).\n",
            "35/35 [==============================] - 1s 10ms/step - loss: 0.6094 - accuracy: 0.9616 - val_loss: 0.1666 - val_accuracy: 0.9800\n",
            "Epoch 2/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1295 - accuracy: 0.9786 - val_loss: 0.0991 - val_accuracy: 0.9800\n",
            "Epoch 3/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1511 - accuracy: 0.9742 - val_loss: 0.0976 - val_accuracy: 0.9800\n",
            "Epoch 4/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1129 - accuracy: 0.9813 - val_loss: 0.0999 - val_accuracy: 0.9800\n",
            "Epoch 5/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1448 - accuracy: 0.9714 - val_loss: 0.0973 - val_accuracy: 0.9800\n",
            "Epoch 6/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1040 - accuracy: 0.9813 - val_loss: 0.0983 - val_accuracy: 0.9800\n",
            "Epoch 7/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1204 - accuracy: 0.9775 - val_loss: 0.0975 - val_accuracy: 0.9800\n",
            "Epoch 8/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0919 - accuracy: 0.9823 - val_loss: 0.0988 - val_accuracy: 0.9800\n",
            "Epoch 9/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0972 - accuracy: 0.9828 - val_loss: 0.0996 - val_accuracy: 0.9800\n",
            "Epoch 10/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1138 - accuracy: 0.9782 - val_loss: 0.1007 - val_accuracy: 0.9800\n",
            "Epoch 11/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1089 - accuracy: 0.9792 - val_loss: 0.1004 - val_accuracy: 0.9800\n",
            "Epoch 12/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1014 - accuracy: 0.9810 - val_loss: 0.0970 - val_accuracy: 0.9800\n",
            "Epoch 13/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1139 - accuracy: 0.9802 - val_loss: 0.0982 - val_accuracy: 0.9800\n",
            "Epoch 14/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0884 - accuracy: 0.9832 - val_loss: 0.1007 - val_accuracy: 0.9800\n",
            "Epoch 15/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0981 - accuracy: 0.9827 - val_loss: 0.0964 - val_accuracy: 0.9800\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 5, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 5, 1), dtype=tf.float32, name='conv1d_73_input'), name='conv1d_73_input', description=\"created by layer 'conv1d_73_input'\"), but it was called on an input with incompatible shape (None, 16, 1).\n",
            "Score for fold 10: loss of 0.12797889113426208; accuracy of 97.14285731315613%\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.1309109628200531 - Accuracy: 97.14285731315613%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.03573020547628403 - Accuracy: 100.0%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.07511761784553528 - Accuracy: 98.57142567634583%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.18229396641254425 - Accuracy: 95.71428298950195%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.07752864807844162 - Accuracy: 98.57142567634583%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.13269759714603424 - Accuracy: 97.14285731315613%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.01808084547519684 - Accuracy: 100.0%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.12021353840827942 - Accuracy: 97.14285731315613%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.07789063453674316 - Accuracy: 98.57142567634583%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.07376355677843094 - Accuracy: 98.57142567634583%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 11 - Loss: 0.08265713602304459 - Accuracy: 98.57142567634583%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 12 - Loss: 0.02019042707979679 - Accuracy: 100.0%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 13 - Loss: 0.13092733919620514 - Accuracy: 97.14285731315613%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 14 - Loss: 0.17258477210998535 - Accuracy: 95.71428298950195%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 15 - Loss: 0.03220674395561218 - Accuracy: 100.0%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 16 - Loss: 0.1274092197418213 - Accuracy: 97.14285731315613%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 17 - Loss: 0.0730312168598175 - Accuracy: 98.57142567634583%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 18 - Loss: 0.13006731867790222 - Accuracy: 97.14285731315613%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 19 - Loss: 0.021197576075792313 - Accuracy: 100.0%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 20 - Loss: 0.12797889113426208 - Accuracy: 97.14285731315613%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 98.14285606145859 (+- 1.3627704564423953)\n",
            "> Loss: 0.09212391069158912\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPYK5iGDQlo3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C85isjlPC-pj"
      },
      "source": [
        "###### chb dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX7I_ko4QlsY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "7a5f9712-9915-4fb0-8a3e-68b0e3ee2b1f"
      },
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"chb.csv\")\n",
        "df.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>T8-P8-1_n_peaks</th>\n",
              "      <th>T8-P8-1_n_crossings</th>\n",
              "      <th>T8-P8-1_hfd</th>\n",
              "      <th>T8-P8-1_pfd</th>\n",
              "      <th>T8-P8-1_hurst_exp</th>\n",
              "      <th>T8-P8-1_spectral_entropy</th>\n",
              "      <th>T8-P8-1_total_power</th>\n",
              "      <th>T8-P8-1_median_freq</th>\n",
              "      <th>T8-P8-1_peak_freq</th>\n",
              "      <th>T8-P8-1_hjorth_mobility</th>\n",
              "      <th>T8-P8-1_hjorth_complexity</th>\n",
              "      <th>T8-P8-1_power_1hz</th>\n",
              "      <th>T8-P8-1_power_5hz</th>\n",
              "      <th>T8-P8-1_power_10hz</th>\n",
              "      <th>T8-P8-1_power_15hz</th>\n",
              "      <th>T8-P8-1_power_20hz</th>\n",
              "      <th>seizure</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>183</td>\n",
              "      <td>153</td>\n",
              "      <td>0.094016</td>\n",
              "      <td>0.611024</td>\n",
              "      <td>0.879401</td>\n",
              "      <td>0.747444</td>\n",
              "      <td>5.090000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.002766</td>\n",
              "      <td>171.222337</td>\n",
              "      <td>0.420823</td>\n",
              "      <td>0.226051</td>\n",
              "      <td>0.128492</td>\n",
              "      <td>0.106838</td>\n",
              "      <td>0.117797</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>183</td>\n",
              "      <td>165</td>\n",
              "      <td>0.092275</td>\n",
              "      <td>0.611024</td>\n",
              "      <td>0.746361</td>\n",
              "      <td>0.765824</td>\n",
              "      <td>5.560000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003211</td>\n",
              "      <td>148.493351</td>\n",
              "      <td>0.384135</td>\n",
              "      <td>0.234295</td>\n",
              "      <td>0.140616</td>\n",
              "      <td>0.115382</td>\n",
              "      <td>0.125572</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>186</td>\n",
              "      <td>170</td>\n",
              "      <td>0.098374</td>\n",
              "      <td>0.610385</td>\n",
              "      <td>0.687973</td>\n",
              "      <td>0.752050</td>\n",
              "      <td>5.450000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003564</td>\n",
              "      <td>142.209197</td>\n",
              "      <td>0.389655</td>\n",
              "      <td>0.238327</td>\n",
              "      <td>0.124545</td>\n",
              "      <td>0.109260</td>\n",
              "      <td>0.138213</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>189</td>\n",
              "      <td>166</td>\n",
              "      <td>0.099552</td>\n",
              "      <td>0.609756</td>\n",
              "      <td>0.726399</td>\n",
              "      <td>0.754101</td>\n",
              "      <td>5.920000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003514</td>\n",
              "      <td>140.840795</td>\n",
              "      <td>0.396989</td>\n",
              "      <td>0.232559</td>\n",
              "      <td>0.120853</td>\n",
              "      <td>0.118132</td>\n",
              "      <td>0.131467</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>188</td>\n",
              "      <td>167</td>\n",
              "      <td>0.101196</td>\n",
              "      <td>0.609756</td>\n",
              "      <td>0.755442</td>\n",
              "      <td>0.758187</td>\n",
              "      <td>5.830000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003494</td>\n",
              "      <td>142.434220</td>\n",
              "      <td>0.390794</td>\n",
              "      <td>0.230665</td>\n",
              "      <td>0.125717</td>\n",
              "      <td>0.119646</td>\n",
              "      <td>0.133177</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   T8-P8-1_n_peaks  T8-P8-1_n_crossings  ...  T8-P8-1_power_20hz  seizure\n",
              "0              183                  153  ...            0.117797        0\n",
              "1              183                  165  ...            0.125572        0\n",
              "2              186                  170  ...            0.138213        0\n",
              "3              189                  166  ...            0.131467        0\n",
              "4              188                  167  ...            0.133177        0\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jiM9qcrQlvI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "acb90adf-531a-4412-e5db-60c2c6d0ab9b"
      },
      "source": [
        "feature_data = df.iloc[:,0:-1]\n",
        "feature_data.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>T8-P8-1_n_peaks</th>\n",
              "      <th>T8-P8-1_n_crossings</th>\n",
              "      <th>T8-P8-1_hfd</th>\n",
              "      <th>T8-P8-1_pfd</th>\n",
              "      <th>T8-P8-1_hurst_exp</th>\n",
              "      <th>T8-P8-1_spectral_entropy</th>\n",
              "      <th>T8-P8-1_total_power</th>\n",
              "      <th>T8-P8-1_median_freq</th>\n",
              "      <th>T8-P8-1_peak_freq</th>\n",
              "      <th>T8-P8-1_hjorth_mobility</th>\n",
              "      <th>T8-P8-1_hjorth_complexity</th>\n",
              "      <th>T8-P8-1_power_1hz</th>\n",
              "      <th>T8-P8-1_power_5hz</th>\n",
              "      <th>T8-P8-1_power_10hz</th>\n",
              "      <th>T8-P8-1_power_15hz</th>\n",
              "      <th>T8-P8-1_power_20hz</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>183</td>\n",
              "      <td>153</td>\n",
              "      <td>0.094016</td>\n",
              "      <td>0.611024</td>\n",
              "      <td>0.879401</td>\n",
              "      <td>0.747444</td>\n",
              "      <td>5.090000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.002766</td>\n",
              "      <td>171.222337</td>\n",
              "      <td>0.420823</td>\n",
              "      <td>0.226051</td>\n",
              "      <td>0.128492</td>\n",
              "      <td>0.106838</td>\n",
              "      <td>0.117797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>183</td>\n",
              "      <td>165</td>\n",
              "      <td>0.092275</td>\n",
              "      <td>0.611024</td>\n",
              "      <td>0.746361</td>\n",
              "      <td>0.765824</td>\n",
              "      <td>5.560000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003211</td>\n",
              "      <td>148.493351</td>\n",
              "      <td>0.384135</td>\n",
              "      <td>0.234295</td>\n",
              "      <td>0.140616</td>\n",
              "      <td>0.115382</td>\n",
              "      <td>0.125572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>186</td>\n",
              "      <td>170</td>\n",
              "      <td>0.098374</td>\n",
              "      <td>0.610385</td>\n",
              "      <td>0.687973</td>\n",
              "      <td>0.752050</td>\n",
              "      <td>5.450000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003564</td>\n",
              "      <td>142.209197</td>\n",
              "      <td>0.389655</td>\n",
              "      <td>0.238327</td>\n",
              "      <td>0.124545</td>\n",
              "      <td>0.109260</td>\n",
              "      <td>0.138213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>189</td>\n",
              "      <td>166</td>\n",
              "      <td>0.099552</td>\n",
              "      <td>0.609756</td>\n",
              "      <td>0.726399</td>\n",
              "      <td>0.754101</td>\n",
              "      <td>5.920000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003514</td>\n",
              "      <td>140.840795</td>\n",
              "      <td>0.396989</td>\n",
              "      <td>0.232559</td>\n",
              "      <td>0.120853</td>\n",
              "      <td>0.118132</td>\n",
              "      <td>0.131467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>188</td>\n",
              "      <td>167</td>\n",
              "      <td>0.101196</td>\n",
              "      <td>0.609756</td>\n",
              "      <td>0.755442</td>\n",
              "      <td>0.758187</td>\n",
              "      <td>5.830000e-10</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003494</td>\n",
              "      <td>142.434220</td>\n",
              "      <td>0.390794</td>\n",
              "      <td>0.230665</td>\n",
              "      <td>0.125717</td>\n",
              "      <td>0.119646</td>\n",
              "      <td>0.133177</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   T8-P8-1_n_peaks  T8-P8-1_n_crossings  ...  T8-P8-1_power_15hz  T8-P8-1_power_20hz\n",
              "0              183                  153  ...            0.106838            0.117797\n",
              "1              183                  165  ...            0.115382            0.125572\n",
              "2              186                  170  ...            0.109260            0.138213\n",
              "3              189                  166  ...            0.118132            0.131467\n",
              "4              188                  167  ...            0.119646            0.133177\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C9oiZk7Qlx4"
      },
      "source": [
        "#dataprocessing\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "X_train_minmax = min_max_scaler.fit_transform(feature_data)\n",
        "#X_train_minmax"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN12I1UFQl0l"
      },
      "source": [
        "y = df[\"seizure\"]\n",
        "x_train, x_test, y_train, y_test = SplitData(X_train_minmax,y , 0.30)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg1yMrOUQl6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f84a18e-6a39-42e3-9da8-e6defb388888"
      },
      "source": [
        "feature_data.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRm5Lj6JDcGI"
      },
      "source": [
        "num_folds=10\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBtIUdRbDcGR"
      },
      "source": [
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "X_train_tf = np.expand_dims(x_train, axis=2)\n",
        "X_valid_tf = np.expand_dims(x_test, axis=2)\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train , num_classes=2)\n",
        "y_valid = keras.utils.to_categorical(y_test , num_classes=2)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjS8hnkyDcGR",
        "outputId": "314ee657-f893-412a-f894-dfb77bd97693"
      },
      "source": [
        "print(x_test.shape)\n",
        "print(y_valid.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(300, 16)\n",
            "(300, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djw0xMAiDcGR",
        "outputId": "f20b26df-404c-40d0-b560-f81e04d902ab"
      },
      "source": [
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(X_train_tf, y_train):\n",
        "\n",
        "  # Define the model architecture\n",
        "  model = Sequential()\n",
        "  model.add(Convolution1D(64, 10, strides=2, padding='same', activation='relu',  input_shape=(16, 1)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(MaxPooling1D(3))\n",
        "  model.add(Convolution1D(40, 5, strides=2, padding='same', activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Convolution1D(32, 4, strides=1, padding='same', activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(GlobalAveragePooling1D())\n",
        "  model.add(Dense(50, activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "  \n",
        "  # Compile the model\n",
        "  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "  data_train=model.fit(X_train_tf, y_train, epochs= 15, validation_data=(X_valid_tf, y_valid), batch_size=20)\n",
        "\n",
        "  # Generate generalization metrics\n",
        "  scores = model.evaluate(X_train_tf[test], y_train[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "\n",
        "# == Provide average scores ==\n",
        "\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/15\n",
            "35/35 [==============================] - 2s 12ms/step - loss: 0.5854 - accuracy: 0.9432 - val_loss: 0.1051 - val_accuracy: 0.9800\n",
            "Epoch 2/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1098 - accuracy: 0.9828 - val_loss: 0.0983 - val_accuracy: 0.9800\n",
            "Epoch 3/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0837 - accuracy: 0.9861 - val_loss: 0.0991 - val_accuracy: 0.9800\n",
            "Epoch 4/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1035 - accuracy: 0.9827 - val_loss: 0.0983 - val_accuracy: 0.9800\n",
            "Epoch 5/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0966 - accuracy: 0.9848 - val_loss: 0.0979 - val_accuracy: 0.9800\n",
            "Epoch 6/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0919 - accuracy: 0.9837 - val_loss: 0.1005 - val_accuracy: 0.9800\n",
            "Epoch 7/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0794 - accuracy: 0.9877 - val_loss: 0.1004 - val_accuracy: 0.9800\n",
            "Epoch 8/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1204 - accuracy: 0.9771 - val_loss: 0.0971 - val_accuracy: 0.9800\n",
            "Epoch 9/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1023 - accuracy: 0.9803 - val_loss: 0.0986 - val_accuracy: 0.9800\n",
            "Epoch 10/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0993 - accuracy: 0.9831 - val_loss: 0.0984 - val_accuracy: 0.9800\n",
            "Epoch 11/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0727 - accuracy: 0.9870 - val_loss: 0.1037 - val_accuracy: 0.9800\n",
            "Epoch 12/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1414 - accuracy: 0.9729 - val_loss: 0.1017 - val_accuracy: 0.9800\n",
            "Epoch 13/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1565 - accuracy: 0.9731 - val_loss: 0.0965 - val_accuracy: 0.9800\n",
            "Epoch 14/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0586 - accuracy: 0.9907 - val_loss: 0.1032 - val_accuracy: 0.9800\n",
            "Epoch 15/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0800 - accuracy: 0.9870 - val_loss: 0.0978 - val_accuracy: 0.9800\n",
            "Score for fold 1: loss of 0.1309109628200531; accuracy of 97.14285731315613%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/15\n",
            "35/35 [==============================] - 1s 10ms/step - loss: 0.6286 - accuracy: 0.9104 - val_loss: 0.1543 - val_accuracy: 0.9800\n",
            "Epoch 2/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1211 - accuracy: 0.9821 - val_loss: 0.0979 - val_accuracy: 0.9800\n",
            "Epoch 3/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1596 - accuracy: 0.9701 - val_loss: 0.0976 - val_accuracy: 0.9800\n",
            "Epoch 4/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1512 - accuracy: 0.9727 - val_loss: 0.0973 - val_accuracy: 0.9800\n",
            "Epoch 5/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0634 - accuracy: 0.9888 - val_loss: 0.1031 - val_accuracy: 0.9800\n",
            "Epoch 6/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0870 - accuracy: 0.9861 - val_loss: 0.0993 - val_accuracy: 0.9800\n",
            "Epoch 7/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1222 - accuracy: 0.9751 - val_loss: 0.0972 - val_accuracy: 0.9800\n",
            "Epoch 8/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1194 - accuracy: 0.9784 - val_loss: 0.0992 - val_accuracy: 0.9800\n",
            "Epoch 9/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1113 - accuracy: 0.9782 - val_loss: 0.0970 - val_accuracy: 0.9800\n",
            "Epoch 10/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1519 - accuracy: 0.9713 - val_loss: 0.0976 - val_accuracy: 0.9800\n",
            "Epoch 11/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1464 - accuracy: 0.9761 - val_loss: 0.0982 - val_accuracy: 0.9800\n",
            "Epoch 12/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1351 - accuracy: 0.9728 - val_loss: 0.0966 - val_accuracy: 0.9800\n",
            "Epoch 13/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0885 - accuracy: 0.9831 - val_loss: 0.0992 - val_accuracy: 0.9800\n",
            "Epoch 14/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0859 - accuracy: 0.9871 - val_loss: 0.0994 - val_accuracy: 0.9800\n",
            "Epoch 15/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0911 - accuracy: 0.9828 - val_loss: 0.1000 - val_accuracy: 0.9800\n",
            "Score for fold 2: loss of 0.03573020547628403; accuracy of 100.0%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/15\n",
            "35/35 [==============================] - 1s 10ms/step - loss: 0.6265 - accuracy: 0.8919 - val_loss: 0.1470 - val_accuracy: 0.9800\n",
            "Epoch 2/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1042 - accuracy: 0.9856 - val_loss: 0.0974 - val_accuracy: 0.9800\n",
            "Epoch 3/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1084 - accuracy: 0.9818 - val_loss: 0.1002 - val_accuracy: 0.9800\n",
            "Epoch 4/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1204 - accuracy: 0.9752 - val_loss: 0.0969 - val_accuracy: 0.9800\n",
            "Epoch 5/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1243 - accuracy: 0.9766 - val_loss: 0.0967 - val_accuracy: 0.9800\n",
            "Epoch 6/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0994 - accuracy: 0.9807 - val_loss: 0.0973 - val_accuracy: 0.9800\n",
            "Epoch 7/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0959 - accuracy: 0.9817 - val_loss: 0.0997 - val_accuracy: 0.9800\n",
            "Epoch 8/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0814 - accuracy: 0.9849 - val_loss: 0.1003 - val_accuracy: 0.9800\n",
            "Epoch 9/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1081 - accuracy: 0.9785 - val_loss: 0.0977 - val_accuracy: 0.9800\n",
            "Epoch 10/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1305 - accuracy: 0.9751 - val_loss: 0.0988 - val_accuracy: 0.9800\n",
            "Epoch 11/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0806 - accuracy: 0.9860 - val_loss: 0.1079 - val_accuracy: 0.9800\n",
            "Epoch 12/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0869 - accuracy: 0.9841 - val_loss: 0.0993 - val_accuracy: 0.9800\n",
            "Epoch 13/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1228 - accuracy: 0.9760 - val_loss: 0.0952 - val_accuracy: 0.9800\n",
            "Epoch 14/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1010 - accuracy: 0.9810 - val_loss: 0.0959 - val_accuracy: 0.9800\n",
            "Epoch 15/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1104 - accuracy: 0.9804 - val_loss: 0.0956 - val_accuracy: 0.9800\n",
            "Score for fold 3: loss of 0.07511761784553528; accuracy of 98.57142567634583%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/15\n",
            "35/35 [==============================] - 1s 10ms/step - loss: 0.6168 - accuracy: 0.9323 - val_loss: 0.1478 - val_accuracy: 0.9800\n",
            "Epoch 2/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1248 - accuracy: 0.9808 - val_loss: 0.0975 - val_accuracy: 0.9800\n",
            "Epoch 3/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1221 - accuracy: 0.9799 - val_loss: 0.1040 - val_accuracy: 0.9800\n",
            "Epoch 4/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0937 - accuracy: 0.9830 - val_loss: 0.0981 - val_accuracy: 0.9800\n",
            "Epoch 5/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0821 - accuracy: 0.9876 - val_loss: 0.1121 - val_accuracy: 0.9800\n",
            "Epoch 6/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1096 - accuracy: 0.9796 - val_loss: 0.0979 - val_accuracy: 0.9800\n",
            "Epoch 7/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1033 - accuracy: 0.9780 - val_loss: 0.0979 - val_accuracy: 0.9800\n",
            "Epoch 8/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0824 - accuracy: 0.9833 - val_loss: 0.0984 - val_accuracy: 0.9800\n",
            "Epoch 9/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1166 - accuracy: 0.9755 - val_loss: 0.0973 - val_accuracy: 0.9800\n",
            "Epoch 10/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0879 - accuracy: 0.9827 - val_loss: 0.1037 - val_accuracy: 0.9800\n",
            "Epoch 11/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1237 - accuracy: 0.9755 - val_loss: 0.0970 - val_accuracy: 0.9800\n",
            "Epoch 12/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1475 - accuracy: 0.9728 - val_loss: 0.0990 - val_accuracy: 0.9800\n",
            "Epoch 13/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1374 - accuracy: 0.9713 - val_loss: 0.0965 - val_accuracy: 0.9800\n",
            "Epoch 14/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1201 - accuracy: 0.9774 - val_loss: 0.0970 - val_accuracy: 0.9800\n",
            "Epoch 15/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1225 - accuracy: 0.9773 - val_loss: 0.0964 - val_accuracy: 0.9800\n",
            "Score for fold 4: loss of 0.18229396641254425; accuracy of 95.71428298950195%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/15\n",
            "35/35 [==============================] - 1s 11ms/step - loss: 0.6019 - accuracy: 0.9353 - val_loss: 0.1170 - val_accuracy: 0.9800\n",
            "Epoch 2/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1301 - accuracy: 0.9768 - val_loss: 0.1003 - val_accuracy: 0.9800\n",
            "Epoch 3/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1271 - accuracy: 0.9763 - val_loss: 0.0976 - val_accuracy: 0.9800\n",
            "Epoch 4/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1281 - accuracy: 0.9737 - val_loss: 0.0974 - val_accuracy: 0.9800\n",
            "Epoch 5/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0946 - accuracy: 0.9856 - val_loss: 0.1008 - val_accuracy: 0.9800\n",
            "Epoch 6/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1321 - accuracy: 0.9733 - val_loss: 0.0969 - val_accuracy: 0.9800\n",
            "Epoch 7/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0835 - accuracy: 0.9846 - val_loss: 0.0979 - val_accuracy: 0.9800\n",
            "Epoch 8/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0672 - accuracy: 0.9898 - val_loss: 0.1063 - val_accuracy: 0.9800\n",
            "Epoch 9/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1154 - accuracy: 0.9806 - val_loss: 0.0969 - val_accuracy: 0.9800\n",
            "Epoch 10/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1257 - accuracy: 0.9765 - val_loss: 0.0966 - val_accuracy: 0.9800\n",
            "Epoch 11/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1289 - accuracy: 0.9750 - val_loss: 0.0967 - val_accuracy: 0.9800\n",
            "Epoch 12/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1100 - accuracy: 0.9784 - val_loss: 0.1018 - val_accuracy: 0.9800\n",
            "Epoch 13/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1385 - accuracy: 0.9737 - val_loss: 0.0976 - val_accuracy: 0.9800\n",
            "Epoch 14/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0893 - accuracy: 0.9840 - val_loss: 0.0964 - val_accuracy: 0.9800\n",
            "Epoch 15/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1084 - accuracy: 0.9781 - val_loss: 0.0965 - val_accuracy: 0.9800\n",
            "Score for fold 5: loss of 0.07752864807844162; accuracy of 98.57142567634583%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 6 ...\n",
            "Epoch 1/15\n",
            "35/35 [==============================] - 1s 10ms/step - loss: 0.6070 - accuracy: 0.9325 - val_loss: 0.1243 - val_accuracy: 0.9800\n",
            "Epoch 2/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1153 - accuracy: 0.9814 - val_loss: 0.0976 - val_accuracy: 0.9800\n",
            "Epoch 3/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1029 - accuracy: 0.9845 - val_loss: 0.1021 - val_accuracy: 0.9800\n",
            "Epoch 4/15\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.1193 - accuracy: 0.9796 - val_loss: 0.1004 - val_accuracy: 0.9800\n",
            "Epoch 5/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1157 - accuracy: 0.9821 - val_loss: 0.0994 - val_accuracy: 0.9800\n",
            "Epoch 6/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0646 - accuracy: 0.9908 - val_loss: 0.1026 - val_accuracy: 0.9800\n",
            "Epoch 7/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0999 - accuracy: 0.9840 - val_loss: 0.0985 - val_accuracy: 0.9800\n",
            "Epoch 8/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0668 - accuracy: 0.9891 - val_loss: 0.0988 - val_accuracy: 0.9800\n",
            "Epoch 9/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1634 - accuracy: 0.9668 - val_loss: 0.1040 - val_accuracy: 0.9800\n",
            "Epoch 10/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0852 - accuracy: 0.9880 - val_loss: 0.1104 - val_accuracy: 0.9800\n",
            "Epoch 11/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0796 - accuracy: 0.9877 - val_loss: 0.0974 - val_accuracy: 0.9800\n",
            "Epoch 12/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1060 - accuracy: 0.9816 - val_loss: 0.0966 - val_accuracy: 0.9800\n",
            "Epoch 13/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0812 - accuracy: 0.9855 - val_loss: 0.0986 - val_accuracy: 0.9800\n",
            "Epoch 14/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0915 - accuracy: 0.9812 - val_loss: 0.0970 - val_accuracy: 0.9800\n",
            "Epoch 15/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0735 - accuracy: 0.9893 - val_loss: 0.1012 - val_accuracy: 0.9800\n",
            "Score for fold 6: loss of 0.13269759714603424; accuracy of 97.14285731315613%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 7 ...\n",
            "Epoch 1/15\n",
            "35/35 [==============================] - 1s 10ms/step - loss: 0.6201 - accuracy: 0.8242 - val_loss: 0.1683 - val_accuracy: 0.9800\n",
            "Epoch 2/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1356 - accuracy: 0.9789 - val_loss: 0.0984 - val_accuracy: 0.9800\n",
            "Epoch 3/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0977 - accuracy: 0.9836 - val_loss: 0.0991 - val_accuracy: 0.9800\n",
            "Epoch 4/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0913 - accuracy: 0.9841 - val_loss: 0.0979 - val_accuracy: 0.9800\n",
            "Epoch 5/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0645 - accuracy: 0.9901 - val_loss: 0.1006 - val_accuracy: 0.9800\n",
            "Epoch 6/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0779 - accuracy: 0.9877 - val_loss: 0.0997 - val_accuracy: 0.9800\n",
            "Epoch 7/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1015 - accuracy: 0.9812 - val_loss: 0.0972 - val_accuracy: 0.9800\n",
            "Epoch 8/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0928 - accuracy: 0.9829 - val_loss: 0.1003 - val_accuracy: 0.9800\n",
            "Epoch 9/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1088 - accuracy: 0.9797 - val_loss: 0.0968 - val_accuracy: 0.9800\n",
            "Epoch 10/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0922 - accuracy: 0.9859 - val_loss: 0.1060 - val_accuracy: 0.9800\n",
            "Epoch 11/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1031 - accuracy: 0.9815 - val_loss: 0.0970 - val_accuracy: 0.9800\n",
            "Epoch 12/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1017 - accuracy: 0.9817 - val_loss: 0.0982 - val_accuracy: 0.9800\n",
            "Epoch 13/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1245 - accuracy: 0.9772 - val_loss: 0.0970 - val_accuracy: 0.9800\n",
            "Epoch 14/15\n",
            "35/35 [==============================] - 0s 6ms/step - loss: 0.0876 - accuracy: 0.9827 - val_loss: 0.0990 - val_accuracy: 0.9800\n",
            "Epoch 15/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1093 - accuracy: 0.9799 - val_loss: 0.0961 - val_accuracy: 0.9800\n",
            "Score for fold 7: loss of 0.01808084547519684; accuracy of 100.0%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 8 ...\n",
            "Epoch 1/15\n",
            "35/35 [==============================] - 1s 10ms/step - loss: 0.5509 - accuracy: 0.9805 - val_loss: 0.0975 - val_accuracy: 0.9800\n",
            "Epoch 2/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0983 - accuracy: 0.9807 - val_loss: 0.0979 - val_accuracy: 0.9800\n",
            "Epoch 3/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1147 - accuracy: 0.9816 - val_loss: 0.0994 - val_accuracy: 0.9800\n",
            "Epoch 4/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1034 - accuracy: 0.9813 - val_loss: 0.0986 - val_accuracy: 0.9800\n",
            "Epoch 5/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1157 - accuracy: 0.9783 - val_loss: 0.0972 - val_accuracy: 0.9800\n",
            "Epoch 6/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0889 - accuracy: 0.9809 - val_loss: 0.0996 - val_accuracy: 0.9800\n",
            "Epoch 7/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1114 - accuracy: 0.9812 - val_loss: 0.0967 - val_accuracy: 0.9800\n",
            "Epoch 8/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0762 - accuracy: 0.9862 - val_loss: 0.1004 - val_accuracy: 0.9800\n",
            "Epoch 9/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0787 - accuracy: 0.9887 - val_loss: 0.0991 - val_accuracy: 0.9800\n",
            "Epoch 10/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0539 - accuracy: 0.9930 - val_loss: 0.1079 - val_accuracy: 0.9800\n",
            "Epoch 11/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0919 - accuracy: 0.9840 - val_loss: 0.0962 - val_accuracy: 0.9800\n",
            "Epoch 12/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1181 - accuracy: 0.9745 - val_loss: 0.0962 - val_accuracy: 0.9800\n",
            "Epoch 13/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0953 - accuracy: 0.9828 - val_loss: 0.0981 - val_accuracy: 0.9800\n",
            "Epoch 14/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0992 - accuracy: 0.9803 - val_loss: 0.0962 - val_accuracy: 0.9800\n",
            "Epoch 15/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0942 - accuracy: 0.9832 - val_loss: 0.0974 - val_accuracy: 0.9800\n",
            "Score for fold 8: loss of 0.12021353840827942; accuracy of 97.14285731315613%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 9 ...\n",
            "Epoch 1/15\n",
            "35/35 [==============================] - 1s 10ms/step - loss: 0.6166 - accuracy: 0.8642 - val_loss: 0.1292 - val_accuracy: 0.9800\n",
            "Epoch 2/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1119 - accuracy: 0.9790 - val_loss: 0.0976 - val_accuracy: 0.9800\n",
            "Epoch 3/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0921 - accuracy: 0.9855 - val_loss: 0.0999 - val_accuracy: 0.9800\n",
            "Epoch 4/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0799 - accuracy: 0.9869 - val_loss: 0.1016 - val_accuracy: 0.9800\n",
            "Epoch 5/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1029 - accuracy: 0.9833 - val_loss: 0.1015 - val_accuracy: 0.9800\n",
            "Epoch 6/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1142 - accuracy: 0.9782 - val_loss: 0.0975 - val_accuracy: 0.9800\n",
            "Epoch 7/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0922 - accuracy: 0.9832 - val_loss: 0.0982 - val_accuracy: 0.9800\n",
            "Epoch 8/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0832 - accuracy: 0.9857 - val_loss: 0.1010 - val_accuracy: 0.9800\n",
            "Epoch 9/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0797 - accuracy: 0.9884 - val_loss: 0.1043 - val_accuracy: 0.9800\n",
            "Epoch 10/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0957 - accuracy: 0.9817 - val_loss: 0.0979 - val_accuracy: 0.9800\n",
            "Epoch 11/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1265 - accuracy: 0.9759 - val_loss: 0.0976 - val_accuracy: 0.9800\n",
            "Epoch 12/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1447 - accuracy: 0.9720 - val_loss: 0.0964 - val_accuracy: 0.9800\n",
            "Epoch 13/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.1139 - accuracy: 0.9799 - val_loss: 0.0980 - val_accuracy: 0.9800\n",
            "Epoch 14/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0816 - accuracy: 0.9872 - val_loss: 0.1036 - val_accuracy: 0.9800\n",
            "Epoch 15/15\n",
            "35/35 [==============================] - 0s 4ms/step - loss: 0.0864 - accuracy: 0.9866 - val_loss: 0.0964 - val_accuracy: 0.9800\n",
            "Score for fold 9: loss of 0.07789063453674316; accuracy of 98.57142567634583%\n",
            "------------------------------------------------------------------------\n",
            "Training for fold 10 ...\n",
            "Epoch 1/15\n",
            "35/35 [==============================] - 1s 11ms/step - loss: 0.6302 - accuracy: 0.9025 - val_loss: 0.1892 - val_accuracy: 0.9800\n",
            "Epoch 2/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1297 - accuracy: 0.9814 - val_loss: 0.0980 - val_accuracy: 0.9800\n",
            "Epoch 3/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1010 - accuracy: 0.9820 - val_loss: 0.1017 - val_accuracy: 0.9800\n",
            "Epoch 4/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1179 - accuracy: 0.9783 - val_loss: 0.0978 - val_accuracy: 0.9800\n",
            "Epoch 5/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1065 - accuracy: 0.9797 - val_loss: 0.0987 - val_accuracy: 0.9800\n",
            "Epoch 6/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0959 - accuracy: 0.9791 - val_loss: 0.0972 - val_accuracy: 0.9800\n",
            "Epoch 7/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1002 - accuracy: 0.9807 - val_loss: 0.0972 - val_accuracy: 0.9800\n",
            "Epoch 8/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0716 - accuracy: 0.9874 - val_loss: 0.1011 - val_accuracy: 0.9800\n",
            "Epoch 9/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0685 - accuracy: 0.9901 - val_loss: 0.1023 - val_accuracy: 0.9800\n",
            "Epoch 10/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1138 - accuracy: 0.9781 - val_loss: 0.0968 - val_accuracy: 0.9800\n",
            "Epoch 11/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1383 - accuracy: 0.9722 - val_loss: 0.0979 - val_accuracy: 0.9800\n",
            "Epoch 12/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0889 - accuracy: 0.9842 - val_loss: 0.0972 - val_accuracy: 0.9800\n",
            "Epoch 13/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.1072 - accuracy: 0.9808 - val_loss: 0.0972 - val_accuracy: 0.9800\n",
            "Epoch 14/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0561 - accuracy: 0.9905 - val_loss: 0.0999 - val_accuracy: 0.9800\n",
            "Epoch 15/15\n",
            "35/35 [==============================] - 0s 5ms/step - loss: 0.0877 - accuracy: 0.9834 - val_loss: 0.0962 - val_accuracy: 0.9800\n",
            "Score for fold 10: loss of 0.07376355677843094; accuracy of 98.57142567634583%\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.1309109628200531 - Accuracy: 97.14285731315613%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.03573020547628403 - Accuracy: 100.0%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.07511761784553528 - Accuracy: 98.57142567634583%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.18229396641254425 - Accuracy: 95.71428298950195%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.07752864807844162 - Accuracy: 98.57142567634583%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.13269759714603424 - Accuracy: 97.14285731315613%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.01808084547519684 - Accuracy: 100.0%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.12021353840827942 - Accuracy: 97.14285731315613%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.07789063453674316 - Accuracy: 98.57142567634583%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 0.07376355677843094 - Accuracy: 98.57142567634583%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 98.14285576343536 (+- 1.285714374649053)\n",
            "> Loss: 0.09242275729775429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "id": "gnUnf0ZaEQkc",
        "outputId": "62b4abdc-bc92-4b11-f622-db590798c9f0"
      },
      "source": [
        "# The history.history[\"loss\"] entry is a dictionary with as many values as epochs that the\n",
        "# model was trained on. \n",
        "df_loss_acc = pd.DataFrame(data_train.history)\n",
        "df_loss= df_loss_acc[['loss','val_loss']]\n",
        "df_loss.rename(columns={'loss':'train','val_loss':'validation'},inplace=True)\n",
        "df_acc= df_loss_acc[['accuracy','val_accuracy']]\n",
        "df_acc.rename(columns={'accuracy':'train','val_accuracy':'validation'},inplace=True)\n",
        "df_loss.plot(title='Model loss',figsize=(7,7)).set(xlabel='Epoch',ylabel='Loss')\n",
        "df_acc.plot(title='Model Accuracy',figsize=(7,7)).set(xlabel='Epoch',ylabel='Accuracy')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0, 0.5, 'Accuracy'), Text(0.5, 0, 'Epoch')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbFH5kxH66_k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqTrb0vV7WLg"
      },
      "source": [
        "# Bayes Algo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "8CWnJafWtjug",
        "outputId": "b22fc50c-a0ae-42cd-b3fc-8b60866f631f"
      },
      "source": [
        "df.drop(df.columns[0], axis=1, inplace=True)\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "      <th>X4</th>\n",
              "      <th>X5</th>\n",
              "      <th>X6</th>\n",
              "      <th>X7</th>\n",
              "      <th>X8</th>\n",
              "      <th>X9</th>\n",
              "      <th>X10</th>\n",
              "      <th>...</th>\n",
              "      <th>X170</th>\n",
              "      <th>X171</th>\n",
              "      <th>X172</th>\n",
              "      <th>X173</th>\n",
              "      <th>X174</th>\n",
              "      <th>X175</th>\n",
              "      <th>X176</th>\n",
              "      <th>X177</th>\n",
              "      <th>X178</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>135</td>\n",
              "      <td>190</td>\n",
              "      <td>229</td>\n",
              "      <td>223</td>\n",
              "      <td>192</td>\n",
              "      <td>125</td>\n",
              "      <td>55</td>\n",
              "      <td>-9</td>\n",
              "      <td>-33</td>\n",
              "      <td>-38</td>\n",
              "      <td>...</td>\n",
              "      <td>-17</td>\n",
              "      <td>-15</td>\n",
              "      <td>-31</td>\n",
              "      <td>-77</td>\n",
              "      <td>-103</td>\n",
              "      <td>-127</td>\n",
              "      <td>-116</td>\n",
              "      <td>-83</td>\n",
              "      <td>-51</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>386</td>\n",
              "      <td>382</td>\n",
              "      <td>356</td>\n",
              "      <td>331</td>\n",
              "      <td>320</td>\n",
              "      <td>315</td>\n",
              "      <td>307</td>\n",
              "      <td>272</td>\n",
              "      <td>244</td>\n",
              "      <td>232</td>\n",
              "      <td>...</td>\n",
              "      <td>164</td>\n",
              "      <td>150</td>\n",
              "      <td>146</td>\n",
              "      <td>152</td>\n",
              "      <td>157</td>\n",
              "      <td>156</td>\n",
              "      <td>154</td>\n",
              "      <td>143</td>\n",
              "      <td>129</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-32</td>\n",
              "      <td>-39</td>\n",
              "      <td>-47</td>\n",
              "      <td>-37</td>\n",
              "      <td>-32</td>\n",
              "      <td>-36</td>\n",
              "      <td>-57</td>\n",
              "      <td>-73</td>\n",
              "      <td>-85</td>\n",
              "      <td>-94</td>\n",
              "      <td>...</td>\n",
              "      <td>57</td>\n",
              "      <td>64</td>\n",
              "      <td>48</td>\n",
              "      <td>19</td>\n",
              "      <td>-12</td>\n",
              "      <td>-30</td>\n",
              "      <td>-35</td>\n",
              "      <td>-35</td>\n",
              "      <td>-36</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-105</td>\n",
              "      <td>-101</td>\n",
              "      <td>-96</td>\n",
              "      <td>-92</td>\n",
              "      <td>-89</td>\n",
              "      <td>-95</td>\n",
              "      <td>-102</td>\n",
              "      <td>-100</td>\n",
              "      <td>-87</td>\n",
              "      <td>-79</td>\n",
              "      <td>...</td>\n",
              "      <td>-82</td>\n",
              "      <td>-81</td>\n",
              "      <td>-80</td>\n",
              "      <td>-77</td>\n",
              "      <td>-85</td>\n",
              "      <td>-77</td>\n",
              "      <td>-72</td>\n",
              "      <td>-69</td>\n",
              "      <td>-65</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-9</td>\n",
              "      <td>-65</td>\n",
              "      <td>-98</td>\n",
              "      <td>-102</td>\n",
              "      <td>-78</td>\n",
              "      <td>-48</td>\n",
              "      <td>-16</td>\n",
              "      <td>0</td>\n",
              "      <td>-21</td>\n",
              "      <td>-59</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>-12</td>\n",
              "      <td>-32</td>\n",
              "      <td>-41</td>\n",
              "      <td>-65</td>\n",
              "      <td>-83</td>\n",
              "      <td>-89</td>\n",
              "      <td>-73</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 179 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    X1   X2   X3   X4   X5   X6   X7   X8   X9  X10  ...  X170  X171  X172  \\\n",
              "0  135  190  229  223  192  125   55   -9  -33  -38  ...   -17   -15   -31   \n",
              "1  386  382  356  331  320  315  307  272  244  232  ...   164   150   146   \n",
              "2  -32  -39  -47  -37  -32  -36  -57  -73  -85  -94  ...    57    64    48   \n",
              "3 -105 -101  -96  -92  -89  -95 -102 -100  -87  -79  ...   -82   -81   -80   \n",
              "4   -9  -65  -98 -102  -78  -48  -16    0  -21  -59  ...     4     2   -12   \n",
              "\n",
              "   X173  X174  X175  X176  X177  X178  y  \n",
              "0   -77  -103  -127  -116   -83   -51  4  \n",
              "1   152   157   156   154   143   129  1  \n",
              "2    19   -12   -30   -35   -35   -36  5  \n",
              "3   -77   -85   -77   -72   -69   -65  5  \n",
              "4   -32   -41   -65   -83   -89   -73  5  \n",
              "\n",
              "[5 rows x 179 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMCG5k4f7pOG",
        "outputId": "9b67088b-5899-456f-a1d7-1b163e60cfc6"
      },
      "source": [
        "df['y'] = df.y.map({1: 1, 2: 0, 3: 0, 4: 0,5:0})\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "      <th>X4</th>\n",
              "      <th>X5</th>\n",
              "      <th>X6</th>\n",
              "      <th>X7</th>\n",
              "      <th>X8</th>\n",
              "      <th>X9</th>\n",
              "      <th>X10</th>\n",
              "      <th>...</th>\n",
              "      <th>X170</th>\n",
              "      <th>X171</th>\n",
              "      <th>X172</th>\n",
              "      <th>X173</th>\n",
              "      <th>X174</th>\n",
              "      <th>X175</th>\n",
              "      <th>X176</th>\n",
              "      <th>X177</th>\n",
              "      <th>X178</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>135</td>\n",
              "      <td>190</td>\n",
              "      <td>229</td>\n",
              "      <td>223</td>\n",
              "      <td>192</td>\n",
              "      <td>125</td>\n",
              "      <td>55</td>\n",
              "      <td>-9</td>\n",
              "      <td>-33</td>\n",
              "      <td>-38</td>\n",
              "      <td>...</td>\n",
              "      <td>-17</td>\n",
              "      <td>-15</td>\n",
              "      <td>-31</td>\n",
              "      <td>-77</td>\n",
              "      <td>-103</td>\n",
              "      <td>-127</td>\n",
              "      <td>-116</td>\n",
              "      <td>-83</td>\n",
              "      <td>-51</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>386</td>\n",
              "      <td>382</td>\n",
              "      <td>356</td>\n",
              "      <td>331</td>\n",
              "      <td>320</td>\n",
              "      <td>315</td>\n",
              "      <td>307</td>\n",
              "      <td>272</td>\n",
              "      <td>244</td>\n",
              "      <td>232</td>\n",
              "      <td>...</td>\n",
              "      <td>164</td>\n",
              "      <td>150</td>\n",
              "      <td>146</td>\n",
              "      <td>152</td>\n",
              "      <td>157</td>\n",
              "      <td>156</td>\n",
              "      <td>154</td>\n",
              "      <td>143</td>\n",
              "      <td>129</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-32</td>\n",
              "      <td>-39</td>\n",
              "      <td>-47</td>\n",
              "      <td>-37</td>\n",
              "      <td>-32</td>\n",
              "      <td>-36</td>\n",
              "      <td>-57</td>\n",
              "      <td>-73</td>\n",
              "      <td>-85</td>\n",
              "      <td>-94</td>\n",
              "      <td>...</td>\n",
              "      <td>57</td>\n",
              "      <td>64</td>\n",
              "      <td>48</td>\n",
              "      <td>19</td>\n",
              "      <td>-12</td>\n",
              "      <td>-30</td>\n",
              "      <td>-35</td>\n",
              "      <td>-35</td>\n",
              "      <td>-36</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-105</td>\n",
              "      <td>-101</td>\n",
              "      <td>-96</td>\n",
              "      <td>-92</td>\n",
              "      <td>-89</td>\n",
              "      <td>-95</td>\n",
              "      <td>-102</td>\n",
              "      <td>-100</td>\n",
              "      <td>-87</td>\n",
              "      <td>-79</td>\n",
              "      <td>...</td>\n",
              "      <td>-82</td>\n",
              "      <td>-81</td>\n",
              "      <td>-80</td>\n",
              "      <td>-77</td>\n",
              "      <td>-85</td>\n",
              "      <td>-77</td>\n",
              "      <td>-72</td>\n",
              "      <td>-69</td>\n",
              "      <td>-65</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-9</td>\n",
              "      <td>-65</td>\n",
              "      <td>-98</td>\n",
              "      <td>-102</td>\n",
              "      <td>-78</td>\n",
              "      <td>-48</td>\n",
              "      <td>-16</td>\n",
              "      <td>0</td>\n",
              "      <td>-21</td>\n",
              "      <td>-59</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>-12</td>\n",
              "      <td>-32</td>\n",
              "      <td>-41</td>\n",
              "      <td>-65</td>\n",
              "      <td>-83</td>\n",
              "      <td>-89</td>\n",
              "      <td>-73</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 179 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    X1   X2   X3   X4   X5   X6   X7   X8   X9  X10  ...  X170  X171  X172  \\\n",
              "0  135  190  229  223  192  125   55   -9  -33  -38  ...   -17   -15   -31   \n",
              "1  386  382  356  331  320  315  307  272  244  232  ...   164   150   146   \n",
              "2  -32  -39  -47  -37  -32  -36  -57  -73  -85  -94  ...    57    64    48   \n",
              "3 -105 -101  -96  -92  -89  -95 -102 -100  -87  -79  ...   -82   -81   -80   \n",
              "4   -9  -65  -98 -102  -78  -48  -16    0  -21  -59  ...     4     2   -12   \n",
              "\n",
              "   X173  X174  X175  X176  X177  X178  y  \n",
              "0   -77  -103  -127  -116   -83   -51  0  \n",
              "1   152   157   156   154   143   129  1  \n",
              "2    19   -12   -30   -35   -35   -36  0  \n",
              "3   -77   -85   -77   -72   -69   -65  0  \n",
              "4   -32   -41   -65   -83   -89   -73  0  \n",
              "\n",
              "[5 rows x 179 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSuBJCn0tjum",
        "outputId": "1e9b9c97-2bec-4bbf-f97d-195befcdfa55"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11500, 179)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UCWItk-tjuq",
        "outputId": "68798aae-164f-4e16-b8d3-0d4d1158afd0"
      },
      "source": [
        "df['y'].value_counts() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    9200\n",
              "1    2300\n",
              "Name: y, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEit5OeD7pOH",
        "outputId": "24852c80-3551-4365-dda8-46c0db1520a6"
      },
      "source": [
        "df['y'].hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIiMG5M67pOH",
        "outputId": "e0f1378c-62e0-4ac5-b92c-7f3e76243437"
      },
      "source": [
        "data = [df[\"X1\"], df[\"X2\"], df[\"X3\"],df[\"X4\"], df[\"X5\"] ]\n",
        "headers = ['X1', 'X2','X3','X4','X5']\n",
        "features_df = pd. concat(data, axis=1, keys=headers)\n",
        "features_df.head(5)\n",
        "#'X11', 'X10','X12','X44','X158'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X1</th>\n",
              "      <th>X2</th>\n",
              "      <th>X3</th>\n",
              "      <th>X4</th>\n",
              "      <th>X5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>135</td>\n",
              "      <td>190</td>\n",
              "      <td>229</td>\n",
              "      <td>223</td>\n",
              "      <td>192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>386</td>\n",
              "      <td>382</td>\n",
              "      <td>356</td>\n",
              "      <td>331</td>\n",
              "      <td>320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-32</td>\n",
              "      <td>-39</td>\n",
              "      <td>-47</td>\n",
              "      <td>-37</td>\n",
              "      <td>-32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-105</td>\n",
              "      <td>-101</td>\n",
              "      <td>-96</td>\n",
              "      <td>-92</td>\n",
              "      <td>-89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-9</td>\n",
              "      <td>-65</td>\n",
              "      <td>-98</td>\n",
              "      <td>-102</td>\n",
              "      <td>-78</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    X1   X2   X3   X4   X5\n",
              "0  135  190  229  223  192\n",
              "1  386  382  356  331  320\n",
              "2  -32  -39  -47  -37  -32\n",
              "3 -105 -101  -96  -92  -89\n",
              "4   -9  -65  -98 -102  -78"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKutjsn17pOH"
      },
      "source": [
        "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(features_df.iloc[:, 0:6].values, df.iloc[:, -1].values, train_size = 0.7, random_state = 101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJVGSWj1tju6",
        "scrolled": true,
        "outputId": "ec56392c-b40e-4965-9d2c-2eb24551b457"
      },
      "source": [
        "X_TRAIN.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8049, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzfsYtMAtju-",
        "outputId": "47348c53-8810-457f-fdd1-1ff5caf8a4b9"
      },
      "source": [
        "X_TEST.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3451, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s70EjdNtjvC",
        "outputId": "27774930-5049-492f-ce0c-cf33c94969f2"
      },
      "source": [
        "X_TRAIN = np.transpose(X_TRAIN) # Converting features in column format\n",
        "X_TEST = np.transpose(X_TEST)\n",
        "X_TEST"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  6,  91, 101, ..., -45,  22,  42],\n",
              "       [  7, 101,  96, ..., -59,  26,  44],\n",
              "       [ -1, 100,  91, ..., -62,  22,  36],\n",
              "       [-13,  97,  84, ..., -53,  15,  28],\n",
              "       [-21, 106,  79, ..., -26,   7,  37]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jcfDwC-tjvF"
      },
      "source": [
        "Y_TRAIN = np.transpose(Y_TRAIN).reshape(1,8049)\n",
        "Y_TEST = np.transpose(Y_TEST).reshape(1,3451)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq32ubeqtjvI",
        "outputId": "bacdadec-1110-48b4-9415-8862baa0e98b"
      },
      "source": [
        "Total_Train_sample = 8049\n",
        "no_of_c1 = np.sum(Y_TRAIN == 1)  # no.of C1 class records\n",
        "no_of_c2 = np.sum(Y_TRAIN == 0)  # no.of C2 Class records\n",
        "\n",
        "\n",
        "print(no_of_c1,no_of_c2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1631 6418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsecfHBPtjvN",
        "scrolled": true,
        "outputId": "eed73e13-6b2a-4761-b8dc-1fcd919a6bb8"
      },
      "source": [
        "# Calculating prior for each class\n",
        "prior_c1 =  no_of_c1/Total_Train_sample\n",
        "prior_c2 =  no_of_c2/Total_Train_sample\n",
        "\n",
        "print(prior_c1,prior_c2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.20263386756118773 0.7973661324388123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebbQZ9ivtjvR"
      },
      "source": [
        "#Y_TRAIN.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53muQTXR7pOJ",
        "outputId": "bdf7f0d5-84b0-4cf7-9abf-64673dd6b9d1"
      },
      "source": [
        "full_train  = np.append(X_TRAIN, Y_TRAIN, axis=0)\n",
        "full_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  32, -219, -135, ...,   44, -925,   23],\n",
              "       [  52, -221, -145, ...,   52, -759,   14],\n",
              "       [  38, -203, -151, ...,   55, -443,   12],\n",
              "       [  24, -165, -155, ...,   57,  -60,    9],\n",
              "       [ -24, -136, -165, ...,   49,  313,   11],\n",
              "       [   0,    0,    1, ...,    0,    1,    0]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA7Mzz3otjvU"
      },
      "source": [
        "c1_index = [index for index, value in enumerate(full_train[5,:]) if value == 1]\n",
        "c2_index = [index for index, value in enumerate(full_train[5,:]) if value == 0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGoUfdhLtjvY"
      },
      "source": [
        "# Dividing train data as per class\n",
        "\n",
        "c1_train_data = X_TRAIN[:,c1_index]\n",
        "c2_train_data = X_TRAIN[:,c2_index]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZyqLdkptjvb"
      },
      "source": [
        "# Calculating mean for all the two class\n",
        "\n",
        "c1_mean = np.mean(c1_train_data, axis=1).reshape(5,1)\n",
        "c2_mean = np.mean(c2_train_data, axis=1).reshape(5,1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md_geXMgtjvg"
      },
      "source": [
        "#Calculating covarience for all the two classes\n",
        "\n",
        "c1_cov = np.cov(c1_train_data)\n",
        "c2_cov = np.cov(c2_train_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F4aygrRtjvj"
      },
      "source": [
        "# Calculating the likelihood by gaussian distribution formula\n",
        "\n",
        "def likelihood(x, mu, cov, k):\n",
        "    numerator = np.exp(-0.5 * np.matmul(np.matmul(np.transpose(x - mu), np.linalg.inv(cov)), (x - mu)))\n",
        "    denominator = np.sqrt(((2 * np.pi) ** k) * np.linalg.det(cov))\n",
        "    return numerator/denominator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJF65mQh7pOK",
        "outputId": "d04516e1-6452-44ab-c0da-0d64d53fcae9"
      },
      "source": [
        "X_TRAIN.shape[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8049"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSNp0D-Rtjvn"
      },
      "source": [
        "predictions =[]\n",
        "for i in range(X_TRAIN.shape[1]):\n",
        "    sample = np.array(X_TRAIN[[0,1,2,3,4],i]).reshape(5,1)\n",
        "    C1_posterior = prior_c1 * likelihood(sample, c1_mean, c1_cov, 5)\n",
        "    C2_posterior = prior_c2 * likelihood(sample, c2_mean, c2_cov, 5)\n",
        "      \n",
        "    if   C1_posterior > C2_posterior:\n",
        "             sample_class = 1\n",
        "    else:\n",
        "            sample_class = 2\n",
        "    \n",
        "    predictions.append(sample_class)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z2t3-41tjvq"
      },
      "source": [
        "#train_result = predictions ==Y_TRAIN\n",
        "train_correct_predict = np.count_nonzero(Y_TRAIN)\n",
        "#correct_predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXtsCDwxtjvt",
        "outputId": "5c4d3342-b6dc-4eb3-bf51-07019f4d2234"
      },
      "source": [
        "# Training accuracy ( total no of samples correctly predicted / total no.of samples)\n",
        "train_accuracy = train_correct_predict/105\n",
        "print('Training data accuracy is {} '.format(train_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data accuracy is 15.533333333333333 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svMRcPKGtjvw"
      },
      "source": [
        "# Checking on the Test Data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCDyh6pStjvy"
      },
      "source": [
        "predictions =[]\n",
        "for i in range(X_TEST.shape[1]):\n",
        "    sample = np.array(X_TEST[[0,1,2,3,4],i]).reshape(5,1)\n",
        "    C1_posterior = prior_c1 * likelihood(sample, c1_mean, c1_cov, 5)\n",
        "    C2_posterior = prior_c2 * likelihood(sample, c2_mean, c2_cov, 5)\n",
        "     \n",
        "    if   C1_posterior > C2_posterior:\n",
        "             sample_class = 1\n",
        "    else:        \n",
        "            sample_class = 2\n",
        "    \n",
        "    predictions.append(sample_class)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3YdR3Vntjv3"
      },
      "source": [
        "#test_result = test_predictions ==Y_TEST\n",
        "test_correct_predict = np.count_nonzero(Y_TEST)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgCy3Aghtjv7",
        "outputId": "a1e33645-c0f1-4c3b-b54d-515cb60957d9"
      },
      "source": [
        "test_accuracy = test_correct_predict/45\n",
        "print('Test data accuracy is {} '.format(test_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test data accuracy is 14.866666666666667 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b5lVMbl7pOL"
      },
      "source": [
        "##### Boston data set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPGr46wJ7pOL",
        "outputId": "4d2654a4-bf28-42e1-ae22-8b2a596b284d"
      },
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"boston.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.887621</td>\n",
              "      <td>0.185875</td>\n",
              "      <td>2.299474</td>\n",
              "      <td>2.299474</td>\n",
              "      <td>0.581396</td>\n",
              "      <td>healthy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.783744</td>\n",
              "      <td>0.163794</td>\n",
              "      <td>2.299480</td>\n",
              "      <td>2.299480</td>\n",
              "      <td>0.588340</td>\n",
              "      <td>healthy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.875873</td>\n",
              "      <td>0.154714</td>\n",
              "      <td>2.299477</td>\n",
              "      <td>2.299477</td>\n",
              "      <td>0.589802</td>\n",
              "      <td>healthy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.881293</td>\n",
              "      <td>0.138396</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>0.592911</td>\n",
              "      <td>healthy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.798116</td>\n",
              "      <td>0.125291</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>0.595338</td>\n",
              "      <td>healthy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0        f1        f2        f3        f4        f5    class\n",
              "0           0  0.887621  0.185875  2.299474  2.299474  0.581396  healthy\n",
              "1           1  0.783744  0.163794  2.299480  2.299480  0.588340  healthy\n",
              "2           2  0.875873  0.154714  2.299477  2.299477  0.589802  healthy\n",
              "3           3  0.881293  0.138396  2.299481  2.299481  0.592911  healthy\n",
              "4           4  0.798116  0.125291  2.299481  2.299481  0.595338  healthy"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kofjQUxL7pOL"
      },
      "source": [
        "df=df.iloc[:,1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTfwujv77pOL"
      },
      "source": [
        "df[\"Target\"] = df[\"class\"]\n",
        "\n",
        "df['Target'] = df.Target.map({\"seizure\": 1, \"healthy\": 0, \"transation\": 0}) # Checking for presence of CLass 1\n",
        "df.head(5)\n",
        "df.drop(df.columns[5], axis=1, inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWN3FwdB7pOL",
        "outputId": "0bcb25f5-996a-4eee-eb44-06071ae6e268"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.887621</td>\n",
              "      <td>0.185875</td>\n",
              "      <td>2.299474</td>\n",
              "      <td>2.299474</td>\n",
              "      <td>0.581396</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.783744</td>\n",
              "      <td>0.163794</td>\n",
              "      <td>2.299480</td>\n",
              "      <td>2.299480</td>\n",
              "      <td>0.588340</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.875873</td>\n",
              "      <td>0.154714</td>\n",
              "      <td>2.299477</td>\n",
              "      <td>2.299477</td>\n",
              "      <td>0.589802</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.881293</td>\n",
              "      <td>0.138396</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>0.592911</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.798116</td>\n",
              "      <td>0.125291</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>2.299481</td>\n",
              "      <td>0.595338</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         f1        f2        f3        f4        f5  Target\n",
              "0  0.887621  0.185875  2.299474  2.299474  0.581396       0\n",
              "1  0.783744  0.163794  2.299480  2.299480  0.588340       0\n",
              "2  0.875873  0.154714  2.299477  2.299477  0.589802       0\n",
              "3  0.881293  0.138396  2.299481  2.299481  0.592911       0\n",
              "4  0.798116  0.125291  2.299481  2.299481  0.595338       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TBCM06f7pOL"
      },
      "source": [
        "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(df.iloc[:, 0:5].values, df.iloc[:, -1].values, train_size = 0.7, random_state = 101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3IRNFABi7pOM",
        "outputId": "ec56392c-b40e-4965-9d2c-2eb24551b457"
      },
      "source": [
        "X_TRAIN.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(210, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMCnOv9P7pOM",
        "outputId": "47348c53-8810-457f-fdd1-1ff5caf8a4b9"
      },
      "source": [
        "X_TEST.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fW5SaYT7pOM"
      },
      "source": [
        "X_TRAIN = np.transpose(X_TRAIN) # Converting features in column format\n",
        "X_TEST = np.transpose(X_TEST)\n",
        "#X_TEST"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE1Mz9DY7pOM"
      },
      "source": [
        "Y_TRAIN = np.transpose(Y_TRAIN).reshape(1,210)\n",
        "Y_TEST = np.transpose(Y_TEST).reshape(1,90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SEK5NL27pOM",
        "outputId": "bacdadec-1110-48b4-9415-8862baa0e98b"
      },
      "source": [
        "Total_Train_sample = 8049\n",
        "no_of_c1 = np.sum(Y_TRAIN == 1)\n",
        "no_of_c2 = np.sum(Y_TRAIN == 0)\n",
        "\n",
        "\n",
        "print(no_of_c1,no_of_c2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "68 142\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "p_XC8vyI7pOM",
        "outputId": "eed73e13-6b2a-4761-b8dc-1fcd919a6bb8"
      },
      "source": [
        "# Calculating prior for each class\n",
        "prior_c1 =  no_of_c1/Total_Train_sample\n",
        "prior_c2 =  no_of_c2/Total_Train_sample\n",
        "\n",
        "print(prior_c1,prior_c2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.008448254441545533 0.017641943098521556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq5tzdmw7pOM"
      },
      "source": [
        "#Y_TRAIN.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK3VsQSb7pOM"
      },
      "source": [
        "#full_train  = np.append(X_TRAIN, Y_TRAIN, axis=0)\n",
        "#full_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA19v3_57pON"
      },
      "source": [
        "c1_index = [index for index, value in enumerate(full_train[5,:]) if value == 1]\n",
        "c2_index = [index for index, value in enumerate(full_train[5,:]) if value == 0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlebVgYh7pON"
      },
      "source": [
        "# Dividing train data as per class\n",
        "\n",
        "#c1_train_data = X_TRAIN[:,c1_index]\n",
        "#c2_train_data = X_TRAIN[:,c2_index]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaxtuDgt7pON"
      },
      "source": [
        "# Calculating mean for all the three class\n",
        "\n",
        "c1_mean = np.mean(c1_train_data, axis=1).reshape(5,1)\n",
        "c2_mean = np.mean(c2_train_data, axis=1).reshape(5,1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5T5rVkiB7pON"
      },
      "source": [
        "#Calculating covarience for all the three classes\n",
        "\n",
        "c1_cov = np.cov(c1_train_data)\n",
        "c2_cov = np.cov(c2_train_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu-8P4-87pON"
      },
      "source": [
        "# Calculating the likelihood by gaussian distribution formula\n",
        "\n",
        "def likelihood(x, mu, cov, k):\n",
        "    numerator = np.exp(-0.5 * np.matmul(np.matmul(np.transpose(x - mu), np.linalg.inv(cov)), (x - mu)))\n",
        "    denominator = np.sqrt(((2 * np.pi) ** k) * np.linalg.det(cov))\n",
        "    return numerator/denominator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9jovo8D7pON"
      },
      "source": [
        "X_TRAIN.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ietgnCo77pON"
      },
      "source": [
        "predictions =[]\n",
        "for i in range(X_TRAIN.shape[1]):\n",
        "    sample = np.array(X_TRAIN[[0,1,2,3,4],i]).reshape(5,1)\n",
        "    C1_posterior = prior_c1 * likelihood(sample, c1_mean, c1_cov, 2)\n",
        "    C2_posterior = prior_c2 * likelihood(sample, c2_mean, c2_cov, 2)\n",
        "      \n",
        "    if   C1_posterior > C2_posterior:\n",
        "             sample_class = 1\n",
        "    else:\n",
        "            sample_class = 2\n",
        "    \n",
        "    predictions.append(sample_class)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC2pX2pY7pOO"
      },
      "source": [
        "#train_result = predictions ==Y_TRAIN\n",
        "train_correct_predict = np.count_nonzero(Y_TRAIN)\n",
        "#correct_predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgssHTjT7pOO"
      },
      "source": [
        "# Training accuracy ( total no of samples correctly predicted / total no.of samples)\n",
        "train_accuracy = train_correct_predict/105\n",
        "print('Training data accuracy is {} '.format(train_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ga72oyR7pOO"
      },
      "source": [
        "# Checking on the Test Data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob_1N8Q97pOO"
      },
      "source": [
        "predictions =[]\n",
        "for i in range(X_TEST.shape[1]):\n",
        "    sample = np.array(X_TEST[[0,1,2,3,4],i]).reshape(5,1)\n",
        "    C1_posterior = prior_c1 * likelihood(sample, c1_mean, c1_cov, 2)\n",
        "    C2_posterior = prior_c2 * likelihood(sample, c2_mean, c2_cov, 2)\n",
        "     \n",
        "    if   C1_posterior > C2_posterior:\n",
        "             sample_class = 1\n",
        "    else:        \n",
        "            sample_class = 2\n",
        "    \n",
        "    predictions.append(sample_class)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7jbzz367pOO"
      },
      "source": [
        "#test_result = test_predictions ==Y_TEST\n",
        "test_correct_predict = np.count_nonzero(Y_TEST)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2ZQw3AF7pOO"
      },
      "source": [
        "test_accuracy = test_correct_predict/45\n",
        "print('Test data accuracy is {} '.format(test_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jRj6_6TiqkM"
      },
      "source": [
        "# ROC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbBJk6VxiRnt",
        "outputId": "55f13e77-1a62-4b39-ad30-55dfd2fa3f62"
      },
      "source": [
        "plt.figure()\n",
        "\n",
        "# Add the models to the list that you want to view on the ROC plot\n",
        "models = [\n",
        "{'label': 'Logistic Regression','model': LogisticRegression(),},\n",
        "{'label': 'RandomForest Classifier','model': RandomForestClassifier(n_estimators = 100)  ,},\n",
        "{'label': 'Naive Bayes','model': MultinomialNB(),},\n",
        "{'label': 'KNN','model': KNeighborsClassifier(n_neighbors=3),},\n",
        "    ]\n",
        "\n",
        "for m in models:\n",
        "    model = m['model'] # select the model\n",
        "    model.fit(x_train, y_train) # train the model\n",
        "    y_pred=model.predict(x_test) # predict the test data\n",
        "# Compute False postive rate, and True positive rate\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y_test, model.predict_proba(x_test)[:,1])\n",
        "# Calculate Area under the curve to display on the plot\n",
        "    auc = metrics.roc_auc_score(y_test,model.predict(x_test))\n",
        "# Now, plot the computed values\n",
        "    plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % (m['label'], auc))\n",
        "# Custom settings for the plot \n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('1-Specificity(False Positive Rate)')\n",
        "plt.ylabel('Sensitivity(True Positive Rate)')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()   # Display"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABov0lEQVR4nO2dZ3hU1dqG7zcFCL0jBFB6h9AFFUGaioAdFRVE5UPBigp2VFRQjkc4YEERPAcQxQKKYEFFioCAhF5EEAy9lyRAyvv9WHuGSTIzmZBJJgnrvq65suta796T2c9e7VmiqlgsFovF4ouwUAdgsVgslryNFQqLxWKx+MUKhcVisVj8YoXCYrFYLH6xQmGxWCwWv1ihsFgsFotfrFBYsoSIbBCRjqGOI68gIs+IyIchynuKiIwMRd7BRkT6isgP53mu/Z/MYaxQ5GNE5G8RSRSRUyKyz3lwFM/JPFW1kaouyMk8XIhIYRF5XUR2Odf5p4g8KSKSG/l7iaejiMR5blPV11T1vhzKT0TkYRFZLyLxIhInIjNFpElO5He+iMgIEZmanTRUdZqqdgsgrwzimJv/kxcqVijyPz1VtTgQAzQHng5tOFlHRCJ87JoJdAauBUoAdwEDgbE5EIOISF77PYwFHgEeBsoCdYFZQI9gZ+TnO8hxQpm3JUBU1X7y6Qf4G+jisf4G8K3H+qXAb8AxYA3Q0WNfWWAysAc4Cszy2HcdEOuc9xvQNH2eQBUgESjrsa85cAiIdNYHAJuc9L8HLvY4VoHBwJ/ADi/X1hk4DVRLt70tkALUdtYXAK8DvwPHgdnpYvJ3DxYArwJLnGupDdzjxHwS2A78n3NsMeeYVOCU86kCjACmOsdc4lxXP2CXcy+e9cgvCvjYuR+bgKeAOB/fbR3nOtv4+f6nABOAb514lwO1PPaPBf4BTgCrgCs89o0APgemOvvvA9oAS517tRcYDxTyOKcR8CNwBNgPPANcDZwFkpx7ssY5thQwyUlnNzASCHf29Xfu+b+dtEY62xY7+8XZd8D5TtcCjTEvCUlOfqeAb9L/DoBwJ66/nHuyinT/Q/ZzHs+aUAdgP9n48tL+QKoC64Cxzno0cBjzNh4GdHXWKzj7vwU+BcoAkcCVzvYWzg+0rfOj6+fkU9hLnj8D93vE8ybwnrN8PbANaABEAM8Bv3kcq85DpywQ5eXaRgG/+rjunZx7gC9wHkSNMQ/zLzj34M7sHizAPNAbOTFGYt7WazkPqyuBBKCFc3xH0j3Y8S4UH2BEoRlwBmjgeU3OPa+KeQD6EopBwM5Mvv8pmAdtGyf+acAMj/13AuWcfUOBfUARj7iTnO8pzIm3JUZYI5xr2QQ86hxfAvPQHwoUcdbbpr8HHnnPAt53vpOKGCF3fWf9gWTgISevKNIKRXfMA7608z00ACp7XPNIP7+DJzG/g3rOuc2AcqH+reb3T8gDsJ9sfHnmB3IK8+akwE9AaWffMOB/6Y7/HvPgr4x5My7jJc13gVfSbdvCOSHx/FHeB/zsLAvm7bWDsz4PuNcjjTDMQ/diZ12Bq/xc24eeD710+5bhvKljHvajPPY1xLxxhvu7Bx7nvpzJPZ4FPOIsdyQwoajqsf934DZneTvQ3WPffenT89j3LLAsk9imAB96rF8LbPZz/FGgmUfcCzNJ/1HgK2f5dmC1j+Pc98BZr4QRyCiPbbcDvzjL/YFd6dLozzmhuArYihGtMC/X7E8otgC9s/vbsp+0n7xWJ2vJOteragnMQ6w+UN7ZfjFwi4gcc32AyzEiUQ04oqpHvaR3MTA03XnVMNUs6fkcaCciVYAOmIfkIo90xnqkcQQjJtEe5//j57oOObF6o7Kz31s6OzElg/L4vwdeYxCRa0RkmYgccY6/lnP3NFD2eSwnAK4OBlXS5efv+g/j+/oDyQsRGSoim0TkuHMtpUh7Lemvva6IzHE6RpwAXvM4vhqmOicQLsZ8B3s97vv7mJKF17w9UdWfMdVeE4D9IjJRREoGmHdW4rQEiBWKAoKq/op52xrjbPoH8zZd2uNTTFVHOfvKikhpL0n9A7ya7ryiqvqJlzyPAT8AtwJ3AJ+o81rnpPN/6dKJUtXfPJPwc0nzgbYiUs1zo4i0wTwMfvbY7HlMdUyVyqFM7kGGGESkMKbqagxQSVVLA3MxApdZvIGwF1Pl5C3u9PwEVBWRVueTkYhcgSlR3YopOZbG1Pd79hhLfz3vApuBOqpaElPX7zr+H0yVnDfSp/MPpkRR3uO+l1TVRn7OSZug6jhVbYmpFqyLqVLK9LxM4rScJ1YoChZvA11FJAbTSNlTRLqLSLiIFHG6d1ZV1b2YqqF3RKSMiESKSAcnjQ+AQSLS1ukJVExEeohICR95TgfuBm5yll28BzwtIo0ARKSUiNwS6IWo6nzMw/ILEWnkXMOlmHr4d1X1T4/D7xSRhiJSFHgZ+FxVU/zdAx/ZFgIKAweBZBG5BvDssrkfKCcipQK9jnR8hrknZUQkGhji60Dn+t4BPnFiLuTEf5uIDA8grxKYdoCDQISIvABk9lZeAtOwfUpE6gMPeOybA1wkIo863ZZLiEhbZ99+4BJXrzHn/+sH4F8iUlJEwkSklohcGUDciEhr5/8vEojHdGpI8cirpp/TPwReEZE6zv9vUxEpF0i+Ft9YoShAqOpB4L/A86r6D9Ab81Z4EPOm9STnvvO7MG/emzGN1486aawE7scU/Y9iGqT7+8n2a0wPnf2qusYjlq+A0cAMpxpjPXBNFi/pJuAX4DtMW8xUTE+ah9Id9z9MaWofpqH1YSeGzO5BGlT1pHPuZ5hrv8O5Ptf+zcAnwHanSsVbdZw/XgbigB2YEtPnmDdvXzzMuSqYY5gqlRuAbwLI63vMy8BWTHXcafxXdQE8gbnmk5gXhk9dO5x70xXoibnPfwKdnN0znb+HReQPZ/lujPBuxNzLzwmsKg2MoH3gnLcTUw3nKilPAho693+Wl3Pfwnx/P2BEbxKmsdySDeRcTYHFkv8QkQWYhtSQjI7ODiLyAKahO6A3bYslVNgShcWSS4hIZRG5zKmKqYfpavpVqOOyWDLDjoi0WHKPQpjePzUwVUkzMO0QFkuexlY9WSwWi8UvturJYrFYLH7Jd1VP5cuX10suuSTUYVgsFku+YtWqVYdUtcL5nJvvhOKSSy5h5cqVoQ7DYrFY8hUisvN8z7VVTxaLxWLxixUKi8VisfjFCoXFYrFY/GKFwmKxWCx+sUJhsVgsFr9YobBYLBaLX3JMKETkIxE5ICLrfewXERknIttEZK2ItMipWCwWi8Vy/uRkiWIKZuJ1X1yDsaeug5k0/d0cjMVisVjyP6rmk5rqfFIgJdn5JEHyWedzBpJOO59E9Ex8trLNsQF3qrpQRC7xc0hv4L/OjGjLRKS0iFR2Jj3Jn6jCiT2QcubcevxBOB5n/uZXVHFPLJZhGbMeyLL7j6+0spOut/NzIl1vy3g/P9vp+rumYOaR/nw/eWQr3fTLnFsOarqh+J/zl0d20vVY9oMqJCeGkRQfQVJ8uMcngrB9Zym553imafgjlCOzo0k7kUqcsy2DUIjIQEypg+rVq+dKcJw5BXtj4cAm86D392WpwrFdsGspnNqfO/EVeJwZOEUCXHbOCWTZ/Scn0vU8318e2Uk3N/MQCPOoeAhq7N7iDTCPLKebPi28b892ut6W8XF+4OmqQvKJRJIOx5N0JJ6kw6fM8uFT5nPkFJqcmia7QlERVDxwkBJ79nOiWCGyQyiFQrxs8/o0VtWJwESAVq1aZS6vgZCSDAc3w7b5RgxO7IZ9a8/tP3MS1LnxYZEQFu4/vWIVocaVUK0NFPaYNbRoeShVFYpXzPiPk6/IyR+tl4eFxXIBoaokHzxI0u7dJO3e4/z1+OzZg549m+ac8HLliIyOpnDLppSIjiYyOprIqlXN38qVCbvzTli/GZ5+muuqb4AHvvaRe+aEUijiSDu5fFVgT47meOogxK2AzXNgy1xIPGq2l6oGkVHQ6AaIKGK2FSkNVVtBpUZQorJ9iFkslvNGVUk5fJikuDjOehODPXvQM2lnxQ0vW9YIQf36FO98FZHR0RTyFIKiRTNmtGEDFC4MUVEwejS8/DI0asSZyTdlK/5QCsXXwBARmQG0BY7naPvEb+Phh+cANSJQpyvU6QZVW0PZGjmWrcViKfioKilHjpAUF0fS7t2OGKQVhAxCUKaMEYK6dSneqROR0VXOiUGVKt6FwBfx8fDKK/Cvf0HfvjBlCtSuHbTryzGhEJFPgI5AeRGJA14EIgFU9T1gLnAtsA1IAO7JkUB2LoUFr8OOXyG6FXR4Amp3hfB8Z5xrsVhChKqScvSoWwi8iYGePp3mnPDSpY0Q1K5N8SuvNCWB6CrnhKBYseAE9+23MHgw7NwJAwaYkkSQycleT7dnsl+BwTmSeWoqHP8Hlr8Py96BomWhy0vQ6h4oUipHsrRYLPkXVSXl2DGS4naTtNu7GGhiYppzwkuVMkJQsybFr7jCEYJzn/DiQRICf7zzjhGJhg1h4UK44oocyabgvVYf2wXTb4MDG8x6s9uhx1tQKAvFOIvFUqBwC8HuPWlKBaZ9YDdnd+9BExLSnBNWqhSR0VUoXKMGxS+73Gks9hSC4qG5mORkOHgQKleGW2+FxER46CEolL2eTf4oWEJxaBtM6QHJiXD1aChdHep2z7zHksViydeoKqnHj58rAcTtztBzKDW9EJQoYXoJXXwxxdq3z1giKFHCR24h5Pff4f/+DyIiYNkyKF8ehg7N8WwLjlCc3AffPm56Mg38xfRWslgsBQJVJfXEiXPVQd6EID7t6OOw4sWNEFSvTtF2l1IovRCULBmiqzkPjh2DZ56B994zJYmxY9OOb8lhCo5QzB5sBrxd8YQVCYslH5LiCIG7fSCdGKSeOpXm+LBixYwQVK1K0bZtz/UacoQgrGRJpCB0a1+3Drp2NdVNDz9surzmssgVDKFIOGIGznV4EjoOC3U0FovFCyknT2YoBXiKQerJk2mODyta1D2IrGjr1u5eQy4xCCtVqmAIgS+SkiAyEurWhU6d4MknoUVovFMLhlAse8f8rdMttHFYLBcwKadOnROBOFevoTh399HUEyfSHC9Fi1IougqR0VUp2rJlujaCKoSXLl2whcAXZ86YLq5Tp8Iff0Dx4vDJJyENqWAIxcHNUL6usc+wWCw5Qsqp+IzWEh5ikHr8eJrjJSrKXQIo2rx5WiGoGn3hCoE/fv4ZHngAtm6FPn2MaISqd5UHBUMoju2C4pVCHYXFkq9JjY/3OqLY1Z00Jb0QFCnifvsvGhOTsddQmTJWCAIlMREGDjSliJo14bvvoHv3UEflJv8LxdqZsHcNdHou1JFYLHma1Ph4kvbsySgGLiE4dizN8VK4sPuhX6Rpk3M+Qy4hKFvWCkGwKFIEDh2C554zvZuiokIdURryt1AkHIFZg4xf0+WPhToaiyWkpCYkkLRnj3evobg4Uo4eTXO8FCp0TggaNyayanSaLqTh5cpZIchJ1q41DdSTJkHVqsaKIxe7vGaF/C0U23+B1GS4/HHr3WQp8KQmJrqFIK3FhBltnHLkSJrjpVAhIquYNoIiXbu62wYKeQpBHn0wFWji42HECPj3v6FMGfjzTyMUefi7yN9P1z/nQ1RZM/raYsnnpJ4+nUEIPMUg5dChNMdLZOQ5Iejc2WM+ArMtonx5KwR5ja+/NnYbu3bB/ffDqFFQtmyoo8qU/C0U+9dBdAtr0WHJF6SeOeN9Uprduzm7ZzcpB9MKAZGRRFapTKHoaIp06ujRPmDaCiIqWCHId8yaZQbLLV4Ml10W6mgCJv8KxYm9sH8j1LGlCUveIPXsWd8zlO3eTfLBdPOmR0YSWbkykdFVKH7llWktJqpWNSWCcPsSlK9JSoJx48yAuRYtjPVGkSJmIF0+Iv8KxbqZoCkQc0eoI7FcIKSePUuyt15DLiE4cCDtCRERjhBEU6zDFWnsJSKrViWiQgUrBAWZZcuMgd/atTBsmBGKvGg0GAD5Vyi2zYdKjaFcrVBHYikg6NmzJO3b5zFdZVoxSD5wANRjyvbw8HNCcPnlGbyGIipVskJwIXL0KDz9NEycCNHR8NVX0Lt3qKPKFvlTKFJTYf96qHt1qCOx5CM0KcktBN66kCbv359RCC66yAhBGhvqKhRyCUFE/vwJWXKQiRPhww/hscdM76Z8WorwJH/+l+9bCwmHoWbHUEdiyUNoUhJJ+/f7mKVsjxGC1NRzJ4SFnROCSy/NMLI48iIrBJYA2bLFuLtefjk8+ihccw00bRrqqIJG/vwVHN1h/lZsGNo4LLmKJieTtG9/mtHEnr2GkvdlFIKISpUoFB1NsTZtMngNRVaqhOSzRkVLHuP0aXj9ddPNtX59iI2FwoULlEhAfhWKY/+Yv6WiQxuHJahocjLJ+/enGUSWpufQ/v2QknLuBBEiKlUismo0xdw21B6fSpWQHJwe0nKB8+OP8OCDsG0b3HEH/OtfkAdHsk9fvosTiUnZSsOvUIhIEeA64AqgCpAIrAe+VdUN2co5O5zaDxFREFUmZCFYso6mpJC8f7/vWcr27csoBBUrEhkdTVSrlpSMTmsxEXnRRVYILKFh4ULo1g3q1DGC0aVLqCPyyezY3dlOw6dQiMgIoCewAFgOHACKAHWBUY6IDFXVtdmOIqskHIZi5XM9W4t/NCWF5AMHPCaliUvbhXTfPjMxvAduIWjRgpLpew1VrkyYFQJLXiElBTZuhCZN4IorjEfTHXeYcRF5nJJR2ati9VeiWKGqI3zse0tEKgLVs5X7+ZJwGIqWC0nWFzKakkLywYPpZijzEIO9ezMKQYUKRgiaNaNkjx5pupBGVKlihcCSP1i9GgYNgk2bjDdTpUowYECoo8o1fAqFqn7ruS4ixVQ13mP/AUwpI/dJOAJF874/Sn5DU1N9CIHTZrB3rxlp6kF4hfIUqhJNVJMmlLzmmjRdSCOrVCGscOEQXY3FEgROnoQXXzQjqsuXh3ffhYoVQx1VljgavpCEsK3ZSiPTxmwRaQ98CBQHqotIM+D/VPXBbOWcHRKPQunQFGbyM0YIDnmxlzADzJL37EXTC0H58kRGVyGqcSNKdu/m9hmKjI4mskplwvJBsdtiOS+OHzfVTP/8Y0ZYv/66cXvNZxwP/z3baQTS6+nfQHfgawBVXSMiHbKdc3awVU9e0dRUkg8dwnMQWZqupHv2ZBSCcuWM+2jDhhTq2tVxH3WEoHJlwvLYBCoWS45z4oQx7itVysw617kztGsX6qiyRdHUuph+SOdHQN1jVfWfdBOYpPg6Nlc4cwKKlAppCKFAVUlxhOCspxh4CsHZs2nOCS9blsjoaAo3aEDxLsaK2j1TWZUqVggsFhdJSWaOiJEjYcEC4830nJ05EwITin+c6icVkULAw8CmnA3LD6qgqRBZ8Ko8VJWUw4fTjSj2GFOwZw965kyac8LLlDFCUK8exa+6ythLeApB0aIhuhqLJR+xZIlprF6/Hq6/HipUCHVEeYpAhGIQMBaIBuKAH4DQtU/gjLyNyH9CoaqkHDniZVIaZ0zBnj3o6dNpzgkvXdoIQZ06FO/Y0d1QXKhqVSMExYqF6GoslgLCQw/B+PFQrRrMng29eoU6ojxHIEJRT1X7em4QkcuAJTkTUia4TNvyoFCoKilHj/qYocyUDDQxMc054aVKGSGoVYviHTqktZioEk14cSsEFkvQUT03ivqii+CJJ0zvpuLFQxtXHiUQofgP0CKAbbmEIxSS+zN7qSopx45lHFG8ezdJe3ZzdvceNCEhzTlhpUoRGV2FwjVqUPyyy8+JgGveYvuPabHkLps3m2qmxx4z9t/PPhvqiPI8/kZmtwPaAxVE5HGPXSWB0Jnsu1ygc0Ao3ELgY4aypN27SU0vBCVKmJ5CF1+czoraEYICYDFssRQIEhPhtddg9GgoVsysWwLCX4miEGbsRATg+bQ7AdwcSOIicjWmfSMc+FBVR6XbXwqYihnhHQGMUdXJfhNNdUb+nofPk6qSevy4zxnKknbvJjU+Ps05YcWLGyGoXp2i7S5N6zUUHU14yZJZjsNiseQyP/1kxkL89RfcdReMGZPvBs6dDy5DwByz8FDVX4FfRWSKqu7MasIiEg5MALpiGsFXiMjXqrrR47DBwEZV7SkiFYAtIjJNVc96SdIVmPkb6b03T8qJEz5nKEvavZvUU6fSHB9WrJgRgqpVKdq2bYZZysJKlkTyoCOkxWLJAnFxEBFhBOOqq0IdTa7hMgQsXyx7DgmBtFEkiMibQCOMKSAAqprZ3W4DbFPV7QAiMgPoDXgKhQIlxDyJiwNHgOT0CaXF9HqKX7+dM0s/TjueYPduUk+eTHN0WNGi7jmKi7qtqM+JQVipUlYILJaCRkoKvPceFCoE998Pd98Nt91m5oq4wCgZFUnFkjkvFNOATzF244OAfsDBAM6LBv7xWI8D2qY7ZjxmxPceTPVWH1VNTXcMIjIQGAhQr2o5kuLD2PXMWLOvaFEKRVchMroqRVu2TNdGUIXw0qWtEFgsFxJ//GGqmVauhJtuMkIhckGKRLAIRCjKqeokEXnEozrq1wDO8/Z01nTr3YFY4CqgFvCjiCxS1RNpTlKdCEwEaFWvqiY7Yw2i3/43Jbp3t0JgsViM9cbzz5sxERUqwCefQJ8+oY6qQBBI1yGXOdBeEekhIs2BqgGcFwdU81iviik5eHIP8KUatgE7gPp+U9UUkpNN+0RklSpWJCwWi2HNGiMSgwaZLrC33ZYnZ5zLLaYv38XyHUeCklYgQjHS6Z00FHgC4yT7aADnrQDqiEgNx/rjNhxjQQ92AZ0BRKQSUA/Y7jfVlCRSMD2Nwstaq3GL5YJmxw746COzfMUVZlrSCROgdOmQhpUXmB27m8jSy7NtMQ4BCIWqzlHV46q6XlU7qWpLTKNzZuclA0OA7zHeUJ+p6gYRGSQig5zDXgHai8g64CdgmKoe8p9wKinJpk09Ih9a/losliBw9qyx/W7YEIYOhaNHzfYaNUIbVx6jbCUzY/W1Na/NVjr+BtyFA7diGqW/U9X1InId8AwQBTTPLHFVnQvMTbftPY/lPUC3rAadnJCKFCqEWMM7i+XCY9EiU720cSPceKOZVMi+NPqkVaVW3FL3lmyl4a8xexKmjeF3YJyI7ATaAcNVdVa2cs0OqSmknAknvKzt1mqxXHAcPAjdupmpSL/5Bq67LtQRXRD4E4pWQFNVTRWRIsAhoLaq7sud0HyQmkzK2XDCy9o3CIvlgkAV5s+Hrl1Nb6Y5c+DSS40Nh8UrrobsSvWCk56/NoqzrjENqnoa2BpykQBITSb5NESUtkJhsRR4NmyAK680pYgFC8y2zp2tSGRCsEZku/AnFPVFZK3zWeexvk5E1gYl9/NBU0lJSLY9niyWgkxCAjzzDMTEGLH48EPoENoZmPMLrtJE3drr2ZW4Lihp+qt6ahCUHIKNppJy6qyterJYCiqq0KkT/P479OsHb75pZ5zLAq7SRGTJNZCY/R5P4N8UMMtGgLlF6ukk2zXWYilo7N1rHF3Dw01polQp6Ngx1FHlS1yliWD0eILABtzlKTTV9HQKL2OrniyWAkFKCowbB/XqwTvvmG29e1uRyAbHw38HglOagHwoFC63KFv1ZLEUAFauhDZt4JFHoH17uDY4D7YLFU/bjmCVJiBAoRCRKBEJUker7OEqUUTYxmyLJX/zxhtGJPbuhU8/hXnzoFatUEeVrwl2bycXmQqFiPTEOLx+56zHiEh6z6ZcwzVvUbhto7BY8h+qkOT4jLZpA4MHw6ZNcOutF7SBXzDIid5OLgIpUYzATEJ0DEBVY4FLghpFFlB12ihsicJiyV/89RdcfTUMH27WO3aE//zHNFpbssX05bt45isjDpEl1wDBa5+AwIQiWVWPBy3H7JIKhImdq9piyS+cOQMjR0LjxrB0qa1eygFcVU6v3dCEiiULB7V9AgKbuGi9iNwBhItIHeBh4LegRZBFVCG8RHEkPDxUIVgslkBZtQruvNPMD3HLLfD221ClSqijKjBMX76L2bG72bj3BG1rlCWyzHJWbl5Jq0qtgppPICWKhzDzZZ8BpgPHCWw+ihxBU4Xw0rY0YbHkC4oXN20Pc+fCZ59ZkQgyLpFoWLkkvWOimbvdmHUHs9oJAhOKeqr6rKq2dj7POd5PoUEhopQVCoslT5KaCpMmwX33mfV69WD9erjmmtDGVQBxNV43rFyST/+vnSlN7F8Z9GonCEwo3hKRzSLyiog0Cmru54GmCuGlSoQ6DIvFkp71640f0333wZ9/Qny82R6W/4Zr5Qdc7RK9Y6KZuXUmLy99GQh+aQICm+GuE9AROAhMdEwBnwt6JAGiiq16sljyEvHxMGwYNG9u2iImTzZOr9bhNcdwlSZc7RIukXih3QtBL01AgAPuVHWfqo4DBmHGVLwQ9EgCRSG8tC1RWCx5htOnjTjcfTds2QL9+9sxETmIZ1fY6peszXGRgAB6PYlIA6APcDNwGJgBDM2RaAJBxbZRWCyhJi7O+DO9/jqUK2dKEnZsU47h6t0EuC06bukUx3f7xgM5KxIQWPfYycAnQDdnjuuQY9soLJYQkZxsBsm98IIx8+vTB1q2tCIRZDyFAc6JQ9saZalbez2RJdfw3T5TqshpkYAAhEJVL83RCM4D20ZhsYSA5cvh//4P1qwx5n3jx0ONGqGOKl+TXhBceAqD62/vmGinPWIqJBrTv2trXpvjIgF+hEJEPlPVW53Z7dRzF6Cq2jTHo/NBeIniocraYrkwSU2Fe+6B48fh88/hxhttO8R54q0aySUILlzCcEfb6gDM3DqTudsnsnLzSiB3ShGe+CtRPOL8vS43AskKEmFHZVssOY6qEYWrr4YSJeDLLyE62ixbsoxLIDzFIb0gpMcIxFxW7jcCkZulCE/8zXC311l8UFWHee4TkdHAsIxn5RK2X7bFkrP8+adxdv3xRxgzBoYOhfr1Qx1VvsFblZKnQPgTB8g7AuEikMbsrmQUhWu8bMs1xBZ5LZac4cwZGD0aXnsNChc27RCDBoU6qnxBZlVK3gTCJQjpySsC4cJfG8UDwINATRFZ67GrBLAkpwPzS5gVCoslRxg82Fhw3HYbvPUWVK4c6ojyBZ5jG7xVKbkE4cej8ON3587zFARP8opAuBBV9b5DpBRQBngdGO6x66SqHsmF2LzSuEiUrlr4JYXbWO8YiyUoHDhgGqsvushUOW3fDt27hzqqfEH6dofXbmhCZJnlGUoJvgQByDVBEJFVqnpetrL+hKKkqp4QEa8dpEMlFo2LROkfi2dRqJX9R7ZYskVqKnz4obHf6NbNTEdqCZgnv3+fOX99C0DJqEjKFytMxZKFfYpCqEsI2REKf20U0zE9nlZhusd61vcoUPN8MgwKtjHbYskea9eatoelS81Mcy+9FOqI8jzTl+9iyrpPOB7+OwAJYVuJKAbVo8xkQS7yWrVRMPDX6+k652+eG1FjG7Mtlmzw+eemDaJMGfjvf83EQvY35Sa9ILg4kZhERLEdABRNrUvR1Lp0qNKNN7v/XyjCzFUC8Xq6DIhV1XgRuRNoAbytqrtyPDpf2MZsiyXrnDgBJUuaEsTgwfDii9Z6w2Hm1plMif2KQ/FnMgiCi5JRkZSPakL/mBsKVGkhEALpHvsu0ExEmgFPAZOA/wFX5mRgfhFb9WSxBMyuXfDQQ7BnDyxbBuXLw9ixoY4qV/Dssno0fGGGUoKLhLCtACQn1rigBcEXgQhFsqqqiPQGxqrqJBHpF0jiInI1MBYIBz5U1VFejukIvA1EAodUNXMBsiUKiyVzkpKMILz4olkfMcKMti6A+PJM+uPoPCJKxlIyKtItBp6lBBdFU+tSKqUN/Vve7ncg3IVKIEJxUkSeBu4CrhCRcMxD3S/OcRMwA/bigBUi8rWqbvQ4pjTwDnC1qu4SkYqBBG3bKCyWTNi5E3r1Mo3WPXsax9eLLw51VNnGmyAcDV/IP2fN0K6SUWkfTUUqG3FoWKkVUPAamXOLQISiD3AHMEBV94lIdeDNAM5rA2xT1e0AIjID6A1s9DjmDuBLV3uHqh4IKGrb68li8Y6qaZi+6CKoVAm++gp69843jdW+SgYulu84QmTp5ZSttMG9LSFsKxGRGXsfGaw4BINAbMb3icg0oLWIXAf8rqr/DSDtaOAfj/U4oG26Y+oCkSKyADPie2wgaYsVCoslLaowbRq8/baZhrR4cfjhh1BHlSVco5vTC4EnleoZYUjAc5yCFYOcJpBeT7diShALMGMp/iMiT6rq55md6mVb+grSCKAl0BmIApaKyDJV3ZouhoHAQIBGhYvYNgqLxZMtW+CBB+CXX6BtWzh82AhFPuHJ799n4Z4fOJGYRFR1iCi2I50QpMcKQ24TSNXTs0BrV7WQiFQA5gOZCUUcUM1jvSqQfoa8OEwDdjwQLyILgWZAGqFQ1YnARDAjs/NLMdpiyVGSk+GVV2DUKIiKgnffhYED81zVbHrjuwMnznAo/ox7PSFsK4RByai6zuhmKwR5jUCEIixd28FhIJD/xBVAHRGpAewGbsO0SXgyGxgvIhFAIUzV1L8zS9hWPVksQHg4LFoEN99sDPwqVcr1EDxFIL0AuEjf2+hEYhJwruH5Qhq4ll8JRCi+E5HvMfNmg2nczuiLmw5VTRaRIcD3mO6xH6nqBhEZ5Ox/T1U3ich3wFogFdOFdn2mEdkSheVCZd8+eOYZY7lRrRrMnQtFigQ9m0AEANKKQHoBcOHqelompYPZEE6m8zFY8hY+TQHTHCRyI3A5pt1hoap+ldOB+aJxkShdu2UhYRe3DlUIFkvuk5ICEyfC009DYiJMnQq3BK9qJn31kMvYzp8AuPAUASsAeZccMQUUkTrAGKAWsA54QlV991vLRWzVk+WCYvVqY+D3++/QuTO88w7UzThoLBBc3U/Tj1JOXz1UlLoc2d+Ik8faBjQjm6Vg46/q6SPgv8BCoCfwH+DG3AgqU6yFh+VCYvx4+Ptv0/319tuZ+efnzP3udZ+H+6sqcpUOIoqk9TLKUD0EXFIGeneyAmHxPx9FrKrGeKz/oaotciswXzQuEqXrti9FqsSEOhSLJUv4mvYyA6q0+G0XhyoVZ1ftchQ9aR76CSXMYDLPaiFvZFZV5Jo3wfYsurDIqfkoiohIc86Nh4jyXFfVP84nw2BgLTwseZHMhMDfLGcuyu0/yZ3vLCfm9ziWdK7Fh09cQUKJwqaUsPcEYIzrkk/E0LCMj1kebWOxJcj4K1H84uc8VdWrciYk/zSOKqLrty+Hys1Ckb3lAiPgUgCBCYHPt/ikJNPF9aWXICyMPwY8ypv1u5Mabt7lXFNttq1hbMGtEFiySo5MhZpXsUJhySpZedinJ5CHvyeZVef48jLq/svnDPj0LX5v1oEpfR5l7vFCwDlhACsOluxhhcJiwbcgZPVhn55A6/IzM7SDtCWD4qeOU+HwXnZcXJ+IpLM02voHaxpd6j7WCoMlmORUG4XFkqfx1fc/vSAEYw7jrIqAL9rWKEvvZlW4Y8sCeO4JKFECtm6FiAhCOReYxeIPKxSWPEtWG4eDPam9pzgELAKZlQI2bYIH+sGvv0K7dvDee45IWCx5l0DcYwXoC9RU1Zed+SguUlXvcwpaLH4IZuNwsIUBfItDUAadrVkDrVsbZ9eJE+Hee/OcgZ/F4o1M2yhE5F2MD9NVqtpARMoAP6hqSDw0bBtF3iKrDcXBbhwOJq75ECDIvYvi4qBqVTNnxKhRRiAqBjSZo8USNHK6jaKtqrYQkdUAqnpURAqdT2aWgoNLILL64M+JUoAvAmlX8MRVgnjthibBaUTeswcee8wY923eDNHRxqvJYslnBCIUSc781wru+ShSczQqS46TnS6jkLZkEKoRvoFMmwn+2xU8CZqnUUqKmRvi2WfhzBnzt3z57KVpsYSQQIRiHPAVUFFEXgVuBp7L0agsQSfQHkKBEiqByEoDc0jM7E6fhg4dYMUK6NrVGPjVrp17+VssOUCgNuP1MdOVCvCTqm7K6cB8YdsoMsdbacGbMOQnrx+XQOTZEcpJSRDpeCsNGwbNm0OfPnbuFEueIUcH3Dm9nDKgqrvOJ8PsciEKRbAajPOjMLjwFIg8Iw5gGqi/+AKGDoWvvoIWIffNtFi8ktON2d9i2icEKALUALYAjc4nQ0vWmLl1Ji8vfRnImw3G2cFfG0P6kkOeEwiA7dthyBCYN8+UIGxXV0sBJVOhUNUmnusi0gKwk9sGmczsJ15o90Kef/B7Iyti4EmeFAZP3nrLNFJHRMDbb8PgwXbgnKXAkuX/bFX9Q0TsPKTZJDftJ3ILb6KQr8XAH6dOwbXXwtixZoyExVKACaSN4nGP1TCgBVBOVbvnZGC+yO9tFP7GH+QXQXDhrx3Bk3wrBp4cOgRPPgk33AC9ekFqqq1qsuQrcrqNooTHcjKmzeKL88nsQid9e0NeFYZAB6rli3aE7JKaClOmGJE4cQKaODWxViQsFxB+hcIZaFdcVZ/MpXgKJOlLEXmlvcGXIAQ6UK1ACoMnGzfCoEGwaBFcfrkx8Gtk+3BYLjz8CoWqpjiN15Ys4K/9IS+NYvYlCAVeAAJl5UrYsAEmTYL+/W0pwnLB4m8q1AhVTRaRfwF1gJlAvGu/qn6ZOyGmJS+3UeSl9ocLqv0gmMydC4cPw113mTESR49C2cAsQCyWvExOtVH8jmm4LgscBjznyFYgJEKRFwhkJrWcEgbbfpBDxMXBo4+awXNt2sCdd5pR1VYkLBa/QiEAqnpPLsWSZ8kLXVl9WVj4wgpDgCQnw4QJ8NxzZvnVV+GJJ6z1hsXigT+hqJCua2waVPWtHIgnz+FtZHSwBCErNth51sIiv7NqlSlJXH21EYyaNUMdkcWS5/AnFOFAcZySxYVGTvVUyur0mi6sQASR48fhp5/gxhuhbVtYvtzMPGdLERaLV/wJxV5VfTnXIskjeGuQDmbpIejTa1oCRxU++8yUIA4fhr//hipVTJuExWLxSaZtFBcKOSEQvkoPVhxCwF9/GT+m77+Hli3hm2+MSFgslkzxJxS9MjtZRIqr6qkgxpNr5MRYB39dUq1AhJCTJ404pKbCuHHw4IMQHh7qqCyWfIM/oZgiIrHAbGCVqsYDiEhNoBNwK/AB8HlOBxlsgtFAHcgANisOIWbtWmjaFEqUMIPmLr3UzFttsViyhE+hUNXOInItxlL8MhEpg/F62oLxe+qnqvtyJ8zg4ipJZKWBOpABbFYY8ggHD5ourv/9L3z7rXF5vemmUEdlseRbMrPwmAsEPrVaOkTkamAspgfVh6o6ysdxrYFlQB9VzdESysytM1m5fyWtKrXKIBL5fiKdC53UVPjoI3jqKWMD/swz0LFjqKOyWPI9mbrHisjnwEfAd6qaGmjCjqHgBKArEAesEJGvVXWjl+NGA99nJfDzwbPK6dqa17q3BzKYzQpDPuCmm2DWLOjQAd59Fxo2DHVEFkuBIBCb8feAe4D/iMhMYIqqbg7gvDbANlXdDiAiM4DewMZ0xz2EsS0PfDIkOb+GyPRVTt4EwopBPiM+HgoXNrPL3X47XH893H23HRNhsQSRQKZCnQ/MF5FSwO3AjyLyD6Yhe6qqJvk4NRr4x2M9DmjreYCIRAM3YHykfAqFiAwEBgI0KlIYJOsunp5VTklH29Ln/aVWIPI733xj5qweOhQefhhuvTXUEVksBZKApkIVkXLAncBdwGpgGnA50A/o6Os0L9vSW9W+DQxz7Mx95q+qE4GJYNxjz0coXKWJ8nIpz3y1DrACkW/55x945BH46iszP0TLlqGOyGIp0ATSRvElUB/4H9BTVfc6uz4VkZV+To0DqnmsVwX2pDumFTDDEYnywLUikqyqs/wGFZb1qqcDJ85QNLUuM38x8xu/dkMTKxD5kalTzWRCqakwahQ89hgUKhTqqCyWAk0gJYoPnd5PbkSksKqeycTbfAVQR0RqALuB24A7PA9Q1RoeaU4B5mQqEpClqqeZW2cyJfYrdp76k5TTlW0pIr+iatodqlY1PZn+8x+oUSPT0ywWS/YJRChGkrGL7FLMXBU+cSY9GoLpzRQOfKSqG0RkkLP/vfOI15CFhsq52+cSl7CNlNOVua5WD97s3u68s7WEgGPH4OmnoVgxGDPGiITt8mqx5Co+hUJELsI0SEeJSHPOtTmUBIoGkri3cRi+BEJV+weSpgku86onl0XHuoObOJtwEU3Dn7YikZ9QhU8+gccfNwPoHnvsXKnCYrHkKv5KFN2B/pi2Bc+5J04Cz+RgTJmTSdWT53iJ5PgaJJ+IoXcna92Qb9ixAwYOhPnzjf33vHnQvHmoo7JYLlj8WXh8DHwsIjep6he5GFPm+GnMnr58F2M3zoAwOL33BlqUuYbenWybRL4iKcn4NE2YAP/3f9bAz2IJMf6qnu5U1anAJd5mugvpDHd+ShRT1n1CQuRWiqbW5elO91mByC/89JPxZXrrLahbF3buhCJFQh2VxWIB/NXhFHP+FgdKePmEDi9CMX35Lrp9OJq9kVMBeOKy26xI5Af274c774QuXeDrr82EQmBFwmLJQ/irenrfWXxHVQ/mUjyB4aUxe3bsbvbqbxAJV180JCjTllpykNRU+OADGD7c2HA8/7zp3RQVFerILBZLOgLpHvubiOwAPgW+VNWjORxT5qTr+TJ9+S6W7zhCpXoRNKzUije7/1+IArMEzPHj8NxzEBNjDPzq1w91RBaLxQeZjlxT1TrAc0AjYJWIzBGRO3M8sgCZvnwXz3y1jsjSy0kI2xrqcCz+OHXKtEGkpECZMrB8Ofz8sxUJiyWPE9AQZ1X9XVUfxzjCHgE+ztGosoBr/oi6NbcBae3DLXmI2bON7ffQofDrr2ZbzZp2XITFkg/IVChEpKSI9BORecBvwF6MYIQcV5VT3drr2ZW4zutkRJYQs3Mn9O5t7L9Ll4YlS+Cqq0IdlcViyQKBtFGsAWYBL6vq0pwNJ2u4ShORJddAoi1NZEZSUhJxcXGcPn069zLduxcefNA0VJcsaUoQmzblXv4WywVGkSJFqFq1KpGRkUFLMxChqKmq6e3B8wy2NBE4cXFxlChRgksuuQR/tu7Z5tQp03spPBwuvtj8LVw45/KzWCwAqCqHDx8mLi6OGkE0zfQ34O5tVX0U+FpEMgiFqvYKWhTZ4Hj474AtTQTC6dOnc1YkkpNh927jzVS5MkRHQ9GAbMEsFksQEBHKlSvHwYPBHdHgr0TxP+fvmKDmGCSmL9/FH0fnUaTyVluayAI5IhKqcOSImVAoORkqVYKLLgp+PhaLJVNy4jfub8DdKmcxRlXHpgvkEeDXoEeTBWbH7iaiZCxgSxMhZ/du2LfPWIHXrWtLERZLASOQ7rH9vGzrH+Q4zouSUZG2NBEqUlONeR9A+fJQvboZD5GJSBQvXjzbWa9cuZKHH37Y5/6///6b6dOnB3x8ejp27Ei9evVo1qwZrVu3JjY2NjvhBpWvv/6aUaNGBSWt8PBwYmJiaNy4MT179uTYsWPufRs2bOCqq66ibt261KlTh1deeQXPpsp58+bRqlUrGjRoQP369XniiSe85jFr1ixefvnloMSbExw5coSuXbtSp04dunbtytGj3scTX3LJJTRp0oSYmBhatTo3X9uTTz5J/fr1adq0KTfccIP7Hq5bt47+/fvnwhXkEqrq9QPcDnwDHAW+9vj8Asz3dV5OfxoVKawaf1i7fjBKG09prP3n9VdLYGzcuDE4CR0/rrp2reqff2b51GLFigUnBj/88ssv2qNHj/M+/8orr9QVK1aoqupHH32kXbp0CUpcycnJQUknWHh+F3fffbeOHDlSVVUTEhK0Zs2a+v3336uqanx8vF599dU6fvx4VVVdt26d1qxZUzdt2qSqqklJSTphwgSvebRr104PHjwYcExJSUnndS3ny5NPPqmvv/66qqq+/vrr+tRTT3k97uKLL/Z6Hd9//7075qeeeirN+Z07d9adO3fmQNSZ4+23DqzU83zu+mujcI2ZKA/8y2P7SWBt8CUra9hG7Ozx0jcb2LjnRNZOUoUzZ0xJIizMGPf9dK7RrGGVkrzYs1GWY4mNjWXQoEEkJCRQq1YtPvroI8qUKcOKFSu49957KVasGJdffjnz5s1j/fr1LFiwgDFjxjBnzhx+/fVXHnnkEcDUzS5cuJDhw4ezadMmYmJi6NevH82bN3cff+rUKR566CFWrlyJiPDiiy9y0003+YytXbt2vPnmmwDEx8fz0EMPsW7dOpKTkxkxYgS9e/cmISGB/v37s3nzZho0aMDff//NhAkTaNWqFcWLF+fxxx/n+++/51//+hd///0348aN4+zZs7Rt25Z33nkHgHvvvdcd04ABA3jssccYN24c7733HhERETRs2JAZM2YwZcoUVq5cyfjx49m5cycDBgzg4MGDVKhQgcmTJ1O9enX69+9PyZIlWblyJfv27eONN97g5ptv9vsdtGvXjrVrzc96+vTpXHbZZXTr1g2AokWLMn78eDp27MjgwYN54403ePbZZ6nvjKiPiIjgwQcfzJDm1q1bKVy4MOXLlwfgm2++YeTIkZw9e5Zy5coxbdo0KlWqxIgRI9izZw9///035cuXZ+zYsQwaNIhdu3YB8Pbbb3PZZZfx+++/8+ijj5KYmEhUVBSTJ0+mXr16Af+feWP27NksWLAAgH79+tGxY0dGjx4d8PmuewRw6aWX8vnnn7vXe/bsyYwZM3jqqaeyFWNewGfVk6ruVNUFqtpOVX/1+Pyhqsm5GaQviqbWtdVOuUVKijHvS04yXV2LFQvaPBF33303o0ePZu3atTRp0oSXXnoJgHvuuYf33nuPpUuXEu4jrzFjxjBhwgRiY2NZtGgRUVFRjBo1iiuuuILY2Fgee+yxNMe/8sorlCpVinXr1rF27VquymTw33fffcf1118PwKuvvspVV13FihUr+OWXX3jyySeJj4/nnXfeoUyZMqxdu5bnn3+eVatWuc+Pj4+ncePGLF++nHLlyvHpp5+yZMkSYmNjCQ8PZ9q0acTGxrJ7927Wr1/PunXruOeeewAYNWoUq1evZu3atbz3XsaJIYcMGcLdd9/N2rVr6du3b5rqtb1797J48WLmzJnD8OHD/V5jSkoKP/30E716mY6MGzZsoGXLlmmOqVWrFqdOneLEiROsX78+w35vLFmyhBYtzs2YfPnll7Ns2TJWr17NbbfdxhtvvOHet2rVKmbPns306dN55JFHeOyxx1ixYgVffPEF9913HwD169dn4cKFrF69mpdffplnnsk4f9rJkyeJiYnx+tm4cWOG4/fv30/lypUBqFy5MgcOHPB6LSJCt27daNmyJRMnTvR6zEcffcQ111zjXm/VqhWLFi3K9D7lB/x1j12sqpeLyEnAs3usAKqqJXM8OkuOEfCbf2qqKT0kJ5tR1tHRQbUAP378OMeOHePKK68EzFvdLbfcwrFjxzh58iTt27cH4I477mDOnDkZzr/ssst4/PHH6du3LzfeeCNVq1b1m9/8+fOZMWOGe71MmTJej+vbty/x8fGkpKTwxx9/APDDDz/w9ddfM2aM6Qh4+vRpdu3axeLFi92lmsaNG9O0aVN3OuHh4e4Sy08//cSqVato3bo1AImJiVSsWJGePXuyfft2HnroIXr06OF+S23atCl9+/bl+uuvd4uVJ0uXLuXLL78E4K677krz5nr99dcTFhZGw4YN2b9/v9drTExMJCYmhr///puWLVvStWtXwFRH++o5k5UeNXv37qVChQru9bi4OPr06cPevXs5e/Zsmn7+vXr1IspxDp4/f36ah/qJEyc4efIkx48fp1+/fvz555+ICEmuNjIPSpQokSNtSkuWLKFKlSocOHCArl27Ur9+fTp06ODe/+qrrxIREUHfvn3d2ypWrMiePXuCHkso8FeiuNz5W0JVS3p8SoRaJD5ftZsTiRn/SSxBJCXFdHfdssVUOUVEQK1auTZPhAY4xnP48OF8+OGHJCYmcumll7J58+ZM0w3kYTdt2jR27NjBHXfcweDBg93nfvHFF8TGxhIbG8uuXbto0KCB31iLFCniLg2pKv369XOfv2XLFkaMGEGZMmVYs2YNHTt2ZMKECe436G+//ZbBgwezatUqWrZsSXKy/4K853UV9hjg6Cu+qKgoYmNj2blzJ2fPnmXChAkANGrUiJUrV6Y5dvv27RQvXpwSJUrQqFGjNKUmX0RFRaVxAXjooYcYMmQI69at4/3330+zr1ixYu7l1NRUli5d6r5Pu3fvpkSJEjz//PN06tSJ9evX880333h1GMhqiaJSpUrs3bsXMMJWsWJFr9dSpUoVwDz8b7jhBn7//Xf3vo8//pg5c+Ywbdq0NN/B6dOn3eKX3wnE66mWiBR2ljuKyMMiUjrHI/PD3PXmiy1fzI72DTqqcPQorF9vJhUqWtSUKnKIUqVKUaZMGXcR/X//+x9XXnklZcqUoUSJEixbtgwgTSnAk7/++osmTZowbNgwWrVqxebNmylRogQnT570eny3bt0YP368e91XLxeAyMhIRo4cybJly9i0aRPdu3fnP//5j/vBu3r1asBUqXz22WcAbNy4kXXr1nlNr3Pnznz++efu6o0jR46wc+dODh06RGpqKjfddBOvvPIKf/zxB6mpqfzzzz906tSJN954g2PHjnHq1Kk06bVv3959X6ZNm8bll1/u81r8UapUKcaNG8eYMWNISkqib9++LF68mPnz5wOm5PHwww+7SyxPPvkkr732Glu3Grfm1NRU3nor44SXDRo0YNu2be7148ePEx1t5q7/+GPfvqLpvyNXCcHz/ClTpng911Wi8PZp2LBhhuN79erljuXjjz+md+/eGY6Jj493/z/Fx8fzww8/0LhxY8BUTY4ePZqvv/6aoul6/G3dutV9XH4nkO6xXwApIlIbmATUAKb7PyVnORC+lIhiO6hY0gpFUElKgm3b4K+/TAmifv1zFhxBIiEhgapVq7o/b731Fh9//DFPPvkkTZs2JTY2lhdeeAGASZMmMXDgQNq1a4eqUqpUqQzpvf322zRu3JhmzZoRFRXFNddcQ9OmTYmIiKBZs2b8+9//TnP8c889x9GjR93n/PLLL37jjYqKYujQoYwZM4bnn3+epKQkmjZtSuPGjXn++ecBePDBBzl48CBNmzZl9OjRNG3a1GusDRs2ZOTIkXTr1o2mTZvStWtX9u7dy+7du+nYsSMxMTH079+f119/nZSUFO68806aNGlC8+bNeeyxxyhdunSa9MaNG8fkyZNp2rQp//vf/xg7dmyGPAOlefPmNGvWjBkzZhAVFcXs2bMZOXIk9erVo0mTJrRu3ZohQ4YApkrs7bff5vbbb6dBgwY0btzY/VbuSYcOHVi9erVbWEeMGMEtt9zCFVdc4W7g9sa4ceNYuXIlTZs2pWHDhu72maeeeoqnn36ayy67jJSUlPO+Vk+GDx/Ojz/+SJ06dfjxxx/d7Tl79uzh2mtNR5n9+/dz+eWX06xZM9q0aUOPHj24+uqrAdNOdPLkSbp27UpMTAyDBg1yp/3LL7/Qo0ePoMQZaiSzIr6I/KGqLUTkSeC0qv5HRFaravPcCTEtjaOKaMkJV3My/C9eaPeCbczOAps2baJBgwa+D0hNhc2boVw5qFgx5Bbgp06dco+7GDVqFHv37s3WwzCnSElJISkpiSJFivDXX3/RuXNntm7dSqFChUIdWsh55JFH6NmzJ126dAl1KLnKmTNnuPLKK1m8eDEREYFY6gUXb791EVmlqq18nOKXQK4gSURuxwy86+lsC54t4XliezwFiZMnjcNrrVqm5NCgQcgFwsW3337L66+/TnJyMhdffLHP6oZQk5CQQKdOnUhKSkJVeffdd61IODzzzDMsX7481GHkOrt27WLUqFEhEYmcIJASRUNgELBUVT8RkRpAH1UNzvDQLOIqUaRIOMvv+SIUIeRb0rxlJCdDXBwcOgSFCkHt2tZ6w2IpIOR6iUJVNwIPe6zvAEIiEpYgoAqHDxuRSE425n2VKwe1HcJisRQsMhUKEbkMGAFc7BzvGkdRM2dD883J00kUjbIPtvPm8GHTzbV6dVuKsFgsmRJIBdok4DFgFRCcrgZBwHaNzQKJiTBqFNx8s2l/cLVH5JG2CIvFkrcJRCiOq+q8HI8kC5QoEmm7xgbK99+bqUi3bwdXV70C0sBmsVhyh0DGUfwiIm+KSDsRaeH65Hhkfsiz87LmJfbsgT594OqrITISfv4ZSpQIdVR+ra2zw5QpU9z9/LOLp6V0TEwMv/32W1DSTU9sbCxz5871uf/333+nQ4cO1KtXj/r163PfffeRkJAQ1GsFuPbaa93fw7hx42jQoAF9+/bNtqX533//TVRUFDExMTRs2JC77747je3G4sWLadOmDfXr16d+/foZPJT++9//0rhxYxo1akTDhg3d1inpefvtt/nvf/973nHmNDt27KBt27bUqVOHPn36cPbsWa/HDRs2jMaNG9O4cWM+/fRT9/b+/ftTo0YN9/+jawDinDlzePHFF3PjEnzbjLs+GFvx9J+fz9euNrufRkUK66WTrrf24pnxwAOqhQurvvyy6unTqhpEm/Fs4MvaOrtMnjxZBw8eHJS0fFlKZ0ZWLbL9xbxv3z6tXr26/vbbb6qqmpqaqjNnztR9+/YF9VrTU69ePd2+fft5nZv++nfs2KGNGjVSVWOx3qlTJ506daqqqu7du1erVaumq1atUlXVgwcPaosWLXTOnDmqqjp37lxt3ry57t69W1VVExMTdeLEiV7zbNKkSZbufW5bmd9yyy36ySefqKrq//3f/+k777yT4Zg5c+Zoly5dNCkpSU+dOqUtW7bU48ePq6pqv379dObMmRnOSU1N1ZiYGI2Pj8+wLzdtxl1C0iknhep8sCUKH6xaZUoPTZvCK6/A44+bbq/emDcc9nm3mjhvLmoC1wT+Buppbe3LQnrKlCl8/fXXJCQk8Ndff3HDDTe4XUcnT57M66+/TuXKlalbt67b38if/XZUVBSbN29m586dTJ48mY8//pilS5fStm1bv+M0/KVZtmxZVq9eTYsWLXjwwQcZPHgwBw8epGjRonzwwQfUr1+fmTNn8tJLLxEeHk6pUqWYP38+L7zwAomJiSxevJinn36aPn36uPObMGEC/fr1o127doDxcfJmFe7Lutub/fqpU6fo06cPJ06cIDk5mXfffZcrrriCSy65hJUrV/Lcc8+xfft2evXqxYABAyhTpozb0vzgwYNerb/TW4R7ThjlSXh4OG3atGH37t3u6+vfv7/bXbZ8+fK88cYbjBgxgh49evD6668zZswYt8dSkSJFuP/++zOk+/PPP9OiRQv3eIUPPviAiRMncvbsWWrXrs3//vc/ihYtGvD35Ot+ni+qys8//+y+L/369WPEiBE88MADaY7buHEjV155JREREW5Xge+++45bb73VZ9oiQseOHZkzZ47f44JCZkoCVMI0aM9z1hsC9waiQsDVwBZgGzDcy/6+mLkt1mLmv2iWWZqNihTWtpNusCUKT44fV33oIdWwMFU/E/akecuYO0z1o2uD+5k7LNNQXSWK5ORkvfnmm3XevHnOJRx3v+n9+OOPeuONN6qqeeuuUaOGHjt2TBMTE7V69eq6a9cu3bNnj1arVk0PHDigZ86c0fbt27vfsq+77jqdMmWKqqpOmjRJe/furarmzaxPnz6ampqqs2bN0hIlSujatWs1JSVFW7RooatXr1ZVU6Jo3LixNmvWTNu0aZNpmj169HBPSnTVVVfp1q1bVVV12bJl2qlTJ1VVbdy4scbFxamq6tGjR93X5qtkcMMNN+isWbO87vM878iRI5qamqqqqh988IE+/vjj7ngXL16sqqonT57UpKQkHTNmjLsEl5ycrCdOnHBfr6sE5bnsmc/tt9+uixYtUlXVnTt3av369VVV9cUXX9QWLVpoQkJChjg9SxSJiYnasWNHXbNmjc/rO3bsmJYpU0ZVVcuUKaPHjh3zev2evPDCCzpu3Dj3+qFDh9zLzz77rHtfoN+Tr/vpyebNm7VZs2ZeP67v1sXBgwe1Vq1a7vVdu3a574kn33//vbZv317j4+P14MGDWqNGDR0zZow79rp162qTJk300Ucf1dNODYGq6tSpU3XIkCEZ0sv1EgUwBZgMPOusbwU+dcTDJyISDkwAugJxwAoR+VrNuAwXO4ArVfWoiFwDTATa+ktXgfjwP4HzGjdSsFCFzz+HRx4xc1Y/+CCMHBnYuVl48w8mvqyt/VlId+7c2e2d1LBhQ7eRXseOHd021n369HGb1Pmz3+7ZsyciQpMmTahUqRJNmjQBjGPq33//TUxMDGB8ejz9iPylecsttxAeHs6pU6f47bffuOWWc44BZ86cAYwdev/+/bn11lu58cYbg3AnDb6su73Zr7du3ZoBAwaQlJTE9ddf777WQPBl/Q1pLcLT89dffxETE8Off/7JzTff7LZgVx8uvlmxMQfj+Oo5sGz9+vU899xzbhPF7t27u/cF8j35s0J3Ua9evYCtzNXLgGZv19itWzdWrFhB+/btqVChAu3atXOXkl5//XUuuugizp49y8CBAxk9erTbDy23rMwDacwur6qfAakAaiYtCqSbbBtgm6puV9WzwAwgjTWjqv6mqi77zmWA/8kEPLAz2wHTp8Ott5pBc8uXw/jxkM44Lq/hy9ran4W0p2V2eHi422470IeKN/vtsLCwNOmGhYVlauPtK02XRXZqaiqlS5dO41i6adMmAN577z1GjhzJP//8Q0xMDIcPH/abfqBW3r6su73Zr3fo0IGFCxcSHR3NXXfdlaUGYF/W357X741atWoRGxvLtm3bWLZsGV9//bX7+tJbma9atcrt8Hq+Vub9+/dn/PjxrFu3jhdffNGrlbm/78mfFbqLLVu2+LQyT985o3z58hw7dsz9vxUXF+euTkvPs88+S2xsLD/++COqSp06dQAzoZKIULhwYe655540Fue5ZWUeiFDEi0g5nKYBEbkUOB7AedHAPx7rcc42X9wLeO2GKyIDRWSliKxULnCfp7NnjXEfmHERH3wAv/8OzmQ4+YX01taBWEh70rZtWxYsWMDhw4dJSkpi5syZ7n3Bst/2JJA0S5YsSY0aNdyxqCpr1qwBzJt127Ztefnllylfvjz//POPXzv0IUOG8PHHH6fxSZo6dSr79u1Lc5wv625v9us7d+6kYsWK3H///dx7773uCZkCwZf1d6BUrlyZUaNG8frrrwMwePBgpkyZ4k7n8OHDDBs2zF1Se/rpp3nqqafc13vmzBnGjRuXId30VuYnT56kcuXKJCUlMW3aNK+x+PueArFCd5UovH3SO/yKCJ06dXJPkerLyjwlJcX98rB27VrWrl3rnsDK5cyrqsyaNSuNdXluWZkHIhSPA18DtURkCfBf4KEAzvP2uue1HVpEOmGEYpi3/ao6UVVb6Xn6lBQYFi6EmBjo1g1OnzZTkt53X74dF+FpbZ1VC+nKlSszYsQI2rVrR5cuXdJMuRlM++2spjlt2jQmTZpEs2bNaNSoEbNnzwbMHA5NmjShcePGdOjQgWbNmtGpUyc2btxITExMmu6QYCbUmTFjBk888QT16tWjQYMGLFq0iJIl084Z5su625v9+oIFC4iJiaF58+Z88cUX7sbuQK/fm/V3Vrj++utJSEhg0aJFVK5cmalTp3L//fdTv3592rdvz4ABA+jZ0/iOXnvttQwePJguXbrQqFEjnxM3XXPNNSxcuNC9/sorr9C2bVv3LHS+8PU9BWqFnhVGjx7NW2+9Re3atTl8+DD33nsvACtXrnRPUpWUlMQVV1xBw4YNGThwIFOnTnVXPfXt25cmTZrQpEkTDh06xHPPPedOO9eszANpyMAMzGsENAYiAzynHfC9x/rTwNNejmsK/AXUDSTdBkUKa5uPbszQUFOgOXhQtX9/VVC95BLVb789r2TyQvdYiyXYXH/99e6G6QuJffv26VVXXeV1X7Abs32WKESktYhc5IhJMtASeBX4l4iUDUCDVgB1RKSGiBQCbsOUTDzzqA58CdylqlsD1LYLi+3boV49mDoVhg+HDRvgWts+Y7G4cM1VcqGxa9cu/vWvf+VKXv7qLN4HugCISAeMY+xDQAymd1LGTt0eqGqyiAwBvgfCgY9UdYOIDHL2vwe8AJQD3nEaB5M1gOqlC8Ln6cQJKFkSatSAe+6B/v2hgEyraLEEk3r16lGvXr1Qh5HrtM7Fdkl/QhGuqkec5T7ARFX9AvhCRGIDSVxV5wJz0217z2P5PuC+LEUMBdvnKSHBDJabOBHWrIGqVcGHdYHFYrHkBv4as8NFxCUknYGfPfblz9bTvM6330KjRsbptXdvyIVubxaLxZIZ/h74nwC/isghIBFYBCAitQmse6wlUJKT4fbbzeC5Bg3g11+hQ4dQR2WxWCyAH6FQ1VdF5CegMvCD02oOphQSSPdYS2aomjkhIiKgUiV47TUYOtRMTWqxWCx5BL/jKFR1map+parxHtu2qmrgI3Us3lmxAtq2Bdegp/Hj4emnC7xIiAhDhw51r48ZM4YRI0b4PSe7dtcupkyZQoUKFYiJiaFRo0bcfPPNJCQkZDvdrOLPan3Dhg1cddVV1K1blzp16vDKK6+ksYGYN28erVq1okGDBtSvX58nnnjCax6zZs3i5ZdfzulLOW+OHDlC165dqVOnDl27duXo0aNej/O0fG/VqlWm569bt47+/fvnxiVcUAQy4M4STI4fhyFDjEjExZlpSS8gChcuzJdffsmhQ4cCPqdXr14MHz48KPn36dOH2NhYNmzYQKFChTIMdMsNXDYm69evp2zZsm4bk8TERPe1bt26lTVr1vDbb7/xzjvvAMbHaMiQIUydOpVNmzaxfv16atb0PiPxG2+8wYMPPhhwTFmxLwkGo0aNonPnzvz555907tzZ74vAL7/8QmxsbBrLD1/nN2nShLi4OLfLrSU42Ebp3GTmTHj4YThwwIjFyJGmC2wIGP37aDYf2RzUNOuXrc+wNl4H17uJiIhg4MCB/Pvf/+bVV19Ns8+XxfOUKVNYuXIlr776Ks2aNWP79u2EhYWRkJBAvXr12L59O7t27fJqG+2L5ORk4uPjKVOmjM+8K1SoQL169fjtt9+oUKECqamp1K1bl2XLlqGqXm23vdl7l/AzYZSn1fr06dO57LLL3NYNRYsWZfz48XTs2JHBgwfzxhtv8Oyzz7qvKyIiwqsYbN26lcKFC7tHFvu6r+ktwseOHev1mnxZwGeH2bNns2DBAsBYb3fs2JHRo0cH5fyePXu6R/tbgoMtUeQmmzZBdLQx8Bs3LmQiEWoGDx7MtGnTOH48bZ+Iyy+/nGXLlrF69Wpuu+0297wTLkqVKkWzZs349ddfAfMA7N69O5GRkQwcOJD//Oc/rFq1ijFjxvh8m/7000+JiYkhOjqaI0eOuC0jvOUdFhbGnXfe6fYMmj9/Ps2aNaN8+fI88sgjPPbYY6xYsYIvvvjCbcUwZswYJkyYQGxsLIsWLfJr2JaSksJPP/1Er169AFPt1LJlyzTH1KpVi1OnTnHixAnWr1+fYb83lixZksbSxN99XbVqFbNnz2b69Ok+r6l+/fosXLiQ1atX8/LLL/PMM89kyPPkyZM+jfI8XWdd7N+/n8qVKwPGjuXAgQNer0VE6NatGy1btkwzA56/81u1asWiRYsyvU+WwLElipzkzBl4801o1gx69jRtEM8+C+HhoY4s0zf/nKRkyZLcfffdjBs3Ls2DNBCL5z59+vDpp5/SqVMnZsyYwYMPPujXNtrb+ePHj0dVGTx4MG+++SbDhw/3mfeAAQPo3bs3jz76KB999BH33HMP4Nt225u9d3p8Wa2rD+ttyJr99t69e9326+D/vnpahPu6Jn8W8C5KlCiRZaPAQFiyZAlVqlThwIEDbv+mDpn0CMwt6+0LCVuiyCl++cUIxPPPw08/mW2RkXlCJPICjz76KJMmTSI+3t1PIiCL5169ejFv3jyOHDnCqlWruOqqq/zaRvtCROjZs6fbUM5X3tWqVaNSpUr8/PPPLF++nGuuuQbwbbvtzd47Pb6s1r1Zb2/fvp3ixYtTokSJ87be9ndfPS3CfV2TPwt4F1ktUVSqVMltu7F3714qVqzo9VpcltwVK1bkhhtucFts+zs/t6y3LySsUASbAwegXz+46ipISoJ58+Dtt0MdVZ6jbNmy3HrrrUyadG7+q0AsnosXL06bNm145JFHuO666wgPD/drG+2PxYsXU6tWrUzzvu+++7jzzju59dZbCXeE3pfttjd7b1+kt1rv27cvixcvZv78+YApeTz88MPuuvYnn3yS1157zT1BU2pqKm+99VaGdNNbbwdyX/1dUyAW8K4ShbePa44JT3r16uWOxZf1dnx8vNuGPT4+nh9++MFtqe3v/Nyy3r6QsEIRbH74AT75xFQxrV8PV18d6ojyLEOHDk3T+ylQi+c+ffowderUNHNM+7KNTo+rjaJp06asXr2a559/PtO8e/XqxalTp9zVTuDbdtubvbc/PK3Wo6KimD17NiNHjqRevXo0adKE1q1bM2TIEACaNm3K22+/ze23306DBg1o3LixVzO8Dh06sHr1ane32kDvq69ryqoFfCAMHz6cH3/8kTp16vDjjz+6e7Xt2bOHax3Ty/3793P55ZfTrFkz2rRpQ48ePbja+T35Oh9y0Xr7AkI8+2jnBxpGFdG2X93O5KsnhzqUc6xbB1u2mImEVGHHDvDRbTGUbNq0Kc20kZbAWLlyJY899li+aiB95JFH6NmzJ126dAl1KLnKmTNnuPLKK1m8eLF7PocLEW+/dRFZFYjpqjdsiSI7xMfDU09B8+bmb1KSGWmdB0XCcn6MGjWKm266yT0zW37hmWeeCclgwlCza9cuRo0adUGLRE5gheJ8+eYbaNjQ9Grq39+MtI6MDHVUliAzfPhwdu7cGZTpVHOTSpUqubvdXkjUqVOHjh07hjqMAkf+k90sdBPMMdavh169jNProkWQzx4iFovFkhXyZYni2pohmOEtORmckaA0bgxz5sDq1VYkLBZLgSf/CYUqt9S9JfPjgsny5dCqFXTuDH/+abb16GGrmiwWywVB/hOK3OToUXjgAWjXDg4dMl5NtWuHOiqLxWLJVaxQ+OLMGdObaeJEePRR49N04415o40kH1O8eHH38ty5c6lTpw67du1ixIgRFC1aNI1nj+exgdqTe1qJ169fn3//+99p9k+cOJH69etTv3592rRpw+LFi937kpKSGD58OHXq1KFx48a0adOGefPmeb2Om2++me3bt2f5+nOL7777jnr16lG7dm2fzqzHjx+nZ8+e7rEnkyebLuenT5+mTZs27u0vvvii+5wnnniCn3/+2Wt6lgKMquarT4MihTVHiYs7tzx5suoff+RsfrnIxo0bQx2CFitWTFVV58+frzVr1tRt27apquqLL76o1apV06eeeirDsaqqhQsX1ksuuUQPHjyoqqpvvvmmvvjiixnSnzx5sg4ePFhVVQ8dOqTlypXTXbt2qarqN998oy1atHCnsWrVKq1WrZru3btXVVWHDRumd999t54+fVpVVfft26effvpphjzWr1+v119/fZauOzk5OUvHZ4fk5GStWbOm/vXXX3rmzBlt2rSpbtiwIcNxr776qvt+HzhwQMuUKaNnzpzR1NRUPXnypKqqnj17Vtu0aaNLly5VVdW///5bu3btmmvXYjk/vP3WgZV6ns/d/NfrKac4fRpGjzazzH32mZmzugBPgLLvtdc4sym4NuOFG9TnIi/OoulZtGgR999/P3PnznVbaIAx4JsyZQrDhg2jbNmyac7xZ0/ui3LlylG7dm327t1LtWrVGD16NG+++aZ7dHKLFi3o168fEyZM4Omnn+aDDz5gx44dFC5cGDBdTG+99dYM6U6bNi2NZcQDDzzAihUrSExM5Oabb+all14CzKQ7AwYM4IcffmDIkCGULVuWF198kTNnzlCrVi0mT55M8eLFefnll/nmm29ITEykffv2vP/++1kyAUzP77//Tu3atd1zVdx2223Mnj07g5WGiHDy5ElUlVOnTlG2bFkiIiIQEXdpLikpiaSkJHc8F198MYcPH2bfvn1cdNFF5x2jJX9hq57AmPY1bQojRsBNN5lJhSw5wpkzZ+jduzezZs3KMF9E8eLFGTBgAGPHjvV6ri97cl/s2rWL06dP07RpU8C7jXerVq3YsGED27Zto3r16pQMwPp9yZIladJ59dVXWblyJWvXruXXX391zy8BUKRIERYvXkyXLl0YOXIk8+fP548//qBVq1Zun6YhQ4awYsUK1q9fT2JiInPmzMmQ57Rp07wa7t18880Zjt29ezfVqlVzr1etWpXdu3dnOG7IkCFs2rSJKlWq0KRJE8aOHUtYmHkkpKSkEBMTQ8WKFenatSttPX4TLVq0YMmSJZneJ0vBwZYoHn0Uxo41jdQ//ACO5XNBJ5A3/5wgMjKS9u3bM2nSJK+C8PDDDxMTE5OmPcKFL3vy9Hz66af88ssvbNmyhQ8++IAiRYr4PFb9WHv7Ir2N92effcbEiRNJTk5m7969bNy40S1OLj+qZcuWsXHjRi677DIAzp49S7t27QDjTfTGG2+QkJDAkSNHaNSokXueDBd9+/alb9++AcWnXmx5vF3j999/T0xMDD///DN//fUXXbt25YorrqBkyZKEh4cTGxvLsWPHuOGGG1i/fr3baM/aeF94XJglitRUcJmbtWkDL7xg/JouEJEIJWFhYXz22WesWLGC1157LcP+0qVLc8cdd7in/0yPN3vy9PTp04cNGzawaNEihg4dyr59+wBo2LBhBpvuP/74g4YNG1K7dm127drldiv1h6eN944dOxgzZgw//fQTa9eupUePHl5tvFWVrl27uh1VN27cyKRJkzh9+jQPPvggn3/+OevWreP+++/3auOdlRJF1apV+eeff9zrcXFxbrtuTyZPnsyNN96IiFC7dm1q1KiRwe22dOnSdOzYke+++869zdp4X3hceEKxZg20bw/OHADccQe89BL4eeu0BJeiRYsyZ84ct+Nreh5//HHef/99r/M4e7Mn90W7du2466673CWXp556imHDhnHYmac8NjaWKVOm8OCDD1K0aFHuvfdeHn74Yc6ePQuYksPUqVMzpOtp433ixAmKFStGqVKl2L9/v89eUpdeeilLlixxn5eQkMDWrVvdolC+fHlOnTrF559/7vX8vn37erXw9nZ869at+fPPP9mxYwdnz55lxowZXu08qlevzk/OXCn79+9ny5Yt1KxZk4MHD3Ls2DHAWJ3Pnz8/TTWhtfG+8Lhwqp5OnYIXXzTVTGXLgm2ICylly5blu+++o0OHDhmsr8uXL88NN9yQoWuri6FDh6aZN8Efw4YNo0WLFjzzzDP06tWL3bt30759e0SEEiVKMHXqVPeUmiNHjuS5556jYcOGFClShGLFivHyyy9nSLNHjx4sWLCALl260KxZM5o3b06jRo2oWbOmu2opPRUqVGDKlCncfvvt7tn3Ro4cSd26dbn//vtp0qQJl1xyCa1btw7ouvwRERHB+PHj6d69OykpKQwYMIBGjRoBuK3DBw0axPPPP0///v1p0qQJqsro0aMpX748a9eupV+/fqSkpJCamsqtt97KddddB5jG7W3bttGq1XmZkFryKfnSZnxjYsaiuV/mz4d77oG4OBg4EEaNgjJlcibAPIy1GQ8OiYmJdOrUiSVLlrgnMrpQ+Oqrr/jjjz945ZVXQh2KxQ/WZvx8KFTIlCKWLIH3378gRcISPKKionjppZe89iQq6CQnJ3vtaGAp2BTMqqekJDP96PHjMHIkdOhgDPzCLgxdtOQ83bt3D3UIIeGWW3LZZ82SJyh4T87ffoOWLc1EQps2mR5OYEXCIb9VNVoslqyRE7/xgvP0PHLEtD9cdhkcOwazZsEXX1iB8KBIkSIcPnzYioXFUkBRVQ4fPux37ND5UHCqng4fhunT4YknTO8mD0M5i6Fq1arExcVx8ODBUIdisVhyiCJFilC1atWgppm/hWLLFvj0UzNgrk4d2LkTypULdVR5lsjISGrUqBHqMCwWSz4jR+tlRORqEdkiIttEZLiX/SIi45z9a0WkRUAJJyYacWjaFP79b3CNQrUiYbFYLEEnx4RCRMKBCcA1QEPgdhFpmO6wa4A6zmcg8G5m6YanpkKTJvDKK3DLLbB5M3gYoFksFosluORk1VMbYJuqbgcQkRlAb2CjxzG9gf86XunLRKS0iFRW1b2+Ei10Nsk0UM+fb6YmtVgsFkuOkpNCEQ3847EeB6T37/Z2TDSQRihEZCCmxAFwRv78cz1dugQ32vxJeeBQqIPII9h7cQ57L85h78U56p3viTkpFN68m9P3ywzkGFR1IjARQERWnu8w9IKGvRfnsPfiHPZenMPei3OIyMrzPTcnG7PjAM/Gg6pAehP7QI6xWCwWSwjJSaFYAdQRkRoiUgi4Dfg63TFfA3c7vZ8uBY77a5+wWCwWS+6TY1VPqposIkOA74Fw4CNV3SAig5z97wFzgWuBbUACcE8ASU/MoZDzI/ZenMPei3PYe3EOey/Ocd73It/ZjFssFosld7FGSBaLxWLxixUKi8VisfglzwpFjtl/5EMCuBd9nXuwVkR+E5FmoYgzN8jsXngc11pEUkTk5tyMLzcJ5F6ISEcRiRWRDSLya27HmFsE8BspJSLfiMga514E0h6a7xCRj0TkgIis97H//J6bqprnPpjG77+AmkAhYA3QMN0x1wLzMGMxLgWWhzruEN6L9kAZZ/maC/leeBz3M6azxM2hjjuE/xelMU4I1Z31iqGOO4T34hlgtLNcATgCFAp17DlwLzoALYD1Pvaf13Mzr5Yo3PYfqnoWcNl/eOK2/1DVZUBpEamc24HmApneC1X9TVWPOqvLMONRCiKB/F8APAR8ARzIzeBymUDuxR3Al6q6C0BVC+r9COReKFBCRAQojhGK5NwNM+dR1YWYa/PFeT0386pQ+LL2yOoxBYGsXue9mDeGgkim90JEooEbgPdyMa5QEMj/RV2gjIgsEJFVInJ3rkWXuwRyL8YDDTADetcBj6hqau6El6c4r+dmXp2PImj2HwWAgK9TRDphhOLyHI0odARyL94Ghqlqinl5LLAEci8igJZAZyAKWCoiy1R1a04Hl8sEci+6A7HAVUAt4EcRWaSqJ3I4trzGeT0386pQWPuPcwR0nSLSFPgQuEZVD+dSbLlNIPeiFTDDEYnywLUikqyqs3Ilwtwj0N/IIVWNB+JFZCHQDChoQhHIvbgHGKWmon6biOwA6gO/506IeYbzem7m1aona/9xjkzvhYhUB74E7iqAb4ueZHovVLWGql6iqpcAnwMPFkCRgMB+I7OBK0QkQkSKYtybN+VynLlBIPdiF6ZkhYhUwjipbs/VKPMG5/XczJMlCs05+498R4D34gWgHPCO8yadrAXQMTPAe3FBEMi9UNVNIvIdsBZIBT5UVa/dJvMzAf5fvAJMEZF1mOqXYapa4OzHReQToCNQXkTigBeBSMjec9NaeFgsFovFL3m16slisVgseQQrFBaLxWLxixUKi8VisfjFCoXFYrFY/GKFwmKxWCx+sUJhSUNm7pMexz3ruHCuddxJ2wY5jrkiUtpZflhENonINBHp5c811jn+N+fvJSJyR4D5XS8iLzjLI0Rkt3NdsSIyys95I0TkiYAvzHsal4hIopPXRhF5T0Sy9NsUkVYiMs5Z7igi7T32DQqGfUe6+7JRRG4P4JxHnTEcmR03Q0TqZDdGS85gu8da0iAiHYBTGOOwxj6OaQe8BXRU1TMiUh7jxJkjI+NFZDNmxPmOLJ7XEXhCVa8L4NjfgF6qekhERgCnVHVMAOcFfKyfNC4B5qhqYxGJwDjfvq2qX55netmOKbN0nYf6KqCcqib5OedvoFVmYxZE5ErgTlW9P4ghW4KELVFY0hCA+yRAZYw1xBnnnEMukRCRv0VktIj87nxqO9sriMgXIrLC+VzmbC8uIpNFZJ1TOrnJI53yIvIexj76axF5TET6i8h455hKIvKVmDkG1rjeokXklBPnKMzI5Fjn3EUiEuO6CBFZIiJNRaQucMbfw0xE7nfiXuNcR4a3ZKfks9G5jhnOtmJOKW2FiKwWEW9ut573Pxn4DagtIheLyE9Oej+JGYGPiNwiIuudWBY62zqKyBxHdAYBjznXfYWr1CMiDUTEbVnhlGTWOsstReRXMeaB30smjqKq+idmwFYZ5/x3RWSlmFLmS677AVQBfhGRX5xt3URkqYj8ISIzRaS4k+QioIsjlJa8Rm77pdtP3v8Al+DDz97ZXxxjsLYVeAe40mPf38CzzvLdmDdlgOnA5c5ydWCTszwa8/bsOr+MRzrlvSz3B8Y7y58CjzrL4UApZ/mU87ejK39nvZ8rL4yz6kpn+R7gXx7HjQB2O9cYizGUK+exfyTwkMexTzjLe4DCznJp5+9rmDdlMPNDbAWK+brfQFGMJcU1wDdAP2f7AGCWs7wOiE6Xj/taPWPyEmMsUNNZHgY8hxm5+xtQwdneBzO6Of337plOC2CRx76yHt/DAqCpl++uPLDQdf1O/i94pPEj0DLU///2k/FjSxSWLKOqpzCupAOBg8CnItLf45BPPP62c5a7AONFJBbjN1NSREo42yd4pH2UwLkKeNc5L0VVj2dy/EzgOhGJxDx4pzjbKzvX4cm/VTXG+XwPNHZKJOuAvkAjL+mvBaaJyJ2cm+ugGzDcue4FQBGMUKanlnPMEuBbVZ2HuXfTnf3/45wr8BKMHcX9mAdzVvgMuNVZ7oMR23pAY4yjaixGPHzNafKYiGwBlmOEw8WtIvIHsBpzbxp6OfdSZ/sSJ59+wMUe+w9gSiCWPIYt5lkyRUSqYd5uAd5T4yOUgnnwLXAenv049+D1bPhyLYcB7VQ1MV3aQi7Zw6tqgoj8iJm85VaM0yxAIlAqk9OnANer6hpHFDt6OaYHZoaxXsDzItII4yt0k6puyST9v1Q1JrNLcK5jkJjOAz2AWM/qtAD4FJgpIl+apPRPEWkCbFDVdpmcC0ZAx4jIjcB/RaQWRmifAFqr6lERmYIRxPQI8KOq+moEL4L5Lix5DFuisGSKqv7j8Xb9nojUk7Q9VGKAnR7rfTz+LnWWfwCGuA7weLil314mC6H9BDzgnBcuIiXT7T8JlEi37UNgHLBCVV1tMZuA2pnkVQLY65RG+qbfKaaXUjVV/QV4ClPNVBxjVPeQI4iISPPALg0w1UG3Oct9gcVOGrVUdbmqvgAcIq1tNHi/bgBU9S8gBXgeIxoAW4AKYjopICKRjsj5RE1D+0rMC0JJIB44LsaZ9RofsSwDLpNz7VZFnfYhF3WBDf7ytYQGKxSWNIhxn1wK1BOROBG518thxYGPXQ23mOqEER77C4vIcuAR4DFn28NAK6dhdiOmwRVMfX8ZV+Ms0CkL4T4CdHJKNKvIWB20Fkh2Gn0fA1DVVcAJYLLHcQuB5q6HuQ+ex1S3/Ahs9rI/HJjqxLIa8+Z9DONaGgmsFdPl+JUsXN/DwD3OPb7LuV6AN8U0/q93Yl+T7rxvgBtcjdle0v0UuBNTDYWa6UNvBkY730EsZh72zHgZeBzTZrIa85D/CFM15mIiME9EflHVg5g2pk+ca1qGmRPCZf2dqAVzqoB8j+0eawkqEmB3yFAhIlUwVWb11WMqTBEZC3yjqvNDFduFjCPkJ1R1UqhjsWTEligsFwxiBp0tx/TKSj9f8muYHkeW0HAM+DjUQVi8Y0sUFovFYvGLLVFYLBaLxS9WKCwWi8XiFysUFovFYvGLFQqLxWKx+MUKhcVisVj88v/a7V48X+Tu5AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}